{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e51f0e",
   "metadata": {},
   "source": [
    "### Task2: Traditional ML Model\n",
    "\n",
    "- Train a traditional ML model (e.g. k-NN, SVM, random forest) with any features\n",
    "extracted from the audio\n",
    "\n",
    "- Need to report how to implement the model clearly\n",
    "\n",
    "- Need to report the testing result (not validation result) with confusion matrix,\n",
    "top1 accuracy, and top3 accuracy\n",
    "\n",
    "- Remember to utilize standardization (e.g. mean, std), pooling and normalization\n",
    "to ensure consistent feature scales, reducing overfitting, and improving model\n",
    "stability and performance during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596aba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db58b348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bass', 'mallet', 'reed', 'string', 'synth_lead', 'keyboard', 'vocal', 'brass', 'flute', 'organ', 'guitar'}\n"
     ]
    }
   ],
   "source": [
    "# load the json file\n",
    "def load_json(json_file):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "data = load_json('nsynth-subtrain/examples.json')\n",
    "\n",
    "# get all \"instrument_family_str\"\n",
    "instrument_family_str = set()\n",
    "for key in data:\n",
    "    instrument_family_str.add(data[key][\"instrument_family_str\"])\n",
    "\n",
    "print(instrument_family_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44de218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all keys in data\n",
    "keys = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe41d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_extraction(key, file_path):\n",
    "#     # feature extraction from audio file\n",
    "#     y, sr = librosa.load(file_path)\n",
    "#     # extract the chroma feature\n",
    "#     chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "#     # extract the spectral contrast feature\n",
    "#     spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "#     # extract the Mel-frequency Cepstral Coefficients\n",
    "#     mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "#     # extract the Zero-Crossing Rate\n",
    "#     zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
    "#     # extract the Spectral Centroid\n",
    "#     spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "\n",
    "#     # put all features into a list\n",
    "#     features = [chroma, spectral_contrast, mfcc, zero_crossing_rate, spectral_centroid]\n",
    "\n",
    "#     return features\n",
    "    \n",
    "\n",
    "def feature_extraction(key, file_path):\n",
    "    y, sr = librosa.load(file_path)\n",
    "\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "\n",
    "    # put all features into a list\n",
    "    features = [spectral_contrast, mfcc]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97f0ef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 6804/48037 [01:35<09:36, 71.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnsynth-subtrain/audio/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# extract the features\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m feature \u001b[38;5;241m=\u001b[39m feature_extraction(key, file)\n\u001b[1;32m     11\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(feature)\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mfeature_extraction\u001b[0;34m(key, file_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m y, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(file_path)\n\u001b[1;32m     24\u001b[0m spectral_contrast \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mspectral_contrast(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[0;32m---> 25\u001b[0m mfcc \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmfcc(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# put all features into a list\u001b[39;00m\n\u001b[1;32m     28\u001b[0m features \u001b[38;5;241m=\u001b[39m [spectral_contrast, mfcc]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/librosa/feature/spectral.py:1989\u001b[0m, in \u001b[0;36mmfcc\u001b[0;34m(y, sr, S, n_mfcc, dct_type, norm, lifter, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mel-frequency cepstral coefficients (MFCCs)\u001b[39;00m\n\u001b[1;32m   1844\u001b[0m \n\u001b[1;32m   1845\u001b[0m \u001b[38;5;124;03m.. warning:: If multi-channel audio input ``y`` is provided, the MFCC\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;124;03m>>> fig.colorbar(img2, ax=[ax[1]])\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m S \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1988\u001b[0m     \u001b[38;5;66;03m# multichannel behavior may be different due to relative noise floor differences between channels\u001b[39;00m\n\u001b[0;32m-> 1989\u001b[0m     S \u001b[38;5;241m=\u001b[39m power_to_db(melspectrogram(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m   1991\u001b[0m M: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mfftpack\u001b[38;5;241m.\u001b[39mdct(S, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mdct_type, norm\u001b[38;5;241m=\u001b[39mnorm)[\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :n_mfcc, :\n\u001b[1;32m   1993\u001b[0m ]\n\u001b[1;32m   1995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lifter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;66;03m# shape lifter for broadcasting\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/librosa/feature/spectral.py:2143\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   2130\u001b[0m S, n_fft \u001b[38;5;241m=\u001b[39m _spectrogram(\n\u001b[1;32m   2131\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m   2132\u001b[0m     S\u001b[38;5;241m=\u001b[39mS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2139\u001b[0m     pad_mode\u001b[38;5;241m=\u001b[39mpad_mode,\n\u001b[1;32m   2140\u001b[0m )\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;66;03m# Build a Mel filter\u001b[39;00m\n\u001b[0;32m-> 2143\u001b[0m mel_basis \u001b[38;5;241m=\u001b[39m filters\u001b[38;5;241m.\u001b[39mmel(sr\u001b[38;5;241m=\u001b[39msr, n_fft\u001b[38;5;241m=\u001b[39mn_fft, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2145\u001b[0m melspec: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...ft,mf->...mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, S, mel_basis, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m melspec\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/librosa/filters.py:239\u001b[0m, in \u001b[0;36mmel\u001b[0;34m(sr, n_fft, n_mels, fmin, fmax, htk, norm, dtype)\u001b[0m\n\u001b[1;32m    236\u001b[0m     upper \u001b[38;5;241m=\u001b[39m ramps[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m fdiff[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# .. then intersect them with each other and zero\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     weights[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39mminimum(lower, upper))\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(norm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslaney\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# Slaney-style mel is scaled to be approx constant energy per channel\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "# extract the features from each audio file\n",
    "\n",
    "features = []\n",
    "\n",
    "# for file in keys:\n",
    "for key in tqdm.tqdm(keys):\n",
    "    file = 'nsynth-subtrain/audio/' + key + '.wav'\n",
    "    # extract the features\n",
    "    feature = feature_extraction(key, file)\n",
    "    features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e4212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(7, 173)\n",
      "(20, 173)\n"
     ]
    }
   ],
   "source": [
    "for i in features:\n",
    "    print(len(i))\n",
    "    print(i[0].shape)\n",
    "    print(i[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87eb94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten\n",
    "spectral_contrast_features = [f[0].flatten() for f in features]\n",
    "mfcc_features = [f[1].flatten() for f in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa375917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# normalize\n",
    "\n",
    "scaler_spectral_contrast = StandardScaler()\n",
    "spectral_contrast_normalized = scaler_spectral_contrast.fit_transform(spectral_contrast_features)\n",
    "\n",
    "scaler_mfcc = StandardScaler()\n",
    "mfcc_normalized = scaler_mfcc.fit_transform(mfcc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b3aba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_combined = np.concatenate((spectral_contrast_normalized, mfcc_normalized), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed9b1ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48037, 4671)\n"
     ]
    }
   ],
   "source": [
    "# print the size of the features\n",
    "print(features_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ede72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the features\n",
    "np.save('features.npy', features_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5e1cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label and file name of each audio file\n",
    "labels = []\n",
    "for key in keys:\n",
    "    labels.append(data[key][\"instrument_family\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e8a47ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier\n\u001b[1;32m     15\u001b[0m knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(features_combined, labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_combined' is not defined"
     ]
    }
   ],
   "source": [
    "# features_combined: the features of each audio file\n",
    "# labels: the labels of each audio file\n",
    "# keys: the file name of each audio file\n",
    "\n",
    "# use knn to classify the audio files\n",
    "# use all the data to train the model\n",
    "\n",
    "# # use svm to classify the audio files\n",
    "# # use all the data to train the model\n",
    "# from sklearn.svm import SVC\n",
    "# model = SVC(kernel='linear')\n",
    "# model.fit(features_combined, labels)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(features_combined, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb224864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model checkpoint\n",
    "import joblib\n",
    "joblib.dump(knn, 'knn_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the knn model\n",
    "# knn = joblib.load('knn_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1acdc2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12678/12678 [03:08<00:00, 67.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# validate the model\n",
    "val_data = load_json('nsynth-valid/examples.json')\n",
    "val_keys = list(val_data.keys())\n",
    "\n",
    "val_features = []\n",
    "for key in tqdm.tqdm(val_keys):\n",
    "    file = 'nsynth-valid/audio/' + key + '.wav'\n",
    "    feature = feature_extraction(key, file)\n",
    "    val_features.append(feature)\n",
    "\n",
    "val_spectral_contrast_features = [f[0].flatten() for f in val_features]\n",
    "val_mfcc_features = [f[1].flatten() for f in val_features]\n",
    "# normalize\n",
    "val_spectral_contrast_normalized = scaler_spectral_contrast.transform(val_spectral_contrast_features)\n",
    "val_mfcc_normalized = scaler_mfcc.transform(val_mfcc_features)\n",
    "val_features_combined = np.concatenate((val_spectral_contrast_normalized, val_mfcc_normalized), axis=1)\n",
    "\n",
    "val_labels = []\n",
    "for key in val_keys:\n",
    "    val_labels.append(val_data[key][\"instrument_family\"])\n",
    "\n",
    "# predict the labels of the validation data\n",
    "val_pred = knn.predict(val_features_combined)\n",
    "\n",
    "# calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(val_labels, val_pred)\n",
    "print(accuracy)\n",
    "\n",
    "# precision, recall, f1-score\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(val_labels, val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa12efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume val_labels and val_pred are your true and predicted labels\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_pred)\n",
    "\n",
    "# Plot confusion matrix using seaborn's heatmap\n",
    "plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "            xticklabels=instrument_family_str,\n",
    "            yticklabels=instrument_family_str)\n",
    "            # xticklabels=['Predicted Class 0', 'Predicted Class 1'], \n",
    "            # yticklabels=['Actual Class 0', 'Actual Class 1'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4948a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 1 accuracy\n",
    "top1 = 0\n",
    "for i in range(len(val_labels)):\n",
    "    if val_labels[i] == val_pred[i]:\n",
    "        top1 += 1\n",
    "top1 /= len(val_labels)\n",
    "print(top1)\n",
    "\n",
    "# top 3 accuracy\n",
    "top3 = 0\n",
    "for i in range(len(val_labels)):\n",
    "    if val_labels[i] in knn.classes_[np.argsort(knn.predict_proba(val_features_combined)[i])[-3:]]:\n",
    "        top3 += 1\n",
    "top3 /= len(val_labels)\n",
    "print(top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e92932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "test_data = load_json('nsynth-test/examples.json')\n",
    "test_keys = list(test_data.keys())\n",
    "\n",
    "test_features = []\n",
    "for key in tqdm.tqdm(test_keys):\n",
    "    file = 'nsynth-test/audio/' + key + '.wav'\n",
    "    feature = feature_extraction(key, file)\n",
    "    test_features.append(feature)\n",
    "\n",
    "test_spectral_contrast_features = [f[0].flatten() for f in test_features]\n",
    "test_mfcc_features = [f[1].flatten() for f in test_features]\n",
    "# normalize\n",
    "test_spectral_contrast_normalized = scaler_spectral_contrast.transform(test_spectral_contrast_features)\n",
    "test_mfcc_normalized = scaler_mfcc.transform(test_mfcc_features)\n",
    "test_features_combined = np.concatenate((test_spectral_contrast_normalized, test_mfcc_normalized), axis=1)\n",
    "\n",
    "test_labels = []\n",
    "for key in test_keys:\n",
    "    test_labels.append(test_data[key][\"instrument_family\"])\n",
    "\n",
    "# predict the labels of the test data\n",
    "test_pred = knn.predict(test_features_combined)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(test_labels, test_pred)\n",
    "print(accuracy)\n",
    "\n",
    "# precision, recall, f1-score\n",
    "print(classification_report(test_labels, test_pred))\n",
    "# confusion matrix\n",
    "print(confusion_matrix(test_labels, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da701427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume val_labels and val_pred are your true and predicted labels\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_pred)\n",
    "\n",
    "# Plot confusion matrix using seaborn's heatmap\n",
    "plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "            xticklabels=instrument_family_str,\n",
    "            yticklabels=instrument_family_str)\n",
    "            # xticklabels=['Predicted Class 0', 'Predicted Class 1'], \n",
    "            # yticklabels=['Actual Class 0', 'Actual Class 1'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 1 accuracy\n",
    "top1 = 0\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] == test_pred[i]:\n",
    "        top1 += 1\n",
    "top1 /= len(test_labels)\n",
    "print(top1)\n",
    "\n",
    "# top 3 accuracy\n",
    "top3 = 0\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] in knn.classes_[np.argsort(knn.predict_proba(test_features_combined)[i])[-3:]]:\n",
    "        top3 += 1\n",
    "top3 /= len(test_labels)\n",
    "print(top3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
