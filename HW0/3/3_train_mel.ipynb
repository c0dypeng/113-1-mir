{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e51f0e",
   "metadata": {},
   "source": [
    "### Task3: Deep Learning Model\n",
    "\n",
    "- Train a deep learning model (e.g. CNN or attention based model) with Mel-spectrograms extracted from the audio as input\n",
    "\n",
    "- Need to compare 2 different kinds of inputs: Mel-spectrograms with or without taking the log\n",
    "\n",
    "- You can choose whatever FFT window size and hop length you like\n",
    "\n",
    "- You can choose whatever deep learning model you like\n",
    "\n",
    "- Need to report how to implement the model clearly\n",
    "\n",
    "- Need to report the testing result (not validation result) with confusion matrix, top1 accuracy, and top3 accuracy\n",
    "\n",
    "- You can use any music tagging model. For a novice, the short chunk CNN in this repo is recommended. (Need to replace the BCE loss to Cross-entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "536f6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset file path:\n",
    "traning_data_path = '<PUT THE PATH TO THE TRAINING DATA HERE>'\n",
    "\n",
    "# traning_data_path = 'nsynth-subtrain'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596aba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import librosa\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db58b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file\n",
    "def load_json(json_file):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# let the json path be /examples.json under the \"traning_data_path\"\n",
    "json_path = os.path.join(traning_data_path, 'examples.json')\n",
    "\n",
    "data = load_json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44de218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all keys in data\n",
    "keys = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe41d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(key, file_path):\n",
    "    y, sr = librosa.load(file_path)\n",
    "\n",
    "    # FFT window size=2048, and the hop length=512\n",
    "    # extract the mel spectrogram feature\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=512, hop_length=512)\n",
    "\n",
    "    # extract the mel spectrogram feature with log scaling\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "\n",
    "    # put all features into a list\n",
    "    features = [mel_spectrogram, log_mel_spectrogram]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97f0ef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48037 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48037/48037 [02:50<00:00, 281.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract the features from each audio file\n",
    "\n",
    "features = []\n",
    "\n",
    "# for file in keys:\n",
    "for key in tqdm.tqdm(keys):\n",
    "    file = os.path.join(traning_data_path, 'audio', key + '.wav')\n",
    "    # extract the features\n",
    "    feature = feature_extraction(key, file)\n",
    "    features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3ede72",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = [f[0] for f in features]\n",
    "log_mel_spectrogram = [f[1] for f in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b205b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one hot encoding of the labels\n",
    "\n",
    "# Extract labels from the data\n",
    "labels = [data[key][\"instrument_family_str\"] for key in keys]\n",
    "\n",
    "# Initialize the LabelEncoder and OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# dump the labels_encoder\n",
    "with open('label_encoder_mel.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Convert string labels to integer labels\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Reshape integer_encoded to 2D array (necessary for OneHotEncoder)\n",
    "integer_encoded = integer_encoded.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92bd4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(mel_spectrogram)\n",
    "y = integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ea81a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_2d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size=3, stride=1, pooling=2):\n",
    "        super(Conv_2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size, stride=stride, padding=kernel_size//2)\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mp = nn.MaxPool2d(pooling)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.mp(self.relu(self.bn(self.conv(x))))\n",
    "        return out\n",
    "\n",
    "class ShortChunkCNN(nn.Module):\n",
    "    '''\n",
    "    Short-chunk CNN architecture.\n",
    "    So-called VGG-like model with a small receptive field.\n",
    "    Deeper layers, smaller pooling (2x2).\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 n_channels=1,\n",
    "                 n_class=11):\n",
    "        super(ShortChunkCNN, self).__init__()\n",
    "\n",
    "        # CNN Layers\n",
    "        self.layer1 = Conv_2d(1, n_channels, pooling=2)\n",
    "        self.layer2 = Conv_2d(n_channels, n_channels, pooling=2)\n",
    "        self.layer3 = Conv_2d(n_channels, n_channels*2, pooling=2)\n",
    "        self.layer4 = Conv_2d(n_channels*2, n_channels*2, pooling=2)\n",
    "        self.layer5 = Conv_2d(n_channels*2, n_channels*2, pooling=2)\n",
    "        self.layer6 = Conv_2d(n_channels*2, n_channels*2, pooling=2)\n",
    "        self.layer7 = Conv_2d(n_channels*2, n_channels*4, pooling=2)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.dense1 = nn.Linear(n_channels*4, n_channels*4)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channels*4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(n_channels*4, n_class)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 1, 128, 137)\n",
    "\n",
    "        # CNN Forward Pass\n",
    "        x = self.layer1(x)  # -> (batch_size, n_channels, H/2, W/2)\n",
    "        x = self.layer2(x)  # -> (batch_size, n_channels, H/4, W/4)\n",
    "        x = self.layer3(x)  # -> (batch_size, n_channels*2, H/8, W/8)\n",
    "        x = self.layer4(x)  # -> (batch_size, n_channels*2, H/16, W/16)\n",
    "        x = self.layer5(x)  # -> (batch_size, n_channels*2, H/32, W/32)\n",
    "        x = self.layer6(x)  # -> (batch_size, n_channels*2, H/64, W/64)\n",
    "        x = self.layer7(x)  # -> (batch_size, n_channels*4, H/128, W/128)\n",
    "\n",
    "        if x.size(3) != 1:\n",
    "            x = nn.MaxPool2d(kernel_size=(1, x.size(3)))(x)\n",
    "        x = x.squeeze(3)  # -> (batch_size, n_channels*4, H/128)\n",
    "\n",
    "        if x.size(2) != 1:\n",
    "            x = nn.MaxPool1d(x.size(2))(x)\n",
    "        x = x.squeeze(2)  # -> (batch_size, n_channels*4)\n",
    "\n",
    "        x = self.dense1(x)          # -> (batch_size, n_channels*4)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)          # -> (batch_size, n_class)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a981dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with GPU\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load the x and y into the data loader\n",
    "train_loader = DataLoader(dataset=list(zip(x, y)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = ShortChunkCNN(n_channels=1, n_class=11)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# 訓練迴圈\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batch_idx = 0\n",
    "    for inputs, integer_labels in train_loader:\n",
    "        # 假設 inputs 的形狀為 (batch_size, 128, 137)\n",
    "        inputs = inputs.unsqueeze(1)  # -> (batch_size, 1, 128, 137)\n",
    "        # labels: from 0 to 10\n",
    "        labels = integer_labels.squeeze() # -> (batch_size)\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # -> (batch_size, n_class)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print batch number \n",
    "        print(f'batch number: {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "        batch_idx += 1\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'mel_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
