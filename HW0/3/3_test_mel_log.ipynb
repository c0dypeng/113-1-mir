{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e51f0e",
   "metadata": {},
   "source": [
    "### Task3: Deep Learning Model\n",
    "\n",
    "- Train a deep learning model (e.g. CNN or attention based model) with Mel-spectrograms extracted from the audio as input\n",
    "\n",
    "- Need to compare 2 different kinds of inputs: Mel-spectrograms with or without taking the log\n",
    "\n",
    "- You can choose whatever FFT window size and hop length you like\n",
    "\n",
    "- You can choose whatever deep learning model you like\n",
    "\n",
    "- Need to report how to implement the model clearly\n",
    "\n",
    "- Need to report the testing result (not validation result) with confusion matrix, top1 accuracy, and top3 accuracy\n",
    "\n",
    "- You can use any music tagging model. For a novice, the short chunk CNN in this repo is recommended. (Need to replace the BCE loss to Cross-entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "536f6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_data_path = '<PUT THE PATH TO THE TEST DATA HERE>'\n",
    "# test_data_path = '../nsynth-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "596aba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import librosa\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db58b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file\n",
    "def load_json(json_file):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fe41d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(key, file_path):\n",
    "    y, sr = librosa.load(file_path)\n",
    "\n",
    "    # FFT window size=2048, and the hop length=512\n",
    "    # extract the mel spectrogram feature\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=512, hop_length=512)\n",
    "\n",
    "    # extract the mel spectrogram feature with log scaling\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "\n",
    "    # put all features into a list\n",
    "    features = [mel_spectrogram, log_mel_spectrogram]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4b205b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = pickle.load(open('label_encoder_mel_log.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ea81a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_2d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size=3, stride=1, pooling=2):\n",
    "        super(Conv_2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size, stride=stride, padding=kernel_size//2)\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mp = nn.MaxPool2d(pooling)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.mp(self.relu(self.bn(self.conv(x))))\n",
    "        return out\n",
    "\n",
    "class ShortChunkCNN(nn.Module):\n",
    "    '''\n",
    "    Short-chunk CNN architecture.\n",
    "    So-called VGG-like model with a small receptive field.\n",
    "    Deeper layers, smaller pooling (2x2).\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 n_channels=1,\n",
    "                 n_class=11):\n",
    "        super(ShortChunkCNN, self).__init__()\n",
    "\n",
    "        # CNN Layers\n",
    "        self.layer1 = Conv_2d(1, n_channels, pooling=2)\n",
    "        self.layer2 = Conv_2d(n_channels, n_channels, pooling=2)\n",
    "        self.layer3 = Conv_2d(n_channels, n_channels*2, pooling=2)\n",
    "        self.layer4 = Conv_2d(n_channels*2, n_channels*2, pooling=2)\n",
    "        self.layer5 = Conv_2d(n_channels*2, n_channels*2, pooling=2)\n",
    "        self.layer6 = Conv_2d(n_channels*2, n_channels*2, pooling=2)\n",
    "        self.layer7 = Conv_2d(n_channels*2, n_channels*4, pooling=2)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.dense1 = nn.Linear(n_channels*4, n_channels*4)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channels*4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(n_channels*4, n_class)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 1, 128, 137)\n",
    "\n",
    "        # CNN Forward Pass\n",
    "        x = self.layer1(x)  # -> (batch_size, n_channels, H/2, W/2)\n",
    "        x = self.layer2(x)  # -> (batch_size, n_channels, H/4, W/4)\n",
    "        x = self.layer3(x)  # -> (batch_size, n_channels*2, H/8, W/8)\n",
    "        x = self.layer4(x)  # -> (batch_size, n_channels*2, H/16, W/16)\n",
    "        x = self.layer5(x)  # -> (batch_size, n_channels*2, H/32, W/32)\n",
    "        x = self.layer6(x)  # -> (batch_size, n_channels*2, H/64, W/64)\n",
    "        x = self.layer7(x)  # -> (batch_size, n_channels*4, H/128, W/128)\n",
    "\n",
    "        if x.size(3) != 1:\n",
    "            x = nn.MaxPool2d(kernel_size=(1, x.size(3)))(x)\n",
    "        x = x.squeeze(3)  # -> (batch_size, n_channels*4, H/128)\n",
    "\n",
    "        if x.size(2) != 1:\n",
    "            x = nn.MaxPool1d(x.size(2))(x)\n",
    "        x = x.squeeze(2)  # -> (batch_size, n_channels*4)\n",
    "\n",
    "        x = self.dense1(x)          # -> (batch_size, n_channels*4)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)          # -> (batch_size, n_class)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cc51d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_22545/3024793992.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('mel_model_log.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the model\n",
    "model = ShortChunkCNN(n_channels=1, n_class=11)\n",
    "model.load_state_dict(torch.load('mel_model_log.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b170b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# load the test data\n",
    "test_data = load_json(os.path.join(test_data_path, 'examples.json'))\n",
    "test_keys = list(test_data.keys())\n",
    "test_features = []\n",
    "for key in test_keys:\n",
    "    file = os.path.join(test_data_path, 'audio', key + '.wav')\n",
    "    feature = feature_extraction(key, file)\n",
    "    test_features.append(feature)\n",
    "\n",
    "test_mel_spectrogram = [f[1] for f in test_features]\n",
    "\n",
    "test_labels = [test_data[key][\"instrument_family_str\"] for key in test_keys]\n",
    "test_integer_encoded = label_encoder.fit_transform(test_labels)\n",
    "test_integer_encoded = test_integer_encoded.reshape(-1, 1)\n",
    "\n",
    "# Reshape integer_encoded to 2D array (necessary for OneHotEncoder)\n",
    "test_integer_encoded = test_integer_encoded.reshape(-1, 1)\n",
    "\n",
    "test_x = np.array(test_mel_spectrogram)\n",
    "# test_x = np.expand_dims(test_x, axis=1)\n",
    "test_y = test_integer_encoded\n",
    "\n",
    "# load the test data into the data loader\n",
    "test_loader = DataLoader(dataset=list(zip(test_x, test_y)), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "234eeec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 128, 173)\n",
      "(4096, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1a7806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 14.38%\n",
      "Top-3 Accuracy: 34.30%\n",
      "Confusion Matrix:\n",
      "[[  0   0   0 460   0 243   0   0   0  14 126]\n",
      " [  0   0   0  95   0   0   0   0   0  11 163]\n",
      " [  0   0   0  50   0   4   0   0   0  35  91]\n",
      " [  0   0   0 390   0 201   0   0   0  30  31]\n",
      " [  0   0   0 332   0 267   0   0   0  99  68]\n",
      " [  0   0   0   7   0 191   0   0   0   4   0]\n",
      " [  0   0   0  70   0  78   0   0   0  73 281]\n",
      " [  0   0   0 183   0   0   0   0   0  20  32]\n",
      " [  0   0   0  91   0  56   0   0   0   6 153]\n",
      " [  0   0   0  15   0   4   0   0   0   8 114]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    top1_correct = 0\n",
    "    top3_correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch_x, batch_y in test_loader:\n",
    "\n",
    "            batch_x = batch_x.unsqueeze(1)  # -> (batch_size, 1, 128, 137)\n",
    "            batch_y = batch_y.squeeze() # -> (batch_size)\n",
    "\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # Get Top-1 predictions\n",
    "            _, top1_pred = torch.max(outputs, dim=1)\n",
    "            \n",
    "            # Get Top-3 predictions\n",
    "            _, top3_pred = torch.topk(outputs, k=3, dim=1)\n",
    "            \n",
    "            # Compute Top-1 accuracy\n",
    "            top1_correct += (top1_pred == batch_y.squeeze()).sum().item()\n",
    "            \n",
    "            # Compute Top-3 accuracy\n",
    "            top3_correct += (batch_y.squeeze().unsqueeze(1) == top3_pred).sum().item()\n",
    "\n",
    "            # Collect predictions and true labels for confusion matrix\n",
    "            all_preds.extend(top1_pred.cpu().numpy())\n",
    "            all_labels.extend(batch_y.squeeze().cpu().numpy())\n",
    "            \n",
    "            total += batch_y.size(0)\n",
    "\n",
    "    # Calculate accuracies\n",
    "    top1_accuracy = top1_correct / total\n",
    "    top3_accuracy = top3_correct / total\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return top1_accuracy, top3_accuracy, conf_matrix\n",
    "\n",
    "\n",
    "# Example usage of the evaluate function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model (assuming model has already been defined and trained)\n",
    "model.to(device)\n",
    "\n",
    "# Load the test data into the data loader\n",
    "test_loader = DataLoader(dataset=list(zip(test_x, test_y)), batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "top1_acc, top3_acc, conf_matrix = evaluate(model, test_loader, device)\n",
    "\n",
    "# Print results\n",
    "print(f\"Top-1 Accuracy: {top1_acc * 100:.2f}%\")\n",
    "print(f\"Top-3 Accuracy: {top3_acc * 100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc253fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
