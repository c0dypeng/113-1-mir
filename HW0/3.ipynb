{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e51f0e",
   "metadata": {},
   "source": [
    "### Task3: Deep Learning Model\n",
    "\n",
    "- Train a deep learning model (e.g. CNN or attention based model) with Mel-spectrograms extracted from the audio as input\n",
    "\n",
    "- Need to compare 2 different kinds of inputs: Mel-spectrograms with or without taking the log\n",
    "\n",
    "- You can choose whatever FFT window size and hop length you like\n",
    "\n",
    "- You can choose whatever deep learning model you like\n",
    "\n",
    "- Need to report how to implement the model clearly\n",
    "\n",
    "- Need to report the testing result (not validation result) with confusion matrix, top1 accuracy, and top3 accuracy\n",
    "\n",
    "- You can use any music tagging model. For a novice, the short chunk CNN in this repo is recommended. (Need to replace the BCE loss to Cross-entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "536f6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset file path:\n",
    "# traning_data_path = '<PUT THE PATH TO THE TRAINING DATA HERE>'\n",
    "\n",
    "traning_data_path = 'nsynth-subtrain'\n",
    "\n",
    "# test_data_path = '<PUT THE PATH TO THE TEST DATA HERE>'\n",
    "test_data_path = 'nsynth-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "596aba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn import metrics\n",
    "import librosa\n",
    "import torchaudio\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "import joblib\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db58b348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flute', 'keyboard', 'organ', 'brass', 'vocal', 'reed', 'mallet', 'string', 'guitar', 'bass', 'synth_lead'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['instrument_family_str.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the json file\n",
    "def load_json(json_file):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# let the json path be /examples.json under the \"traning_data_path\"\n",
    "json_path = os.path.join(traning_data_path, 'examples.json')\n",
    "\n",
    "data = load_json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44de218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all keys in data\n",
    "keys = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fe41d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(key, file_path):\n",
    "    y, sr = librosa.load(file_path)\n",
    "\n",
    "    # FFT window size=2048, and the hop length=512\n",
    "    # extract the mel spectrogram feature\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=512, hop_length=512)\n",
    "\n",
    "    # extract the mel spectrogram feature with log scaling\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "\n",
    "    # put all features into a list\n",
    "    features = [mel_spectrogram, log_mel_spectrogram]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97f0ef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48037/48037 [03:43<00:00, 214.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "# extract the features from each audio file\n",
    "\n",
    "features = []\n",
    "\n",
    "# for file in keys:\n",
    "for key in tqdm.tqdm(keys):\n",
    "    file = os.path.join(traning_data_path, 'audio', key + '.wav')\n",
    "    # extract the features\n",
    "    feature = feature_extraction(key, file)\n",
    "    features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c3ede72",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = [f[0] for f in features]\n",
    "log_mel_spectrogram = [f[1] for f in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a84b9e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 173)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of the features\n",
    "print(mel_spectrogram[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4b205b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one hot encoding of the labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Extract labels from the data\n",
    "labels = [data[key][\"instrument_family_str\"] for key in keys]\n",
    "\n",
    "# Initialize the LabelEncoder and OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convert string labels to integer labels\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Reshape integer_encoded to 2D array (necessary for OneHotEncoder)\n",
    "integer_encoded = integer_encoded.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92bd4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(mel_spectrogram)\n",
    "y = integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "939d7d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 5005, 5: 4674, 3: 4633, 9: 4580, 4: 4535, 10: 4520, 6: 4499, 7: 4232, 2: 4118, 8: 3830, 1: 3411})\n"
     ]
    }
   ],
   "source": [
    "# print the count of each number in y\n",
    "from collections import Counter\n",
    "print(Counter(y.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ea81a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_2d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size=3, stride=1, pooling=2):\n",
    "        super(Conv_2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size, stride=stride, padding=kernel_size//2)\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mp = nn.MaxPool2d(pooling)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.mp(self.relu(self.bn(self.conv(x))))\n",
    "        return out\n",
    "\n",
    "class ShortChunkCNN(nn.Module):\n",
    "    '''\n",
    "    Short-chunk CNN architecture.\n",
    "    So-called VGG-like model with a small receptive field.\n",
    "    Deeper layers, smaller pooling (2x2).\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 n_channels=128,\n",
    "                 n_class=11):\n",
    "        super(ShortChunkCNN, self).__init__()\n",
    "\n",
    "        # CNN Layers\n",
    "        self.layer1 = Conv_2d(1, n_channels, pooling=2)\n",
    "        self.layer2 = Conv_2d(n_channels, n_channels, pooling=2)\n",
    "        self.layer3 = Conv_2d(n_channels, n_channels*2, pooling=2)\n",
    "        self.layer4 = Conv_2d(n_channels*2, n_channels*2, pooling=2)\n",
    "        self.layer5 = Conv_2d(n_channels*2, n_channels*2, pooling=2)\n",
    "        self.layer6 = Conv_2d(n_channels*2, n_channels*2, pooling=2)\n",
    "        self.layer7 = Conv_2d(n_channels*2, n_channels*4, pooling=2)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.dense1 = nn.Linear(n_channels*4, n_channels*4)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channels*4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(n_channels*4, n_class)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 1, 128, 137)\n",
    "\n",
    "        # CNN Forward Pass\n",
    "        x = self.layer1(x)  # -> (batch_size, n_channels, H/2, W/2)\n",
    "        x = self.layer2(x)  # -> (batch_size, n_channels, H/4, W/4)\n",
    "        x = self.layer3(x)  # -> (batch_size, n_channels*2, H/8, W/8)\n",
    "        x = self.layer4(x)  # -> (batch_size, n_channels*2, H/16, W/16)\n",
    "        x = self.layer5(x)  # -> (batch_size, n_channels*2, H/32, W/32)\n",
    "        x = self.layer6(x)  # -> (batch_size, n_channels*2, H/64, W/64)\n",
    "        x = self.layer7(x)  # -> (batch_size, n_channels*4, H/128, W/128)\n",
    "\n",
    "        # 確保特徵圖的寬度為1，進行全局池化\n",
    "        if x.size(3) != 1:\n",
    "            x = nn.MaxPool2d(kernel_size=(1, x.size(3)))(x)\n",
    "        x = x.squeeze(3)  # -> (batch_size, n_channels*4, H/128)\n",
    "\n",
    "        # 全局池化後，如果高度仍大於1，進行一次全局池化\n",
    "        if x.size(2) != 1:\n",
    "            x = nn.MaxPool1d(x.size(2))(x)\n",
    "        x = x.squeeze(2)  # -> (batch_size, n_channels*4)\n",
    "\n",
    "        # 全連接層\n",
    "        x = self.dense1(x)          # -> (batch_size, n_channels*4)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)          # -> (batch_size, n_class)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a981dfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch number: 1/3003, Loss: 2.3807\n",
      "batch number: 2/3003, Loss: 2.4101\n",
      "batch number: 3/3003, Loss: 2.3950\n",
      "batch number: 4/3003, Loss: 2.4274\n",
      "batch number: 5/3003, Loss: 2.3892\n",
      "batch number: 6/3003, Loss: 2.3949\n",
      "batch number: 7/3003, Loss: 2.3652\n",
      "batch number: 8/3003, Loss: 2.3548\n",
      "batch number: 9/3003, Loss: 2.3890\n",
      "batch number: 10/3003, Loss: 2.3623\n",
      "batch number: 11/3003, Loss: 2.3992\n",
      "batch number: 12/3003, Loss: 2.3844\n",
      "batch number: 13/3003, Loss: 2.3917\n",
      "batch number: 14/3003, Loss: 2.4079\n",
      "batch number: 15/3003, Loss: 2.4107\n",
      "batch number: 16/3003, Loss: 2.3846\n",
      "batch number: 17/3003, Loss: 2.3555\n",
      "batch number: 18/3003, Loss: 2.4159\n",
      "batch number: 19/3003, Loss: 2.3334\n",
      "batch number: 20/3003, Loss: 2.3416\n",
      "batch number: 21/3003, Loss: 2.3795\n",
      "batch number: 22/3003, Loss: 2.3930\n",
      "batch number: 23/3003, Loss: 2.4102\n",
      "batch number: 24/3003, Loss: 2.3633\n",
      "batch number: 25/3003, Loss: 2.3737\n",
      "batch number: 26/3003, Loss: 2.3418\n",
      "batch number: 27/3003, Loss: 2.3861\n",
      "batch number: 28/3003, Loss: 2.3715\n",
      "batch number: 29/3003, Loss: 2.3642\n",
      "batch number: 30/3003, Loss: 2.2920\n",
      "batch number: 31/3003, Loss: 2.3960\n",
      "batch number: 32/3003, Loss: 2.3284\n",
      "batch number: 33/3003, Loss: 2.3879\n",
      "batch number: 34/3003, Loss: 2.3823\n",
      "batch number: 35/3003, Loss: 2.3661\n",
      "batch number: 36/3003, Loss: 2.3876\n",
      "batch number: 37/3003, Loss: 2.4197\n",
      "batch number: 38/3003, Loss: 2.4024\n",
      "batch number: 39/3003, Loss: 2.3968\n",
      "batch number: 40/3003, Loss: 2.4217\n",
      "batch number: 41/3003, Loss: 2.4269\n",
      "batch number: 42/3003, Loss: 2.3581\n",
      "batch number: 43/3003, Loss: 2.3707\n",
      "batch number: 44/3003, Loss: 2.3721\n",
      "batch number: 45/3003, Loss: 2.3629\n",
      "batch number: 46/3003, Loss: 2.4127\n",
      "batch number: 47/3003, Loss: 2.3888\n",
      "batch number: 48/3003, Loss: 2.3762\n",
      "batch number: 49/3003, Loss: 2.3910\n",
      "batch number: 50/3003, Loss: 2.3926\n",
      "batch number: 51/3003, Loss: 2.3797\n",
      "batch number: 52/3003, Loss: 2.3618\n",
      "batch number: 53/3003, Loss: 2.3797\n",
      "batch number: 54/3003, Loss: 2.3338\n",
      "batch number: 55/3003, Loss: 2.3250\n",
      "batch number: 56/3003, Loss: 2.4309\n",
      "batch number: 57/3003, Loss: 2.3603\n",
      "batch number: 58/3003, Loss: 2.3830\n",
      "batch number: 59/3003, Loss: 2.3907\n",
      "batch number: 60/3003, Loss: 2.4379\n",
      "batch number: 61/3003, Loss: 2.2868\n",
      "batch number: 62/3003, Loss: 2.3512\n",
      "batch number: 63/3003, Loss: 2.4430\n",
      "batch number: 64/3003, Loss: 2.3525\n",
      "batch number: 65/3003, Loss: 2.4070\n",
      "batch number: 66/3003, Loss: 2.4136\n",
      "batch number: 67/3003, Loss: 2.2741\n",
      "batch number: 68/3003, Loss: 2.3684\n",
      "batch number: 69/3003, Loss: 2.3366\n",
      "batch number: 70/3003, Loss: 2.4105\n",
      "batch number: 71/3003, Loss: 2.3248\n",
      "batch number: 72/3003, Loss: 2.3892\n",
      "batch number: 73/3003, Loss: 2.3850\n",
      "batch number: 74/3003, Loss: 2.3183\n",
      "batch number: 75/3003, Loss: 2.3496\n",
      "batch number: 76/3003, Loss: 2.3089\n",
      "batch number: 77/3003, Loss: 2.3502\n",
      "batch number: 78/3003, Loss: 2.2809\n",
      "batch number: 79/3003, Loss: 2.3875\n",
      "batch number: 80/3003, Loss: 2.3878\n",
      "batch number: 81/3003, Loss: 2.3924\n",
      "batch number: 82/3003, Loss: 2.3618\n",
      "batch number: 83/3003, Loss: 2.3341\n",
      "batch number: 84/3003, Loss: 2.3542\n",
      "batch number: 85/3003, Loss: 2.3277\n",
      "batch number: 86/3003, Loss: 2.3405\n",
      "batch number: 87/3003, Loss: 2.3593\n",
      "batch number: 88/3003, Loss: 2.4574\n",
      "batch number: 89/3003, Loss: 2.3694\n",
      "batch number: 90/3003, Loss: 2.3938\n",
      "batch number: 91/3003, Loss: 2.3555\n",
      "batch number: 92/3003, Loss: 2.3490\n",
      "batch number: 93/3003, Loss: 2.4061\n",
      "batch number: 94/3003, Loss: 2.2796\n",
      "batch number: 95/3003, Loss: 2.3119\n",
      "batch number: 96/3003, Loss: 2.3913\n",
      "batch number: 97/3003, Loss: 2.4309\n",
      "batch number: 98/3003, Loss: 2.3893\n",
      "batch number: 99/3003, Loss: 2.3983\n",
      "batch number: 100/3003, Loss: 2.3872\n",
      "batch number: 101/3003, Loss: 2.4288\n",
      "batch number: 102/3003, Loss: 2.3882\n",
      "batch number: 103/3003, Loss: 2.3462\n",
      "batch number: 104/3003, Loss: 2.2903\n",
      "batch number: 105/3003, Loss: 2.2224\n",
      "batch number: 106/3003, Loss: 2.3772\n",
      "batch number: 107/3003, Loss: 2.3563\n",
      "batch number: 108/3003, Loss: 2.3341\n",
      "batch number: 109/3003, Loss: 2.3999\n",
      "batch number: 110/3003, Loss: 2.3979\n",
      "batch number: 111/3003, Loss: 2.3742\n",
      "batch number: 112/3003, Loss: 2.3951\n",
      "batch number: 113/3003, Loss: 2.3878\n",
      "batch number: 114/3003, Loss: 2.3470\n",
      "batch number: 115/3003, Loss: 2.3435\n",
      "batch number: 116/3003, Loss: 2.3557\n",
      "batch number: 117/3003, Loss: 2.3384\n",
      "batch number: 118/3003, Loss: 2.2756\n",
      "batch number: 119/3003, Loss: 2.3466\n",
      "batch number: 120/3003, Loss: 2.3503\n",
      "batch number: 121/3003, Loss: 2.4023\n",
      "batch number: 122/3003, Loss: 2.2653\n",
      "batch number: 123/3003, Loss: 2.3711\n",
      "batch number: 124/3003, Loss: 2.3865\n",
      "batch number: 125/3003, Loss: 2.2210\n",
      "batch number: 126/3003, Loss: 2.3484\n",
      "batch number: 127/3003, Loss: 2.2709\n",
      "batch number: 128/3003, Loss: 2.3433\n",
      "batch number: 129/3003, Loss: 2.2457\n",
      "batch number: 130/3003, Loss: 2.2840\n",
      "batch number: 131/3003, Loss: 2.3760\n",
      "batch number: 132/3003, Loss: 2.3845\n",
      "batch number: 133/3003, Loss: 2.3717\n",
      "batch number: 134/3003, Loss: 2.2447\n",
      "batch number: 135/3003, Loss: 2.3773\n",
      "batch number: 136/3003, Loss: 2.3317\n",
      "batch number: 137/3003, Loss: 2.3022\n",
      "batch number: 138/3003, Loss: 2.3776\n",
      "batch number: 139/3003, Loss: 2.2614\n",
      "batch number: 140/3003, Loss: 2.1538\n",
      "batch number: 141/3003, Loss: 2.3575\n",
      "batch number: 142/3003, Loss: 2.4214\n",
      "batch number: 143/3003, Loss: 2.3428\n",
      "batch number: 144/3003, Loss: 2.1850\n",
      "batch number: 145/3003, Loss: 2.2579\n",
      "batch number: 146/3003, Loss: 2.3061\n",
      "batch number: 147/3003, Loss: 2.1858\n",
      "batch number: 148/3003, Loss: 2.3340\n",
      "batch number: 149/3003, Loss: 2.3972\n",
      "batch number: 150/3003, Loss: 2.3347\n",
      "batch number: 151/3003, Loss: 2.2966\n",
      "batch number: 152/3003, Loss: 2.2536\n",
      "batch number: 153/3003, Loss: 2.3259\n",
      "batch number: 154/3003, Loss: 2.2095\n",
      "batch number: 155/3003, Loss: 2.4676\n",
      "batch number: 156/3003, Loss: 2.3114\n",
      "batch number: 157/3003, Loss: 2.2999\n",
      "batch number: 158/3003, Loss: 2.3865\n",
      "batch number: 159/3003, Loss: 2.3300\n",
      "batch number: 160/3003, Loss: 2.3716\n",
      "batch number: 161/3003, Loss: 2.3153\n",
      "batch number: 162/3003, Loss: 2.4143\n",
      "batch number: 163/3003, Loss: 2.2935\n",
      "batch number: 164/3003, Loss: 2.4154\n",
      "batch number: 165/3003, Loss: 2.3015\n",
      "batch number: 166/3003, Loss: 2.4227\n",
      "batch number: 167/3003, Loss: 2.3312\n",
      "batch number: 168/3003, Loss: 2.3751\n",
      "batch number: 169/3003, Loss: 2.2907\n",
      "batch number: 170/3003, Loss: 2.3977\n",
      "batch number: 171/3003, Loss: 2.4320\n",
      "batch number: 172/3003, Loss: 2.4753\n",
      "batch number: 173/3003, Loss: 2.3252\n",
      "batch number: 174/3003, Loss: 2.2952\n",
      "batch number: 175/3003, Loss: 2.3973\n",
      "batch number: 176/3003, Loss: 2.3766\n",
      "batch number: 177/3003, Loss: 2.4070\n",
      "batch number: 178/3003, Loss: 2.3333\n",
      "batch number: 179/3003, Loss: 2.1789\n",
      "batch number: 180/3003, Loss: 2.2835\n",
      "batch number: 181/3003, Loss: 2.2475\n",
      "batch number: 182/3003, Loss: 2.3633\n",
      "batch number: 183/3003, Loss: 2.2122\n",
      "batch number: 184/3003, Loss: 2.3443\n",
      "batch number: 185/3003, Loss: 2.3949\n",
      "batch number: 186/3003, Loss: 2.2780\n",
      "batch number: 187/3003, Loss: 2.3462\n",
      "batch number: 188/3003, Loss: 2.2679\n",
      "batch number: 189/3003, Loss: 2.2039\n",
      "batch number: 190/3003, Loss: 2.4135\n",
      "batch number: 191/3003, Loss: 2.3517\n",
      "batch number: 192/3003, Loss: 2.4150\n",
      "batch number: 193/3003, Loss: 2.4085\n",
      "batch number: 194/3003, Loss: 2.3087\n",
      "batch number: 195/3003, Loss: 2.3750\n",
      "batch number: 196/3003, Loss: 2.3707\n",
      "batch number: 197/3003, Loss: 2.4786\n",
      "batch number: 198/3003, Loss: 2.2367\n",
      "batch number: 199/3003, Loss: 2.2588\n",
      "batch number: 200/3003, Loss: 2.3066\n",
      "batch number: 201/3003, Loss: 2.2660\n",
      "batch number: 202/3003, Loss: 2.3226\n",
      "batch number: 203/3003, Loss: 2.3673\n",
      "batch number: 204/3003, Loss: 2.3005\n",
      "batch number: 205/3003, Loss: 2.3287\n",
      "batch number: 206/3003, Loss: 2.3903\n",
      "batch number: 207/3003, Loss: 2.3260\n",
      "batch number: 208/3003, Loss: 2.3442\n",
      "batch number: 209/3003, Loss: 2.3259\n",
      "batch number: 210/3003, Loss: 2.3625\n",
      "batch number: 211/3003, Loss: 2.3551\n",
      "batch number: 212/3003, Loss: 2.3553\n",
      "batch number: 213/3003, Loss: 2.2022\n",
      "batch number: 214/3003, Loss: 2.3488\n",
      "batch number: 215/3003, Loss: 2.4101\n",
      "batch number: 216/3003, Loss: 2.2557\n",
      "batch number: 217/3003, Loss: 2.2545\n",
      "batch number: 218/3003, Loss: 2.3594\n",
      "batch number: 219/3003, Loss: 2.3374\n",
      "batch number: 220/3003, Loss: 2.3101\n",
      "batch number: 221/3003, Loss: 2.2578\n",
      "batch number: 222/3003, Loss: 2.3594\n",
      "batch number: 223/3003, Loss: 2.3956\n",
      "batch number: 224/3003, Loss: 2.2639\n",
      "batch number: 225/3003, Loss: 2.3690\n",
      "batch number: 226/3003, Loss: 2.2914\n",
      "batch number: 227/3003, Loss: 2.3597\n",
      "batch number: 228/3003, Loss: 2.1924\n",
      "batch number: 229/3003, Loss: 2.3893\n",
      "batch number: 230/3003, Loss: 2.2846\n",
      "batch number: 231/3003, Loss: 2.4118\n",
      "batch number: 232/3003, Loss: 2.3425\n",
      "batch number: 233/3003, Loss: 2.3900\n",
      "batch number: 234/3003, Loss: 2.4514\n",
      "batch number: 235/3003, Loss: 2.4751\n",
      "batch number: 236/3003, Loss: 2.2467\n",
      "batch number: 237/3003, Loss: 2.1099\n",
      "batch number: 238/3003, Loss: 2.3512\n",
      "batch number: 239/3003, Loss: 2.2608\n",
      "batch number: 240/3003, Loss: 2.2268\n",
      "batch number: 241/3003, Loss: 2.3899\n",
      "batch number: 242/3003, Loss: 2.2463\n",
      "batch number: 243/3003, Loss: 2.4517\n",
      "batch number: 244/3003, Loss: 2.3766\n",
      "batch number: 245/3003, Loss: 2.4211\n",
      "batch number: 246/3003, Loss: 2.1630\n",
      "batch number: 247/3003, Loss: 2.2237\n",
      "batch number: 248/3003, Loss: 2.5058\n",
      "batch number: 249/3003, Loss: 2.3514\n",
      "batch number: 250/3003, Loss: 2.2517\n",
      "batch number: 251/3003, Loss: 2.3556\n",
      "batch number: 252/3003, Loss: 2.3263\n",
      "batch number: 253/3003, Loss: 2.4352\n",
      "batch number: 254/3003, Loss: 2.3434\n",
      "batch number: 255/3003, Loss: 2.3721\n",
      "batch number: 256/3003, Loss: 2.3132\n",
      "batch number: 257/3003, Loss: 2.2818\n",
      "batch number: 258/3003, Loss: 2.2836\n",
      "batch number: 259/3003, Loss: 2.3561\n",
      "batch number: 260/3003, Loss: 2.3021\n",
      "batch number: 261/3003, Loss: 2.3710\n",
      "batch number: 262/3003, Loss: 2.3696\n",
      "batch number: 263/3003, Loss: 2.3819\n",
      "batch number: 264/3003, Loss: 2.2508\n",
      "batch number: 265/3003, Loss: 2.0596\n",
      "batch number: 266/3003, Loss: 2.2399\n",
      "batch number: 267/3003, Loss: 2.4822\n",
      "batch number: 268/3003, Loss: 2.3418\n",
      "batch number: 269/3003, Loss: 2.2276\n",
      "batch number: 270/3003, Loss: 2.2359\n",
      "batch number: 271/3003, Loss: 2.3281\n",
      "batch number: 272/3003, Loss: 2.3541\n",
      "batch number: 273/3003, Loss: 2.3520\n",
      "batch number: 274/3003, Loss: 2.3656\n",
      "batch number: 275/3003, Loss: 2.3083\n",
      "batch number: 276/3003, Loss: 2.3088\n",
      "batch number: 277/3003, Loss: 2.1949\n",
      "batch number: 278/3003, Loss: 2.2689\n",
      "batch number: 279/3003, Loss: 2.1456\n",
      "batch number: 280/3003, Loss: 2.3350\n",
      "batch number: 281/3003, Loss: 2.3266\n",
      "batch number: 282/3003, Loss: 2.4292\n",
      "batch number: 283/3003, Loss: 2.2429\n",
      "batch number: 284/3003, Loss: 2.4488\n",
      "batch number: 285/3003, Loss: 2.4073\n",
      "batch number: 286/3003, Loss: 2.4398\n",
      "batch number: 287/3003, Loss: 2.3084\n",
      "batch number: 288/3003, Loss: 2.2500\n",
      "batch number: 289/3003, Loss: 2.3606\n",
      "batch number: 290/3003, Loss: 2.3549\n",
      "batch number: 291/3003, Loss: 2.2807\n",
      "batch number: 292/3003, Loss: 2.2115\n",
      "batch number: 293/3003, Loss: 2.3091\n",
      "batch number: 294/3003, Loss: 2.3731\n",
      "batch number: 295/3003, Loss: 2.3956\n",
      "batch number: 296/3003, Loss: 2.3821\n",
      "batch number: 297/3003, Loss: 2.3224\n",
      "batch number: 298/3003, Loss: 2.1949\n",
      "batch number: 299/3003, Loss: 2.4860\n",
      "batch number: 300/3003, Loss: 2.2842\n",
      "batch number: 301/3003, Loss: 2.1482\n",
      "batch number: 302/3003, Loss: 2.2033\n",
      "batch number: 303/3003, Loss: 2.4421\n",
      "batch number: 304/3003, Loss: 2.3683\n",
      "batch number: 305/3003, Loss: 2.1974\n",
      "batch number: 306/3003, Loss: 2.2885\n",
      "batch number: 307/3003, Loss: 2.2292\n",
      "batch number: 308/3003, Loss: 2.1659\n",
      "batch number: 309/3003, Loss: 2.3568\n",
      "batch number: 310/3003, Loss: 2.3806\n",
      "batch number: 311/3003, Loss: 2.3041\n",
      "batch number: 312/3003, Loss: 2.2008\n",
      "batch number: 313/3003, Loss: 2.3262\n",
      "batch number: 314/3003, Loss: 2.4332\n",
      "batch number: 315/3003, Loss: 2.3333\n",
      "batch number: 316/3003, Loss: 2.4764\n",
      "batch number: 317/3003, Loss: 2.2403\n",
      "batch number: 318/3003, Loss: 2.3431\n",
      "batch number: 319/3003, Loss: 2.4177\n",
      "batch number: 320/3003, Loss: 2.1849\n",
      "batch number: 321/3003, Loss: 2.2933\n",
      "batch number: 322/3003, Loss: 2.1715\n",
      "batch number: 323/3003, Loss: 2.3697\n",
      "batch number: 324/3003, Loss: 2.4511\n",
      "batch number: 325/3003, Loss: 2.3074\n",
      "batch number: 326/3003, Loss: 2.3921\n",
      "batch number: 327/3003, Loss: 2.2548\n",
      "batch number: 328/3003, Loss: 2.3742\n",
      "batch number: 329/3003, Loss: 2.3599\n",
      "batch number: 330/3003, Loss: 2.4255\n",
      "batch number: 331/3003, Loss: 2.4072\n",
      "batch number: 332/3003, Loss: 2.3173\n",
      "batch number: 333/3003, Loss: 2.2667\n",
      "batch number: 334/3003, Loss: 2.2751\n",
      "batch number: 335/3003, Loss: 2.2542\n",
      "batch number: 336/3003, Loss: 2.4295\n",
      "batch number: 337/3003, Loss: 2.1836\n",
      "batch number: 338/3003, Loss: 2.3847\n",
      "batch number: 339/3003, Loss: 2.3557\n",
      "batch number: 340/3003, Loss: 2.2141\n",
      "batch number: 341/3003, Loss: 2.3888\n",
      "batch number: 342/3003, Loss: 2.3548\n",
      "batch number: 343/3003, Loss: 2.3258\n",
      "batch number: 344/3003, Loss: 2.3924\n",
      "batch number: 345/3003, Loss: 2.3050\n",
      "batch number: 346/3003, Loss: 2.4082\n",
      "batch number: 347/3003, Loss: 2.3790\n",
      "batch number: 348/3003, Loss: 2.3024\n",
      "batch number: 349/3003, Loss: 2.3541\n",
      "batch number: 350/3003, Loss: 2.3875\n",
      "batch number: 351/3003, Loss: 2.3626\n",
      "batch number: 352/3003, Loss: 2.1890\n",
      "batch number: 353/3003, Loss: 2.3150\n",
      "batch number: 354/3003, Loss: 2.4148\n",
      "batch number: 355/3003, Loss: 2.2379\n",
      "batch number: 356/3003, Loss: 2.2442\n",
      "batch number: 357/3003, Loss: 2.2182\n",
      "batch number: 358/3003, Loss: 2.3273\n",
      "batch number: 359/3003, Loss: 2.3068\n",
      "batch number: 360/3003, Loss: 2.2717\n",
      "batch number: 361/3003, Loss: 2.1703\n",
      "batch number: 362/3003, Loss: 2.2885\n",
      "batch number: 363/3003, Loss: 2.2957\n",
      "batch number: 364/3003, Loss: 2.2494\n",
      "batch number: 365/3003, Loss: 2.4039\n",
      "batch number: 366/3003, Loss: 2.3040\n",
      "batch number: 367/3003, Loss: 2.2237\n",
      "batch number: 368/3003, Loss: 2.2329\n",
      "batch number: 369/3003, Loss: 2.3192\n",
      "batch number: 370/3003, Loss: 2.2606\n",
      "batch number: 371/3003, Loss: 2.1467\n",
      "batch number: 372/3003, Loss: 2.4043\n",
      "batch number: 373/3003, Loss: 2.2865\n",
      "batch number: 374/3003, Loss: 2.2714\n",
      "batch number: 375/3003, Loss: 2.3265\n",
      "batch number: 376/3003, Loss: 2.3976\n",
      "batch number: 377/3003, Loss: 2.3036\n",
      "batch number: 378/3003, Loss: 2.4066\n",
      "batch number: 379/3003, Loss: 2.4396\n",
      "batch number: 380/3003, Loss: 2.2463\n",
      "batch number: 381/3003, Loss: 2.3452\n",
      "batch number: 382/3003, Loss: 2.1528\n",
      "batch number: 383/3003, Loss: 2.2064\n",
      "batch number: 384/3003, Loss: 2.4234\n",
      "batch number: 385/3003, Loss: 2.1228\n",
      "batch number: 386/3003, Loss: 2.3483\n",
      "batch number: 387/3003, Loss: 2.2585\n",
      "batch number: 388/3003, Loss: 2.2549\n",
      "batch number: 389/3003, Loss: 2.2613\n",
      "batch number: 390/3003, Loss: 2.2841\n",
      "batch number: 391/3003, Loss: 2.3625\n",
      "batch number: 392/3003, Loss: 2.2565\n",
      "batch number: 393/3003, Loss: 2.2746\n",
      "batch number: 394/3003, Loss: 2.2624\n",
      "batch number: 395/3003, Loss: 2.2014\n",
      "batch number: 396/3003, Loss: 2.3630\n",
      "batch number: 397/3003, Loss: 2.2897\n",
      "batch number: 398/3003, Loss: 2.3286\n",
      "batch number: 399/3003, Loss: 2.1051\n",
      "batch number: 400/3003, Loss: 2.3346\n",
      "batch number: 401/3003, Loss: 2.2346\n",
      "batch number: 402/3003, Loss: 2.3658\n",
      "batch number: 403/3003, Loss: 2.3541\n",
      "batch number: 404/3003, Loss: 2.3595\n",
      "batch number: 405/3003, Loss: 2.2036\n",
      "batch number: 406/3003, Loss: 2.2460\n",
      "batch number: 407/3003, Loss: 2.2921\n",
      "batch number: 408/3003, Loss: 2.1624\n",
      "batch number: 409/3003, Loss: 2.2751\n",
      "batch number: 410/3003, Loss: 2.1998\n",
      "batch number: 411/3003, Loss: 2.1920\n",
      "batch number: 412/3003, Loss: 2.3898\n",
      "batch number: 413/3003, Loss: 2.1466\n",
      "batch number: 414/3003, Loss: 2.2795\n",
      "batch number: 415/3003, Loss: 2.3851\n",
      "batch number: 416/3003, Loss: 2.2209\n",
      "batch number: 417/3003, Loss: 2.3700\n",
      "batch number: 418/3003, Loss: 2.3761\n",
      "batch number: 419/3003, Loss: 2.3057\n",
      "batch number: 420/3003, Loss: 2.3949\n",
      "batch number: 421/3003, Loss: 2.2285\n",
      "batch number: 422/3003, Loss: 2.4059\n",
      "batch number: 423/3003, Loss: 2.3247\n",
      "batch number: 424/3003, Loss: 2.4091\n",
      "batch number: 425/3003, Loss: 2.2690\n",
      "batch number: 426/3003, Loss: 2.3514\n",
      "batch number: 427/3003, Loss: 2.3226\n",
      "batch number: 428/3003, Loss: 2.3523\n",
      "batch number: 429/3003, Loss: 2.1781\n",
      "batch number: 430/3003, Loss: 2.3120\n",
      "batch number: 431/3003, Loss: 2.1564\n",
      "batch number: 432/3003, Loss: 2.3803\n",
      "batch number: 433/3003, Loss: 2.2448\n",
      "batch number: 434/3003, Loss: 2.3512\n",
      "batch number: 435/3003, Loss: 2.1391\n",
      "batch number: 436/3003, Loss: 2.3623\n",
      "batch number: 437/3003, Loss: 2.3241\n",
      "batch number: 438/3003, Loss: 2.4585\n",
      "batch number: 439/3003, Loss: 2.3050\n",
      "batch number: 440/3003, Loss: 2.3457\n",
      "batch number: 441/3003, Loss: 2.3756\n",
      "batch number: 442/3003, Loss: 2.3334\n",
      "batch number: 443/3003, Loss: 2.2036\n",
      "batch number: 444/3003, Loss: 2.2371\n",
      "batch number: 445/3003, Loss: 2.2339\n",
      "batch number: 446/3003, Loss: 2.3732\n",
      "batch number: 447/3003, Loss: 2.3111\n",
      "batch number: 448/3003, Loss: 2.2172\n",
      "batch number: 449/3003, Loss: 2.3964\n",
      "batch number: 450/3003, Loss: 2.3255\n",
      "batch number: 451/3003, Loss: 2.2649\n",
      "batch number: 452/3003, Loss: 2.3484\n",
      "batch number: 453/3003, Loss: 2.4427\n",
      "batch number: 454/3003, Loss: 2.3843\n",
      "batch number: 455/3003, Loss: 2.3699\n",
      "batch number: 456/3003, Loss: 2.3230\n",
      "batch number: 457/3003, Loss: 2.3336\n",
      "batch number: 458/3003, Loss: 2.3637\n",
      "batch number: 459/3003, Loss: 2.3469\n",
      "batch number: 460/3003, Loss: 2.1200\n",
      "batch number: 461/3003, Loss: 2.2272\n",
      "batch number: 462/3003, Loss: 2.2363\n",
      "batch number: 463/3003, Loss: 2.3466\n",
      "batch number: 464/3003, Loss: 2.3129\n",
      "batch number: 465/3003, Loss: 2.2671\n",
      "batch number: 466/3003, Loss: 2.2075\n",
      "batch number: 467/3003, Loss: 2.4486\n",
      "batch number: 468/3003, Loss: 2.2858\n",
      "batch number: 469/3003, Loss: 2.3698\n",
      "batch number: 470/3003, Loss: 2.3094\n",
      "batch number: 471/3003, Loss: 2.3100\n",
      "batch number: 472/3003, Loss: 2.3504\n",
      "batch number: 473/3003, Loss: 2.3637\n",
      "batch number: 474/3003, Loss: 2.2282\n",
      "batch number: 475/3003, Loss: 2.3178\n",
      "batch number: 476/3003, Loss: 2.3600\n",
      "batch number: 477/3003, Loss: 2.3054\n",
      "batch number: 478/3003, Loss: 2.2637\n",
      "batch number: 479/3003, Loss: 2.2870\n",
      "batch number: 480/3003, Loss: 2.3401\n",
      "batch number: 481/3003, Loss: 2.2848\n",
      "batch number: 482/3003, Loss: 2.3990\n",
      "batch number: 483/3003, Loss: 2.3856\n",
      "batch number: 484/3003, Loss: 2.1810\n",
      "batch number: 485/3003, Loss: 2.2873\n",
      "batch number: 486/3003, Loss: 2.2846\n",
      "batch number: 487/3003, Loss: 2.3117\n",
      "batch number: 488/3003, Loss: 2.2831\n",
      "batch number: 489/3003, Loss: 2.2626\n",
      "batch number: 490/3003, Loss: 2.2595\n",
      "batch number: 491/3003, Loss: 2.4173\n",
      "batch number: 492/3003, Loss: 2.3761\n",
      "batch number: 493/3003, Loss: 2.3085\n",
      "batch number: 494/3003, Loss: 2.4472\n",
      "batch number: 495/3003, Loss: 2.3840\n",
      "batch number: 496/3003, Loss: 2.2433\n",
      "batch number: 497/3003, Loss: 2.2398\n",
      "batch number: 498/3003, Loss: 2.3431\n",
      "batch number: 499/3003, Loss: 2.4219\n",
      "batch number: 500/3003, Loss: 2.2593\n",
      "batch number: 501/3003, Loss: 2.4177\n",
      "batch number: 502/3003, Loss: 2.3422\n",
      "batch number: 503/3003, Loss: 2.2641\n",
      "batch number: 504/3003, Loss: 2.2478\n",
      "batch number: 505/3003, Loss: 2.4782\n",
      "batch number: 506/3003, Loss: 2.3293\n",
      "batch number: 507/3003, Loss: 2.4186\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# -> (batch_size, n_class)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 46\u001b[0m, in \u001b[0;36mShortChunkCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# x: (batch_size, 1, 128, 137)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# CNN Forward Pass\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)  \u001b[38;5;66;03m# -> (batch_size, n_channels, H/2, W/2)\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)  \u001b[38;5;66;03m# -> (batch_size, n_channels, H/4, W/4)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)  \u001b[38;5;66;03m# -> (batch_size, n_channels*2, H/8, W/8)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)  \u001b[38;5;66;03m# -> (batch_size, n_channels*2, H/16, W/16)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 10\u001b[0m, in \u001b[0;36mConv_2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))))\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with GPU\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load the x and y into the data loader\n",
    "train_loader = DataLoader(dataset=list(zip(x, y)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = ShortChunkCNN(n_channels=128, n_class=11)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# 訓練迴圈\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batch_idx = 0\n",
    "    for inputs, integer_labels in train_loader:\n",
    "        # 假設 inputs 的形狀為 (batch_size, 128, 137)\n",
    "        inputs = inputs.unsqueeze(1)  # -> (batch_size, 1, 128, 137)\n",
    "        # labels: from 0 to 10\n",
    "        labels = integer_labels.squeeze() # -> (batch_size)\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # -> (batch_size, n_class)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print batch number \n",
    "        print(f'batch number: {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "        batch_idx += 1\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'mel_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cc51d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the model\n",
    "model = ShortChunkCNN(n_channels=128, n_class=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b170b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# load the test data\n",
    "test_data = load_json(os.path.join(test_data_path, 'examples.json'))\n",
    "test_keys = list(test_data.keys())\n",
    "test_features = []\n",
    "for key in test_keys:\n",
    "    file = os.path.join(test_data_path, 'audio', key + '.wav')\n",
    "    feature = feature_extraction(key, file)\n",
    "    test_features.append(feature)\n",
    "\n",
    "test_mel_spectrogram = [f[0] for f in test_features]\n",
    "\n",
    "test_labels = [test_data[key][\"instrument_family_str\"] for key in test_keys]\n",
    "test_integer_encoded = label_encoder.fit_transform(test_labels)\n",
    "test_integer_encoded = test_integer_encoded.reshape(-1, 1)\n",
    "\n",
    "# Reshape integer_encoded to 2D array (necessary for OneHotEncoder)\n",
    "integer_encoded = integer_encoded.reshape(-1, 1)\n",
    "\n",
    "test_x = np.array(test_mel_spectrogram)\n",
    "# test_x = np.expand_dims(test_x, axis=1)\n",
    "test_y = test_integer_encoded\n",
    "\n",
    "# load the test data into the data loader\n",
    "test_loader = DataLoader(dataset=list(zip(test_x, test_y)), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e118463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 173)\n"
     ]
    }
   ],
   "source": [
    "print(test_mel_spectrogram[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "234eeec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 128, 173)\n",
      "(4096, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b68167dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 843, 4: 766, 3: 652, 6: 502, 8: 306, 1: 269, 7: 235, 5: 202, 2: 180, 9: 141})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(test_y.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d3db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6062edfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'bass', 1: 'brass', 2: 'flute', 3: 'guitar', 4: 'keyboard', 5: 'mallet', 6: 'organ', 7: 'reed', 8: 'string', 9: 'synth_lead', 10: 'vocal'}\n"
     ]
    }
   ],
   "source": [
    "# print the integer and its corresponding label\n",
    "print(dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1a7806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_15977/561182298.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('mel_model.pth', ))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 100.00%\n",
      "Top-3 Accuracy: 100.00%\n",
      "Confusion Matrix:\n",
      "[[4096]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    top1_correct = 0\n",
    "    top3_correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch_x, batch_y in test_loader:\n",
    "\n",
    "            batch_x = batch_x.unsqueeze(1)  # -> (batch_size, 1, 128, 137)\n",
    "            batch_y = batch_y.squeeze() # -> (batch_size)\n",
    "\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # Get Top-1 predictions\n",
    "            _, top1_pred = torch.max(outputs, dim=1)\n",
    "            \n",
    "            # Get Top-3 predictions\n",
    "            _, top3_pred = torch.topk(outputs, k=3, dim=1)\n",
    "            \n",
    "            # Compute Top-1 accuracy\n",
    "            top1_correct += (top1_pred == batch_y.squeeze()).sum().item()\n",
    "            \n",
    "            # Compute Top-3 accuracy\n",
    "            top3_correct += (batch_y.squeeze().unsqueeze(1) == top3_pred).sum().item()\n",
    "\n",
    "            # Collect predictions and true labels for confusion matrix\n",
    "            all_preds.extend(top1_pred.mps().numpy())\n",
    "            all_labels.extend(batch_y.squeeze().mps().numpy())\n",
    "            \n",
    "            total += batch_y.size(0)\n",
    "\n",
    "    # Calculate accuracies\n",
    "    top1_accuracy = top1_correct / total\n",
    "    top3_accuracy = top3_correct / total\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return top1_accuracy, top3_accuracy, conf_matrix\n",
    "\n",
    "\n",
    "# Example usage of the evaluate function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# Load the model (assuming model has already been defined and trained)\n",
    "model = ShortChunkCNN(n_channels=128, n_class=11)  # Adjust based on your model\n",
    "model.load_state_dict(torch.load('mel_model.pth', ))\n",
    "model.to(device)\n",
    "\n",
    "# Load the test data into the data loader\n",
    "test_loader = DataLoader(dataset=list(zip(test_x, test_y)), batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "top1_acc, top3_acc, conf_matrix = evaluate(model, test_loader, device)\n",
    "\n",
    "# Print results\n",
    "print(f\"Top-1 Accuracy: {top1_acc * 100:.2f}%\")\n",
    "print(f\"Top-3 Accuracy: {top3_acc * 100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc253fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
