{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e51f0e",
   "metadata": {},
   "source": [
    "### Task3: Deep Learning Model\n",
    "\n",
    "- Train a deep learning model (e.g. CNN or attention based model) with Mel-spectrograms extracted from the audio as input\n",
    "\n",
    "- Need to compare 2 different kinds of inputs: Mel-spectrograms with or without taking the log\n",
    "\n",
    "- You can choose whatever FFT window size and hop length you like\n",
    "\n",
    "- You can choose whatever deep learning model you like\n",
    "\n",
    "- Need to report how to implement the model clearly\n",
    "\n",
    "- Need to report the testing result (not validation result) with confusion matrix, top1 accuracy, and top3 accuracy\n",
    "\n",
    "- You can use any music tagging model. For a novice, the short chunk CNN in this repo is recommended. (Need to replace the BCE loss to Cross-entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536f6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset file path:\n",
    "# traning_data_path = '<PUT THE PATH TO THE TRAINING DATA HERE>'\n",
    "\n",
    "traning_data_path = 'nsynth-subtrain'\n",
    "\n",
    "# test_data_path = '<PUT THE PATH TO THE TEST DATA HERE>'\n",
    "test_data_path = '../nsynth-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596aba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn import metrics\n",
    "import librosa\n",
    "import torchaudio\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "import joblib\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db58b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file\n",
    "def load_json(json_file):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# let the json path be /examples.json under the \"traning_data_path\"\n",
    "json_path = os.path.join(traning_data_path, 'examples.json')\n",
    "\n",
    "data = load_json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44de218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all keys in data\n",
    "keys = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe41d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(key, file_path):\n",
    "    y, sr = librosa.load(file_path)\n",
    "\n",
    "    # FFT window size=2048, and the hop length=512\n",
    "    # extract the mel spectrogram feature\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=512, hop_length=512)\n",
    "\n",
    "    # extract the mel spectrogram feature with log scaling\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "\n",
    "    # put all features into a list\n",
    "    features = [mel_spectrogram, log_mel_spectrogram]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97f0ef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48037 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48037/48037 [02:50<00:00, 281.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "# extract the features from each audio file\n",
    "\n",
    "features = []\n",
    "\n",
    "# for file in keys:\n",
    "for key in tqdm.tqdm(keys):\n",
    "    file = os.path.join(traning_data_path, 'audio', key + '.wav')\n",
    "    # extract the features\n",
    "    feature = feature_extraction(key, file)\n",
    "    features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3ede72",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = [f[0] for f in features]\n",
    "log_mel_spectrogram = [f[1] for f in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a84b9e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 173)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of the features\n",
    "print(mel_spectrogram[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4b205b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one hot encoding of the labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Extract labels from the data\n",
    "labels = [data[key][\"instrument_family_str\"] for key in keys]\n",
    "\n",
    "# Initialize the LabelEncoder and OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convert string labels to integer labels\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Reshape integer_encoded to 2D array (necessary for OneHotEncoder)\n",
    "integer_encoded = integer_encoded.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92bd4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(mel_spectrogram)\n",
    "y = integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "939d7d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 5005, 5: 4674, 3: 4633, 9: 4580, 4: 4535, 10: 4520, 6: 4499, 7: 4232, 2: 4118, 8: 3830, 1: 3411})\n"
     ]
    }
   ],
   "source": [
    "# print the count of each number in y\n",
    "from collections import Counter\n",
    "print(Counter(y.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ea81a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_2d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size=3, stride=1, pooling=2):\n",
    "        super(Conv_2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size, stride=stride, padding=kernel_size//2)\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mp = nn.MaxPool2d(pooling)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.mp(self.relu(self.bn(self.conv(x))))\n",
    "        return out\n",
    "\n",
    "class ShortChunkCNN(nn.Module):\n",
    "    '''\n",
    "    Short-chunk CNN architecture.\n",
    "    So-called VGG-like model with a small receptive field.\n",
    "    Deeper layers, smaller pooling (2x2).\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 n_channels=1,\n",
    "                 n_class=11):\n",
    "        super(ShortChunkCNN, self).__init__()\n",
    "\n",
    "        # CNN Layers\n",
    "        self.layer1 = Conv_2d(1, n_channels, pooling=2)\n",
    "        self.layer2 = Conv_2d(n_channels, n_channels, pooling=2)\n",
    "        self.layer3 = Conv_2d(n_channels, n_channels*2, pooling=2)\n",
    "        self.layer4 = Conv_2d(n_channels*2, n_channels*2, pooling=2)\n",
    "        self.layer5 = Conv_2d(n_channels*2, n_channels*2, pooling=2)\n",
    "        self.layer6 = Conv_2d(n_channels*2, n_channels*2, pooling=2)\n",
    "        self.layer7 = Conv_2d(n_channels*2, n_channels*4, pooling=2)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.dense1 = nn.Linear(n_channels*4, n_channels*4)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channels*4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(n_channels*4, n_class)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 1, 128, 137)\n",
    "\n",
    "        # CNN Forward Pass\n",
    "        x = self.layer1(x)  # -> (batch_size, n_channels, H/2, W/2)\n",
    "        x = self.layer2(x)  # -> (batch_size, n_channels, H/4, W/4)\n",
    "        x = self.layer3(x)  # -> (batch_size, n_channels*2, H/8, W/8)\n",
    "        x = self.layer4(x)  # -> (batch_size, n_channels*2, H/16, W/16)\n",
    "        x = self.layer5(x)  # -> (batch_size, n_channels*2, H/32, W/32)\n",
    "        x = self.layer6(x)  # -> (batch_size, n_channels*2, H/64, W/64)\n",
    "        x = self.layer7(x)  # -> (batch_size, n_channels*4, H/128, W/128)\n",
    "\n",
    "        # 確保特徵圖的寬度為1，進行全局池化\n",
    "        if x.size(3) != 1:\n",
    "            x = nn.MaxPool2d(kernel_size=(1, x.size(3)))(x)\n",
    "        x = x.squeeze(3)  # -> (batch_size, n_channels*4, H/128)\n",
    "\n",
    "        # 全局池化後，如果高度仍大於1，進行一次全局池化\n",
    "        if x.size(2) != 1:\n",
    "            x = nn.MaxPool1d(x.size(2))(x)\n",
    "        x = x.squeeze(2)  # -> (batch_size, n_channels*4)\n",
    "\n",
    "        # 全連接層\n",
    "        x = self.dense1(x)          # -> (batch_size, n_channels*4)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)          # -> (batch_size, n_class)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a981dfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch number: 1/3003, Loss: 2.3991\n",
      "batch number: 2/3003, Loss: 2.4031\n",
      "batch number: 3/3003, Loss: 2.4066\n",
      "batch number: 4/3003, Loss: 2.3617\n",
      "batch number: 5/3003, Loss: 2.4083\n",
      "batch number: 6/3003, Loss: 2.3641\n",
      "batch number: 7/3003, Loss: 2.4048\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# -> (batch_size, n_class)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print batch number \u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with GPU\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load the x and y into the data loader\n",
    "train_loader = DataLoader(dataset=list(zip(x, y)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = ShortChunkCNN(n_channels=128, n_class=11)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# 訓練迴圈\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batch_idx = 0\n",
    "    for inputs, integer_labels in train_loader:\n",
    "        # 假設 inputs 的形狀為 (batch_size, 128, 137)\n",
    "        inputs = inputs.unsqueeze(1)  # -> (batch_size, 1, 128, 137)\n",
    "        # labels: from 0 to 10\n",
    "        labels = integer_labels.squeeze() # -> (batch_size)\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # -> (batch_size, n_class)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print batch number \n",
    "        print(f'batch number: {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "        batch_idx += 1\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'mel_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cc51d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the model\n",
    "model = ShortChunkCNN(n_channels=1, n_class=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b170b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# load the test data\n",
    "test_data = load_json(os.path.join(test_data_path, 'examples.json'))\n",
    "test_keys = list(test_data.keys())\n",
    "test_features = []\n",
    "for key in test_keys:\n",
    "    file = os.path.join(test_data_path, 'audio', key + '.wav')\n",
    "    feature = feature_extraction(key, file)\n",
    "    test_features.append(feature)\n",
    "\n",
    "test_mel_spectrogram = [f[0] for f in test_features]\n",
    "\n",
    "test_labels = [test_data[key][\"instrument_family_str\"] for key in test_keys]\n",
    "test_integer_encoded = label_encoder.fit_transform(test_labels)\n",
    "test_integer_encoded = test_integer_encoded.reshape(-1, 1)\n",
    "\n",
    "# Reshape integer_encoded to 2D array (necessary for OneHotEncoder)\n",
    "integer_encoded = integer_encoded.reshape(-1, 1)\n",
    "\n",
    "test_x = np.array(test_mel_spectrogram)\n",
    "# test_x = np.expand_dims(test_x, axis=1)\n",
    "test_y = test_integer_encoded\n",
    "\n",
    "# load the test data into the data loader\n",
    "test_loader = DataLoader(dataset=list(zip(test_x, test_y)), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e118463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 173)\n"
     ]
    }
   ],
   "source": [
    "print(test_mel_spectrogram[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "234eeec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 128, 173)\n",
      "(4096, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b68167dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 843, 4: 766, 3: 652, 6: 502, 8: 306, 1: 269, 7: 235, 5: 202, 2: 180, 9: 141})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(test_y.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d3db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6062edfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'bass', 1: 'brass', 2: 'flute', 3: 'guitar', 4: 'keyboard', 5: 'mallet', 6: 'organ', 7: 'reed', 8: 'string', 9: 'synth_lead', 10: 'vocal'}\n"
     ]
    }
   ],
   "source": [
    "# print the integer and its corresponding label\n",
    "print(dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1a7806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_15977/561182298.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('mel_model.pth', ))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 100.00%\n",
      "Top-3 Accuracy: 100.00%\n",
      "Confusion Matrix:\n",
      "[[4096]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    top1_correct = 0\n",
    "    top3_correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch_x, batch_y in test_loader:\n",
    "\n",
    "            batch_x = batch_x.unsqueeze(1)  # -> (batch_size, 1, 128, 137)\n",
    "            batch_y = batch_y.squeeze() # -> (batch_size)\n",
    "\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # Get Top-1 predictions\n",
    "            _, top1_pred = torch.max(outputs, dim=1)\n",
    "            \n",
    "            # Get Top-3 predictions\n",
    "            _, top3_pred = torch.topk(outputs, k=3, dim=1)\n",
    "            \n",
    "            # Compute Top-1 accuracy\n",
    "            top1_correct += (top1_pred == batch_y.squeeze()).sum().item()\n",
    "            \n",
    "            # Compute Top-3 accuracy\n",
    "            top3_correct += (batch_y.squeeze().unsqueeze(1) == top3_pred).sum().item()\n",
    "\n",
    "            # Collect predictions and true labels for confusion matrix\n",
    "            all_preds.extend(top1_pred.mps().numpy())\n",
    "            all_labels.extend(batch_y.squeeze().mps().numpy())\n",
    "            \n",
    "            total += batch_y.size(0)\n",
    "\n",
    "    # Calculate accuracies\n",
    "    top1_accuracy = top1_correct / total\n",
    "    top3_accuracy = top3_correct / total\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return top1_accuracy, top3_accuracy, conf_matrix\n",
    "\n",
    "\n",
    "# Example usage of the evaluate function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# Load the model (assuming model has already been defined and trained)\n",
    "model = ShortChunkCNN(n_channels=128, n_class=11)  # Adjust based on your model\n",
    "model.load_state_dict(torch.load('mel_model.pth', ))\n",
    "model.to(device)\n",
    "\n",
    "# Load the test data into the data loader\n",
    "test_loader = DataLoader(dataset=list(zip(test_x, test_y)), batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "top1_acc, top3_acc, conf_matrix = evaluate(model, test_loader, device)\n",
    "\n",
    "# Print results\n",
    "print(f\"Top-1 Accuracy: {top1_acc * 100:.2f}%\")\n",
    "print(f\"Top-3 Accuracy: {top3_acc * 100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc253fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
