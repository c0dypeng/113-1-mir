{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-label classification\n",
    "# using MERT model as pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx2MIDIClass_path = \"hw1/class_idx2MIDIClass.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_path = \"hw1/slakh/train/\"\n",
    "train_label_path = \"hw1/slakh/train_labels.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_audio_path = \"hw1/slakh/validation/\"\n",
    "validation_label_path = \"hw1/slakh/validation_labels.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_path = \"hw1/slakh/test/\"\n",
    "test_label_path = \"hw1/slakh/test_labels.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# huggingface\n",
    "# from transformers import Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchaudio.transforms as T\n",
    "from datasets import load_dataset\n",
    "import nnAudio\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'Piano', '1': 'Percussion', '2': 'Organ', '3': 'Guitar', '4': 'Bass', '5': 'Strings', '6': 'Voice', '7': 'Wind Instruments', '8': 'Synth'}\n",
      "Piano\n"
     ]
    }
   ],
   "source": [
    "# read index-label mapping\n",
    "with open(class_idx2MIDIClass_path) as f:\n",
    "    class_idx2MIDIClass = json.load(f)\n",
    "\n",
    "print(class_idx2MIDIClass)\n",
    "# print the mapping\n",
    "print(class_idx2MIDIClass['0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get all the audio file names\n",
    "train_audio_files = []\n",
    "for root, dirs, files in os.walk(train_audio_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.npy'):\n",
    "            train_audio_files.append(file)\n",
    "\n",
    "# for all the file in the dataset(under the train audio path), store the audio-file name pair in a list\n",
    "train_audio = []\n",
    "for file in range(len(train_audio_files)):\n",
    "    audio = np.load(train_audio_path + train_audio_files[file])\n",
    "    train_audio.append((train_audio_files[file], audio))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read the label file\n",
    "with open(train_label_path, 'r') as f:\n",
    "    train_label = json.load(f)\n",
    "\n",
    "# for every key in the label file, find the corresponding label in train_audio, and append it in the tuple\n",
    "train_data = []\n",
    "for key in train_label:\n",
    "    for audio in train_audio:\n",
    "        if key == audio[0]:\n",
    "            train_data.append((audio[0], audio[1], train_label[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if GPU is available, use it, otherwise use CPU\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# loading our model weights\n",
    "# model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True)\n",
    "MERT_model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True).to(device)\n",
    "# loading the corresponding preprocessor config\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-95M\",trust_remote_code=True)\n",
    "\n",
    "# happen to be 24kHz, the same as the dataset\n",
    "resample_rate = processor.sampling_rate\n",
    "\n",
    "# (label, embedding)\n",
    "train_embedding_label = []\n",
    "\n",
    "# use tqdm to show the progress\n",
    "# for(filename, audio, label) in tqdm(train_data):\n",
    "# process the data in batches, or the kernel will die\n",
    "# total: 14994\n",
    "\n",
    "for(filename, audio, label) in tqdm(train_data):\n",
    "    input_audio = torch.tensor(audio).float().to(device)\n",
    "    # input_audio = torch.tensor(audio).float()\n",
    "    inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\").to(device)\n",
    "    # inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = MERT_model(**inputs, output_hidden_states=True)\n",
    "    all_layer_hidden_states = torch.stack(outputs.hidden_states).squeeze() # (13, 374, 768)\n",
    "    time_reduced_hidden_states = all_layer_hidden_states.mean(-2) # (13, 768)\n",
    "    \n",
    "    train_embedding_label.append((label, time_reduced_hidden_states))\n",
    "\n",
    "# Convert tensors to lists\n",
    "serializable_train_embedding_label = [\n",
    "    (label, embedding.tolist()) for label, embedding in train_embedding_label\n",
    "]\n",
    "\n",
    "# save train_embedding_label\n",
    "with open('train_embedding_label.json', 'w') as f:\n",
    "    json.dump(serializable_train_embedding_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the label file back\n",
    "# Load all JSON file\n",
    "embedded_data_filename = [\"train_embedding_label.json\"]\n",
    "\n",
    "train_embedding_label = []\n",
    "for filename in embedded_data_filename:\n",
    "    with open(filename, 'r') as f:\n",
    "        loaded_train_embedding_label = json.load(f)\n",
    "        loaded_train_embedding_label = [\n",
    "            (label, torch.tensor(embedding)) for label, embedding in loaded_train_embedding_label\n",
    "        ]\n",
    "        train_embedding_label.extend(loaded_train_embedding_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, embedding = self.data[idx]\n",
    "        embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
    "        return embedding, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "class MultiClassClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, thresholds=None):\n",
    "        super(MultiClassClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size[0] * input_size[1], 512)  # Adjusted input size\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.thresholds = thresholds if thresholds is not None else [0.5] * num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 13, 768]\n",
    "        x = x.view(x.size(0), -1)  # Flatten, shape: [batch_size, 13 * 768]\n",
    "        x = torch.relu(self.fc1(x))  # Fully connected layer with ReLU activation\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return self.sigmoid(x)  # Apply sigmoid to get probabilities\n",
    "\n",
    "    def predict(self, x, thresholds=None):\n",
    "        if thresholds is None:\n",
    "            thresholds = self.thresholds\n",
    "        with torch.no_grad():\n",
    "            probabilities = self.forward(x)\n",
    "            return (probabilities >= torch.tensor(thresholds).to(probabilities.device)).float()  # Apply thresholds to get binary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, num_epochs):\n",
    "    model.train()\n",
    "    total_batches = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_num}/{total_batches}], Loss: {loss.item():.4f}')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, thresholds):\n",
    "    num_classes = len(thresholds)\n",
    "    model.eval()\n",
    "    best_thresholds = [0.5] * num_classes\n",
    "    best_scores = [0] * num_classes\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        for threshold in thresholds:\n",
    "            all_labels = []\n",
    "            all_preds = []\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                preds = model.predict(inputs, thresholds=[threshold] * num_classes)\n",
    "                all_labels.append(labels.cpu().numpy()[:, i])\n",
    "                all_preds.append(preds.cpu().numpy()[:, i])\n",
    "            all_labels = np.concatenate(all_labels, axis=0)\n",
    "            all_preds = np.concatenate(all_preds, axis=0)\n",
    "            score = f1_score(all_labels, all_preds)  # Use F1-score for evaluation\n",
    "            if score > best_scores[i]:\n",
    "                best_scores[i] = score\n",
    "                best_thresholds[i] = threshold\n",
    "    \n",
    "    return best_thresholds, best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, thresholds, class_idx2MIDIClass):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    # Collect predictions and true labels\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        preds = model.predict(inputs, thresholds)\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all predictions and labels\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # Generate classification report\n",
    "    target_names = [class_idx2MIDIClass[str(i)] for i in range(len(class_idx2MIDIClass))]\n",
    "    report = classification_report(all_labels, all_preds, target_names=target_names, zero_division=0)\n",
    "    \n",
    "    # Print accuracy and classification report\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(report)\n",
    "    \n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [1/938], Loss: 0.7474\n",
      "Epoch [1/10], Batch [2/938], Loss: 0.6523\n",
      "Epoch [1/10], Batch [3/938], Loss: 0.6986\n",
      "Epoch [1/10], Batch [4/938], Loss: 0.6303\n",
      "Epoch [1/10], Batch [5/938], Loss: 0.5842\n",
      "Epoch [1/10], Batch [6/938], Loss: 0.6325\n",
      "Epoch [1/10], Batch [7/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [8/938], Loss: 0.5947\n",
      "Epoch [1/10], Batch [9/938], Loss: 0.6132\n",
      "Epoch [1/10], Batch [10/938], Loss: 0.6147\n",
      "Epoch [1/10], Batch [11/938], Loss: 0.6220\n",
      "Epoch [1/10], Batch [12/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [13/938], Loss: 0.6176\n",
      "Epoch [1/10], Batch [14/938], Loss: 0.6216\n",
      "Epoch [1/10], Batch [15/938], Loss: 0.6081\n",
      "Epoch [1/10], Batch [16/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [17/938], Loss: 0.6045\n",
      "Epoch [1/10], Batch [18/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [19/938], Loss: 0.6152\n",
      "Epoch [1/10], Batch [20/938], Loss: 0.6126\n",
      "Epoch [1/10], Batch [21/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [22/938], Loss: 0.6163\n",
      "Epoch [1/10], Batch [23/938], Loss: 0.6150\n",
      "Epoch [1/10], Batch [24/938], Loss: 0.5804\n",
      "Epoch [1/10], Batch [25/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [26/938], Loss: 0.5799\n",
      "Epoch [1/10], Batch [27/938], Loss: 0.5887\n",
      "Epoch [1/10], Batch [28/938], Loss: 0.5907\n",
      "Epoch [1/10], Batch [29/938], Loss: 0.5887\n",
      "Epoch [1/10], Batch [30/938], Loss: 0.6486\n",
      "Epoch [1/10], Batch [31/938], Loss: 0.6210\n",
      "Epoch [1/10], Batch [32/938], Loss: 0.6733\n",
      "Epoch [1/10], Batch [33/938], Loss: 0.6230\n",
      "Epoch [1/10], Batch [34/938], Loss: 0.6297\n",
      "Epoch [1/10], Batch [35/938], Loss: 0.6061\n",
      "Epoch [1/10], Batch [36/938], Loss: 0.6087\n",
      "Epoch [1/10], Batch [37/938], Loss: 0.5933\n",
      "Epoch [1/10], Batch [38/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [39/938], Loss: 0.6093\n",
      "Epoch [1/10], Batch [40/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [41/938], Loss: 0.6014\n",
      "Epoch [1/10], Batch [42/938], Loss: 0.6347\n",
      "Epoch [1/10], Batch [43/938], Loss: 0.6343\n",
      "Epoch [1/10], Batch [44/938], Loss: 0.6193\n",
      "Epoch [1/10], Batch [45/938], Loss: 0.6164\n",
      "Epoch [1/10], Batch [46/938], Loss: 0.6276\n",
      "Epoch [1/10], Batch [47/938], Loss: 0.6021\n",
      "Epoch [1/10], Batch [48/938], Loss: 0.5946\n",
      "Epoch [1/10], Batch [49/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [50/938], Loss: 0.6219\n",
      "Epoch [1/10], Batch [51/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [52/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [53/938], Loss: 0.6000\n",
      "Epoch [1/10], Batch [54/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [55/938], Loss: 0.6305\n",
      "Epoch [1/10], Batch [56/938], Loss: 0.6494\n",
      "Epoch [1/10], Batch [57/938], Loss: 0.6099\n",
      "Epoch [1/10], Batch [58/938], Loss: 0.6071\n",
      "Epoch [1/10], Batch [59/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [60/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [61/938], Loss: 0.6386\n",
      "Epoch [1/10], Batch [62/938], Loss: 0.6350\n",
      "Epoch [1/10], Batch [63/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [64/938], Loss: 0.6062\n",
      "Epoch [1/10], Batch [65/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [66/938], Loss: 0.5866\n",
      "Epoch [1/10], Batch [67/938], Loss: 0.5938\n",
      "Epoch [1/10], Batch [68/938], Loss: 0.6171\n",
      "Epoch [1/10], Batch [69/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [70/938], Loss: 0.5999\n",
      "Epoch [1/10], Batch [71/938], Loss: 0.5903\n",
      "Epoch [1/10], Batch [72/938], Loss: 0.6117\n",
      "Epoch [1/10], Batch [73/938], Loss: 0.6299\n",
      "Epoch [1/10], Batch [74/938], Loss: 0.6081\n",
      "Epoch [1/10], Batch [75/938], Loss: 0.6152\n",
      "Epoch [1/10], Batch [76/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [77/938], Loss: 0.6013\n",
      "Epoch [1/10], Batch [78/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [79/938], Loss: 0.6285\n",
      "Epoch [1/10], Batch [80/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [81/938], Loss: 0.5876\n",
      "Epoch [1/10], Batch [82/938], Loss: 0.6557\n",
      "Epoch [1/10], Batch [83/938], Loss: 0.6290\n",
      "Epoch [1/10], Batch [84/938], Loss: 0.6096\n",
      "Epoch [1/10], Batch [85/938], Loss: 0.6017\n",
      "Epoch [1/10], Batch [86/938], Loss: 0.5735\n",
      "Epoch [1/10], Batch [87/938], Loss: 0.6367\n",
      "Epoch [1/10], Batch [88/938], Loss: 0.6248\n",
      "Epoch [1/10], Batch [89/938], Loss: 0.5735\n",
      "Epoch [1/10], Batch [90/938], Loss: 0.6201\n",
      "Epoch [1/10], Batch [91/938], Loss: 0.5871\n",
      "Epoch [1/10], Batch [92/938], Loss: 0.6260\n",
      "Epoch [1/10], Batch [93/938], Loss: 0.6199\n",
      "Epoch [1/10], Batch [94/938], Loss: 0.6226\n",
      "Epoch [1/10], Batch [95/938], Loss: 0.6162\n",
      "Epoch [1/10], Batch [96/938], Loss: 0.6119\n",
      "Epoch [1/10], Batch [97/938], Loss: 0.5894\n",
      "Epoch [1/10], Batch [98/938], Loss: 0.6229\n",
      "Epoch [1/10], Batch [99/938], Loss: 0.6033\n",
      "Epoch [1/10], Batch [100/938], Loss: 0.6229\n",
      "Epoch [1/10], Batch [101/938], Loss: 0.6014\n",
      "Epoch [1/10], Batch [102/938], Loss: 0.6013\n",
      "Epoch [1/10], Batch [103/938], Loss: 0.5944\n",
      "Epoch [1/10], Batch [104/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [105/938], Loss: 0.6255\n",
      "Epoch [1/10], Batch [106/938], Loss: 0.5959\n",
      "Epoch [1/10], Batch [107/938], Loss: 0.6283\n",
      "Epoch [1/10], Batch [108/938], Loss: 0.6145\n",
      "Epoch [1/10], Batch [109/938], Loss: 0.5884\n",
      "Epoch [1/10], Batch [110/938], Loss: 0.6202\n",
      "Epoch [1/10], Batch [111/938], Loss: 0.6162\n",
      "Epoch [1/10], Batch [112/938], Loss: 0.6374\n",
      "Epoch [1/10], Batch [113/938], Loss: 0.6088\n",
      "Epoch [1/10], Batch [114/938], Loss: 0.6178\n",
      "Epoch [1/10], Batch [115/938], Loss: 0.6174\n",
      "Epoch [1/10], Batch [116/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [117/938], Loss: 0.6359\n",
      "Epoch [1/10], Batch [118/938], Loss: 0.6113\n",
      "Epoch [1/10], Batch [119/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [120/938], Loss: 0.6259\n",
      "Epoch [1/10], Batch [121/938], Loss: 0.6463\n",
      "Epoch [1/10], Batch [122/938], Loss: 0.5968\n",
      "Epoch [1/10], Batch [123/938], Loss: 0.5880\n",
      "Epoch [1/10], Batch [124/938], Loss: 0.6013\n",
      "Epoch [1/10], Batch [125/938], Loss: 0.6049\n",
      "Epoch [1/10], Batch [126/938], Loss: 0.5873\n",
      "Epoch [1/10], Batch [127/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [128/938], Loss: 0.5853\n",
      "Epoch [1/10], Batch [129/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [130/938], Loss: 0.6013\n",
      "Epoch [1/10], Batch [131/938], Loss: 0.6476\n",
      "Epoch [1/10], Batch [132/938], Loss: 0.6269\n",
      "Epoch [1/10], Batch [133/938], Loss: 0.6454\n",
      "Epoch [1/10], Batch [134/938], Loss: 0.6106\n",
      "Epoch [1/10], Batch [135/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [136/938], Loss: 0.6103\n",
      "Epoch [1/10], Batch [137/938], Loss: 0.6007\n",
      "Epoch [1/10], Batch [138/938], Loss: 0.6228\n",
      "Epoch [1/10], Batch [139/938], Loss: 0.6150\n",
      "Epoch [1/10], Batch [140/938], Loss: 0.6131\n",
      "Epoch [1/10], Batch [141/938], Loss: 0.6154\n",
      "Epoch [1/10], Batch [142/938], Loss: 0.6031\n",
      "Epoch [1/10], Batch [143/938], Loss: 0.5866\n",
      "Epoch [1/10], Batch [144/938], Loss: 0.5770\n",
      "Epoch [1/10], Batch [145/938], Loss: 0.5807\n",
      "Epoch [1/10], Batch [146/938], Loss: 0.5903\n",
      "Epoch [1/10], Batch [147/938], Loss: 0.6262\n",
      "Epoch [1/10], Batch [148/938], Loss: 0.6098\n",
      "Epoch [1/10], Batch [149/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [150/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [151/938], Loss: 0.6360\n",
      "Epoch [1/10], Batch [152/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [153/938], Loss: 0.6369\n",
      "Epoch [1/10], Batch [154/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [155/938], Loss: 0.5735\n",
      "Epoch [1/10], Batch [156/938], Loss: 0.6147\n",
      "Epoch [1/10], Batch [157/938], Loss: 0.6011\n",
      "Epoch [1/10], Batch [158/938], Loss: 0.6089\n",
      "Epoch [1/10], Batch [159/938], Loss: 0.6038\n",
      "Epoch [1/10], Batch [160/938], Loss: 0.6230\n",
      "Epoch [1/10], Batch [161/938], Loss: 0.6360\n",
      "Epoch [1/10], Batch [162/938], Loss: 0.6083\n",
      "Epoch [1/10], Batch [163/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [164/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [165/938], Loss: 0.6021\n",
      "Epoch [1/10], Batch [166/938], Loss: 0.6455\n",
      "Epoch [1/10], Batch [167/938], Loss: 0.6023\n",
      "Epoch [1/10], Batch [168/938], Loss: 0.6300\n",
      "Epoch [1/10], Batch [169/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [170/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [171/938], Loss: 0.5901\n",
      "Epoch [1/10], Batch [172/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [173/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [174/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [175/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [176/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [177/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [178/938], Loss: 0.5804\n",
      "Epoch [1/10], Batch [179/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [180/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [181/938], Loss: 0.6163\n",
      "Epoch [1/10], Batch [182/938], Loss: 0.6222\n",
      "Epoch [1/10], Batch [183/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [184/938], Loss: 0.5883\n",
      "Epoch [1/10], Batch [185/938], Loss: 0.6744\n",
      "Epoch [1/10], Batch [186/938], Loss: 0.6290\n",
      "Epoch [1/10], Batch [187/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [188/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [189/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [190/938], Loss: 0.5735\n",
      "Epoch [1/10], Batch [191/938], Loss: 0.6360\n",
      "Epoch [1/10], Batch [192/938], Loss: 0.6439\n",
      "Epoch [1/10], Batch [193/938], Loss: 0.6031\n",
      "Epoch [1/10], Batch [194/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [195/938], Loss: 0.6162\n",
      "Epoch [1/10], Batch [196/938], Loss: 0.5886\n",
      "Epoch [1/10], Batch [197/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [198/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [199/938], Loss: 0.6163\n",
      "Epoch [1/10], Batch [200/938], Loss: 0.6230\n",
      "Epoch [1/10], Batch [201/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [202/938], Loss: 0.5893\n",
      "Epoch [1/10], Batch [203/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [204/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [205/938], Loss: 0.6159\n",
      "Epoch [1/10], Batch [206/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [207/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [208/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [209/938], Loss: 0.6508\n",
      "Epoch [1/10], Batch [210/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [211/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [212/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [213/938], Loss: 0.6153\n",
      "Epoch [1/10], Batch [214/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [215/938], Loss: 0.5991\n",
      "Epoch [1/10], Batch [216/938], Loss: 0.5962\n",
      "Epoch [1/10], Batch [217/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [218/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [219/938], Loss: 0.6148\n",
      "Epoch [1/10], Batch [220/938], Loss: 0.6122\n",
      "Epoch [1/10], Batch [221/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [222/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [223/938], Loss: 0.6051\n",
      "Epoch [1/10], Batch [224/938], Loss: 0.6228\n",
      "Epoch [1/10], Batch [225/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [226/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [227/938], Loss: 0.6291\n",
      "Epoch [1/10], Batch [228/938], Loss: 0.6290\n",
      "Epoch [1/10], Batch [229/938], Loss: 0.6300\n",
      "Epoch [1/10], Batch [230/938], Loss: 0.6171\n",
      "Epoch [1/10], Batch [231/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [232/938], Loss: 0.5959\n",
      "Epoch [1/10], Batch [233/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [234/938], Loss: 0.5883\n",
      "Epoch [1/10], Batch [235/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [236/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [237/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [238/938], Loss: 0.6047\n",
      "Epoch [1/10], Batch [239/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [240/938], Loss: 0.5876\n",
      "Epoch [1/10], Batch [241/938], Loss: 0.6360\n",
      "Epoch [1/10], Batch [242/938], Loss: 0.6300\n",
      "Epoch [1/10], Batch [243/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [244/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [245/938], Loss: 0.6408\n",
      "Epoch [1/10], Batch [246/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [247/938], Loss: 0.5999\n",
      "Epoch [1/10], Batch [248/938], Loss: 0.5958\n",
      "Epoch [1/10], Batch [249/938], Loss: 0.6164\n",
      "Epoch [1/10], Batch [250/938], Loss: 0.6219\n",
      "Epoch [1/10], Batch [251/938], Loss: 0.6010\n",
      "Epoch [1/10], Batch [252/938], Loss: 0.6083\n",
      "Epoch [1/10], Batch [253/938], Loss: 0.6320\n",
      "Epoch [1/10], Batch [254/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [255/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [256/938], Loss: 0.6379\n",
      "Epoch [1/10], Batch [257/938], Loss: 0.6343\n",
      "Epoch [1/10], Batch [258/938], Loss: 0.6074\n",
      "Epoch [1/10], Batch [259/938], Loss: 0.6212\n",
      "Epoch [1/10], Batch [260/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [261/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [262/938], Loss: 0.6023\n",
      "Epoch [1/10], Batch [263/938], Loss: 0.5928\n",
      "Epoch [1/10], Batch [264/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [265/938], Loss: 0.5939\n",
      "Epoch [1/10], Batch [266/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [267/938], Loss: 0.6083\n",
      "Epoch [1/10], Batch [268/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [269/938], Loss: 0.6229\n",
      "Epoch [1/10], Batch [270/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [271/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [272/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [273/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [274/938], Loss: 0.6034\n",
      "Epoch [1/10], Batch [275/938], Loss: 0.6090\n",
      "Epoch [1/10], Batch [276/938], Loss: 0.6164\n",
      "Epoch [1/10], Batch [277/938], Loss: 0.6300\n",
      "Epoch [1/10], Batch [278/938], Loss: 0.6102\n",
      "Epoch [1/10], Batch [279/938], Loss: 0.6437\n",
      "Epoch [1/10], Batch [280/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [281/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [282/938], Loss: 0.6091\n",
      "Epoch [1/10], Batch [283/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [284/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [285/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [286/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [287/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [288/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [289/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [290/938], Loss: 0.6150\n",
      "Epoch [1/10], Batch [291/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [292/938], Loss: 0.5804\n",
      "Epoch [1/10], Batch [293/938], Loss: 0.6099\n",
      "Epoch [1/10], Batch [294/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [295/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [296/938], Loss: 0.6310\n",
      "Epoch [1/10], Batch [297/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [298/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [299/938], Loss: 0.5788\n",
      "Epoch [1/10], Batch [300/938], Loss: 0.5950\n",
      "Epoch [1/10], Batch [301/938], Loss: 0.6026\n",
      "Epoch [1/10], Batch [302/938], Loss: 0.6369\n",
      "Epoch [1/10], Batch [303/938], Loss: 0.6062\n",
      "Epoch [1/10], Batch [304/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [305/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [306/938], Loss: 0.6102\n",
      "Epoch [1/10], Batch [307/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [308/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [309/938], Loss: 0.6452\n",
      "Epoch [1/10], Batch [310/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [311/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [312/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [313/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [314/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [315/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [316/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [317/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [318/938], Loss: 0.6106\n",
      "Epoch [1/10], Batch [319/938], Loss: 0.6215\n",
      "Epoch [1/10], Batch [320/938], Loss: 0.6300\n",
      "Epoch [1/10], Batch [321/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [322/938], Loss: 0.5735\n",
      "Epoch [1/10], Batch [323/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [324/938], Loss: 0.6026\n",
      "Epoch [1/10], Batch [325/938], Loss: 0.5962\n",
      "Epoch [1/10], Batch [326/938], Loss: 0.6206\n",
      "Epoch [1/10], Batch [327/938], Loss: 0.5962\n",
      "Epoch [1/10], Batch [328/938], Loss: 0.6229\n",
      "Epoch [1/10], Batch [329/938], Loss: 0.6290\n",
      "Epoch [1/10], Batch [330/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [331/938], Loss: 0.6101\n",
      "Epoch [1/10], Batch [332/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [333/938], Loss: 0.6160\n",
      "Epoch [1/10], Batch [334/938], Loss: 0.5944\n",
      "Epoch [1/10], Batch [335/938], Loss: 0.6169\n",
      "Epoch [1/10], Batch [336/938], Loss: 0.5873\n",
      "Epoch [1/10], Batch [337/938], Loss: 0.6263\n",
      "Epoch [1/10], Batch [338/938], Loss: 0.6128\n",
      "Epoch [1/10], Batch [339/938], Loss: 0.6378\n",
      "Epoch [1/10], Batch [340/938], Loss: 0.5814\n",
      "Epoch [1/10], Batch [341/938], Loss: 0.6331\n",
      "Epoch [1/10], Batch [342/938], Loss: 0.6069\n",
      "Epoch [1/10], Batch [343/938], Loss: 0.6214\n",
      "Epoch [1/10], Batch [344/938], Loss: 0.6029\n",
      "Epoch [1/10], Batch [345/938], Loss: 0.5937\n",
      "Epoch [1/10], Batch [346/938], Loss: 0.6618\n",
      "Epoch [1/10], Batch [347/938], Loss: 0.6173\n",
      "Epoch [1/10], Batch [348/938], Loss: 0.6147\n",
      "Epoch [1/10], Batch [349/938], Loss: 0.6014\n",
      "Epoch [1/10], Batch [350/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [351/938], Loss: 0.6049\n",
      "Epoch [1/10], Batch [352/938], Loss: 0.6145\n",
      "Epoch [1/10], Batch [353/938], Loss: 0.6011\n",
      "Epoch [1/10], Batch [354/938], Loss: 0.5804\n",
      "Epoch [1/10], Batch [355/938], Loss: 0.6370\n",
      "Epoch [1/10], Batch [356/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [357/938], Loss: 0.5873\n",
      "Epoch [1/10], Batch [358/938], Loss: 0.5962\n",
      "Epoch [1/10], Batch [359/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [360/938], Loss: 0.5805\n",
      "Epoch [1/10], Batch [361/938], Loss: 0.6297\n",
      "Epoch [1/10], Batch [362/938], Loss: 0.5884\n",
      "Epoch [1/10], Batch [363/938], Loss: 0.6319\n",
      "Epoch [1/10], Batch [364/938], Loss: 0.6027\n",
      "Epoch [1/10], Batch [365/938], Loss: 0.6169\n",
      "Epoch [1/10], Batch [366/938], Loss: 0.6251\n",
      "Epoch [1/10], Batch [367/938], Loss: 0.5804\n",
      "Epoch [1/10], Batch [368/938], Loss: 0.5780\n",
      "Epoch [1/10], Batch [369/938], Loss: 0.6231\n",
      "Epoch [1/10], Batch [370/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [371/938], Loss: 0.6180\n",
      "Epoch [1/10], Batch [372/938], Loss: 0.6013\n",
      "Epoch [1/10], Batch [373/938], Loss: 0.5944\n",
      "Epoch [1/10], Batch [374/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [375/938], Loss: 0.6303\n",
      "Epoch [1/10], Batch [376/938], Loss: 0.6070\n",
      "Epoch [1/10], Batch [377/938], Loss: 0.6239\n",
      "Epoch [1/10], Batch [378/938], Loss: 0.5960\n",
      "Epoch [1/10], Batch [379/938], Loss: 0.6172\n",
      "Epoch [1/10], Batch [380/938], Loss: 0.6250\n",
      "Epoch [1/10], Batch [381/938], Loss: 0.6081\n",
      "Epoch [1/10], Batch [382/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [383/938], Loss: 0.6013\n",
      "Epoch [1/10], Batch [384/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [385/938], Loss: 0.6058\n",
      "Epoch [1/10], Batch [386/938], Loss: 0.6230\n",
      "Epoch [1/10], Batch [387/938], Loss: 0.6100\n",
      "Epoch [1/10], Batch [388/938], Loss: 0.6298\n",
      "Epoch [1/10], Batch [389/938], Loss: 0.5962\n",
      "Epoch [1/10], Batch [390/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [391/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [392/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [393/938], Loss: 0.6481\n",
      "Epoch [1/10], Batch [394/938], Loss: 0.6081\n",
      "Epoch [1/10], Batch [395/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [396/938], Loss: 0.6074\n",
      "Epoch [1/10], Batch [397/938], Loss: 0.6075\n",
      "Epoch [1/10], Batch [398/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [399/938], Loss: 0.6429\n",
      "Epoch [1/10], Batch [400/938], Loss: 0.6274\n",
      "Epoch [1/10], Batch [401/938], Loss: 0.5735\n",
      "Epoch [1/10], Batch [402/938], Loss: 0.6222\n",
      "Epoch [1/10], Batch [403/938], Loss: 0.6144\n",
      "Epoch [1/10], Batch [404/938], Loss: 0.6268\n",
      "Epoch [1/10], Batch [405/938], Loss: 0.6135\n",
      "Epoch [1/10], Batch [406/938], Loss: 0.6347\n",
      "Epoch [1/10], Batch [407/938], Loss: 0.6015\n",
      "Epoch [1/10], Batch [408/938], Loss: 0.6187\n",
      "Epoch [1/10], Batch [409/938], Loss: 0.5974\n",
      "Epoch [1/10], Batch [410/938], Loss: 0.6103\n",
      "Epoch [1/10], Batch [411/938], Loss: 0.6184\n",
      "Epoch [1/10], Batch [412/938], Loss: 0.6363\n",
      "Epoch [1/10], Batch [413/938], Loss: 0.6009\n",
      "Epoch [1/10], Batch [414/938], Loss: 0.6175\n",
      "Epoch [1/10], Batch [415/938], Loss: 0.6085\n",
      "Epoch [1/10], Batch [416/938], Loss: 0.6337\n",
      "Epoch [1/10], Batch [417/938], Loss: 0.6029\n",
      "Epoch [1/10], Batch [418/938], Loss: 0.6163\n",
      "Epoch [1/10], Batch [419/938], Loss: 0.5876\n",
      "Epoch [1/10], Batch [420/938], Loss: 0.6253\n",
      "Epoch [1/10], Batch [421/938], Loss: 0.6231\n",
      "Epoch [1/10], Batch [422/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [423/938], Loss: 0.6099\n",
      "Epoch [1/10], Batch [424/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [425/938], Loss: 0.5954\n",
      "Epoch [1/10], Batch [426/938], Loss: 0.6233\n",
      "Epoch [1/10], Batch [427/938], Loss: 0.6001\n",
      "Epoch [1/10], Batch [428/938], Loss: 0.6025\n",
      "Epoch [1/10], Batch [429/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [430/938], Loss: 0.6152\n",
      "Epoch [1/10], Batch [431/938], Loss: 0.6389\n",
      "Epoch [1/10], Batch [432/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [433/938], Loss: 0.5910\n",
      "Epoch [1/10], Batch [434/938], Loss: 0.6291\n",
      "Epoch [1/10], Batch [435/938], Loss: 0.6158\n",
      "Epoch [1/10], Batch [436/938], Loss: 0.5823\n",
      "Epoch [1/10], Batch [437/938], Loss: 0.5875\n",
      "Epoch [1/10], Batch [438/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [439/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [440/938], Loss: 0.5857\n",
      "Epoch [1/10], Batch [441/938], Loss: 0.5952\n",
      "Epoch [1/10], Batch [442/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [443/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [444/938], Loss: 0.5883\n",
      "Epoch [1/10], Batch [445/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [446/938], Loss: 0.6101\n",
      "Epoch [1/10], Batch [447/938], Loss: 0.6050\n",
      "Epoch [1/10], Batch [448/938], Loss: 0.6484\n",
      "Epoch [1/10], Batch [449/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [450/938], Loss: 0.6332\n",
      "Epoch [1/10], Batch [451/938], Loss: 0.6080\n",
      "Epoch [1/10], Batch [452/938], Loss: 0.6112\n",
      "Epoch [1/10], Batch [453/938], Loss: 0.6160\n",
      "Epoch [1/10], Batch [454/938], Loss: 0.6171\n",
      "Epoch [1/10], Batch [455/938], Loss: 0.6240\n",
      "Epoch [1/10], Batch [456/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [457/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [458/938], Loss: 0.6268\n",
      "Epoch [1/10], Batch [459/938], Loss: 0.6081\n",
      "Epoch [1/10], Batch [460/938], Loss: 0.6226\n",
      "Epoch [1/10], Batch [461/938], Loss: 0.6109\n",
      "Epoch [1/10], Batch [462/938], Loss: 0.6171\n",
      "Epoch [1/10], Batch [463/938], Loss: 0.5792\n",
      "Epoch [1/10], Batch [464/938], Loss: 0.6039\n",
      "Epoch [1/10], Batch [465/938], Loss: 0.6066\n",
      "Epoch [1/10], Batch [466/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [467/938], Loss: 0.6048\n",
      "Epoch [1/10], Batch [468/938], Loss: 0.5985\n",
      "Epoch [1/10], Batch [469/938], Loss: 0.6235\n",
      "Epoch [1/10], Batch [470/938], Loss: 0.6084\n",
      "Epoch [1/10], Batch [471/938], Loss: 0.6182\n",
      "Epoch [1/10], Batch [472/938], Loss: 0.6126\n",
      "Epoch [1/10], Batch [473/938], Loss: 0.5977\n",
      "Epoch [1/10], Batch [474/938], Loss: 0.5989\n",
      "Epoch [1/10], Batch [475/938], Loss: 0.5883\n",
      "Epoch [1/10], Batch [476/938], Loss: 0.6366\n",
      "Epoch [1/10], Batch [477/938], Loss: 0.6301\n",
      "Epoch [1/10], Batch [478/938], Loss: 0.5985\n",
      "Epoch [1/10], Batch [479/938], Loss: 0.6292\n",
      "Epoch [1/10], Batch [480/938], Loss: 0.6008\n",
      "Epoch [1/10], Batch [481/938], Loss: 0.6180\n",
      "Epoch [1/10], Batch [482/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [483/938], Loss: 0.6132\n",
      "Epoch [1/10], Batch [484/938], Loss: 0.6075\n",
      "Epoch [1/10], Batch [485/938], Loss: 0.5947\n",
      "Epoch [1/10], Batch [486/938], Loss: 0.6081\n",
      "Epoch [1/10], Batch [487/938], Loss: 0.6105\n",
      "Epoch [1/10], Batch [488/938], Loss: 0.6007\n",
      "Epoch [1/10], Batch [489/938], Loss: 0.6152\n",
      "Epoch [1/10], Batch [490/938], Loss: 0.6081\n",
      "Epoch [1/10], Batch [491/938], Loss: 0.5905\n",
      "Epoch [1/10], Batch [492/938], Loss: 0.6106\n",
      "Epoch [1/10], Batch [493/938], Loss: 0.6305\n",
      "Epoch [1/10], Batch [494/938], Loss: 0.6104\n",
      "Epoch [1/10], Batch [495/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [496/938], Loss: 0.6410\n",
      "Epoch [1/10], Batch [497/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [498/938], Loss: 0.5869\n",
      "Epoch [1/10], Batch [499/938], Loss: 0.6013\n",
      "Epoch [1/10], Batch [500/938], Loss: 0.6240\n",
      "Epoch [1/10], Batch [501/938], Loss: 0.6408\n",
      "Epoch [1/10], Batch [502/938], Loss: 0.6081\n",
      "Epoch [1/10], Batch [503/938], Loss: 0.6032\n",
      "Epoch [1/10], Batch [504/938], Loss: 0.6078\n",
      "Epoch [1/10], Batch [505/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [506/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [507/938], Loss: 0.6163\n",
      "Epoch [1/10], Batch [508/938], Loss: 0.6167\n",
      "Epoch [1/10], Batch [509/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [510/938], Loss: 0.6238\n",
      "Epoch [1/10], Batch [511/938], Loss: 0.6230\n",
      "Epoch [1/10], Batch [512/938], Loss: 0.6135\n",
      "Epoch [1/10], Batch [513/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [514/938], Loss: 0.5841\n",
      "Epoch [1/10], Batch [515/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [516/938], Loss: 0.5962\n",
      "Epoch [1/10], Batch [517/938], Loss: 0.5804\n",
      "Epoch [1/10], Batch [518/938], Loss: 0.5991\n",
      "Epoch [1/10], Batch [519/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [520/938], Loss: 0.6295\n",
      "Epoch [1/10], Batch [521/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [522/938], Loss: 0.6146\n",
      "Epoch [1/10], Batch [523/938], Loss: 0.6287\n",
      "Epoch [1/10], Batch [524/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [525/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [526/938], Loss: 0.6175\n",
      "Epoch [1/10], Batch [527/938], Loss: 0.6076\n",
      "Epoch [1/10], Batch [528/938], Loss: 0.6302\n",
      "Epoch [1/10], Batch [529/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [530/938], Loss: 0.6003\n",
      "Epoch [1/10], Batch [531/938], Loss: 0.6162\n",
      "Epoch [1/10], Batch [532/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [533/938], Loss: 0.6321\n",
      "Epoch [1/10], Batch [534/938], Loss: 0.6104\n",
      "Epoch [1/10], Batch [535/938], Loss: 0.5838\n",
      "Epoch [1/10], Batch [536/938], Loss: 0.6126\n",
      "Epoch [1/10], Batch [537/938], Loss: 0.5965\n",
      "Epoch [1/10], Batch [538/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [539/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [540/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [541/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [542/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [543/938], Loss: 0.6075\n",
      "Epoch [1/10], Batch [544/938], Loss: 0.5735\n",
      "Epoch [1/10], Batch [545/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [546/938], Loss: 0.6195\n",
      "Epoch [1/10], Batch [547/938], Loss: 0.6085\n",
      "Epoch [1/10], Batch [548/938], Loss: 0.6344\n",
      "Epoch [1/10], Batch [549/938], Loss: 0.6290\n",
      "Epoch [1/10], Batch [550/938], Loss: 0.6080\n",
      "Epoch [1/10], Batch [551/938], Loss: 0.6236\n",
      "Epoch [1/10], Batch [552/938], Loss: 0.6065\n",
      "Epoch [1/10], Batch [553/938], Loss: 0.6369\n",
      "Epoch [1/10], Batch [554/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [555/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [556/938], Loss: 0.6040\n",
      "Epoch [1/10], Batch [557/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [558/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [559/938], Loss: 0.6219\n",
      "Epoch [1/10], Batch [560/938], Loss: 0.6101\n",
      "Epoch [1/10], Batch [561/938], Loss: 0.6253\n",
      "Epoch [1/10], Batch [562/938], Loss: 0.6285\n",
      "Epoch [1/10], Batch [563/938], Loss: 0.6182\n",
      "Epoch [1/10], Batch [564/938], Loss: 0.6163\n",
      "Epoch [1/10], Batch [565/938], Loss: 0.6362\n",
      "Epoch [1/10], Batch [566/938], Loss: 0.6266\n",
      "Epoch [1/10], Batch [567/938], Loss: 0.5927\n",
      "Epoch [1/10], Batch [568/938], Loss: 0.6032\n",
      "Epoch [1/10], Batch [569/938], Loss: 0.6314\n",
      "Epoch [1/10], Batch [570/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [571/938], Loss: 0.6077\n",
      "Epoch [1/10], Batch [572/938], Loss: 0.5873\n",
      "Epoch [1/10], Batch [573/938], Loss: 0.6111\n",
      "Epoch [1/10], Batch [574/938], Loss: 0.6016\n",
      "Epoch [1/10], Batch [575/938], Loss: 0.6171\n",
      "Epoch [1/10], Batch [576/938], Loss: 0.6230\n",
      "Epoch [1/10], Batch [577/938], Loss: 0.6148\n",
      "Epoch [1/10], Batch [578/938], Loss: 0.6470\n",
      "Epoch [1/10], Batch [579/938], Loss: 0.5941\n",
      "Epoch [1/10], Batch [580/938], Loss: 0.6134\n",
      "Epoch [1/10], Batch [581/938], Loss: 0.6077\n",
      "Epoch [1/10], Batch [582/938], Loss: 0.5926\n",
      "Epoch [1/10], Batch [583/938], Loss: 0.5977\n",
      "Epoch [1/10], Batch [584/938], Loss: 0.6045\n",
      "Epoch [1/10], Batch [585/938], Loss: 0.6142\n",
      "Epoch [1/10], Batch [586/938], Loss: 0.6185\n",
      "Epoch [1/10], Batch [587/938], Loss: 0.6013\n",
      "Epoch [1/10], Batch [588/938], Loss: 0.6203\n",
      "Epoch [1/10], Batch [589/938], Loss: 0.5964\n",
      "Epoch [1/10], Batch [590/938], Loss: 0.6229\n",
      "Epoch [1/10], Batch [591/938], Loss: 0.6287\n",
      "Epoch [1/10], Batch [592/938], Loss: 0.6061\n",
      "Epoch [1/10], Batch [593/938], Loss: 0.6150\n",
      "Epoch [1/10], Batch [594/938], Loss: 0.5900\n",
      "Epoch [1/10], Batch [595/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [596/938], Loss: 0.6238\n",
      "Epoch [1/10], Batch [597/938], Loss: 0.6037\n",
      "Epoch [1/10], Batch [598/938], Loss: 0.6222\n",
      "Epoch [1/10], Batch [599/938], Loss: 0.6083\n",
      "Epoch [1/10], Batch [600/938], Loss: 0.6046\n",
      "Epoch [1/10], Batch [601/938], Loss: 0.6439\n",
      "Epoch [1/10], Batch [602/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [603/938], Loss: 0.6008\n",
      "Epoch [1/10], Batch [604/938], Loss: 0.6145\n",
      "Epoch [1/10], Batch [605/938], Loss: 0.6125\n",
      "Epoch [1/10], Batch [606/938], Loss: 0.6034\n",
      "Epoch [1/10], Batch [607/938], Loss: 0.6308\n",
      "Epoch [1/10], Batch [608/938], Loss: 0.6193\n",
      "Epoch [1/10], Batch [609/938], Loss: 0.6110\n",
      "Epoch [1/10], Batch [610/938], Loss: 0.5883\n",
      "Epoch [1/10], Batch [611/938], Loss: 0.5883\n",
      "Epoch [1/10], Batch [612/938], Loss: 0.5883\n",
      "Epoch [1/10], Batch [613/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [614/938], Loss: 0.6286\n",
      "Epoch [1/10], Batch [615/938], Loss: 0.6499\n",
      "Epoch [1/10], Batch [616/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [617/938], Loss: 0.6231\n",
      "Epoch [1/10], Batch [618/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [619/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [620/938], Loss: 0.6148\n",
      "Epoch [1/10], Batch [621/938], Loss: 0.6171\n",
      "Epoch [1/10], Batch [622/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [623/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [624/938], Loss: 0.6360\n",
      "Epoch [1/10], Batch [625/938], Loss: 0.6369\n",
      "Epoch [1/10], Batch [626/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [627/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [628/938], Loss: 0.6214\n",
      "Epoch [1/10], Batch [629/938], Loss: 0.5804\n",
      "Epoch [1/10], Batch [630/938], Loss: 0.6080\n",
      "Epoch [1/10], Batch [631/938], Loss: 0.6062\n",
      "Epoch [1/10], Batch [632/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [633/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [634/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [635/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [636/938], Loss: 0.6169\n",
      "Epoch [1/10], Batch [637/938], Loss: 0.6020\n",
      "Epoch [1/10], Batch [638/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [639/938], Loss: 0.5804\n",
      "Epoch [1/10], Batch [640/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [641/938], Loss: 0.5979\n",
      "Epoch [1/10], Batch [642/938], Loss: 0.6134\n",
      "Epoch [1/10], Batch [643/938], Loss: 0.5944\n",
      "Epoch [1/10], Batch [644/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [645/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [646/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [647/938], Loss: 0.6478\n",
      "Epoch [1/10], Batch [648/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [649/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [650/938], Loss: 0.5804\n",
      "Epoch [1/10], Batch [651/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [652/938], Loss: 0.6290\n",
      "Epoch [1/10], Batch [653/938], Loss: 0.6131\n",
      "Epoch [1/10], Batch [654/938], Loss: 0.6465\n",
      "Epoch [1/10], Batch [655/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [656/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [657/938], Loss: 0.5969\n",
      "Epoch [1/10], Batch [658/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [659/938], Loss: 0.6369\n",
      "Epoch [1/10], Batch [660/938], Loss: 0.6177\n",
      "Epoch [1/10], Batch [661/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [662/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [663/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [664/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [665/938], Loss: 0.6290\n",
      "Epoch [1/10], Batch [666/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [667/938], Loss: 0.6425\n",
      "Epoch [1/10], Batch [668/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [669/938], Loss: 0.6041\n",
      "Epoch [1/10], Batch [670/938], Loss: 0.6506\n",
      "Epoch [1/10], Batch [671/938], Loss: 0.5830\n",
      "Epoch [1/10], Batch [672/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [673/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [674/938], Loss: 0.6108\n",
      "Epoch [1/10], Batch [675/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [676/938], Loss: 0.6427\n",
      "Epoch [1/10], Batch [677/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [678/938], Loss: 0.6230\n",
      "Epoch [1/10], Batch [679/938], Loss: 0.6270\n",
      "Epoch [1/10], Batch [680/938], Loss: 0.6135\n",
      "Epoch [1/10], Batch [681/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [682/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [683/938], Loss: 0.6117\n",
      "Epoch [1/10], Batch [684/938], Loss: 0.6081\n",
      "Epoch [1/10], Batch [685/938], Loss: 0.6238\n",
      "Epoch [1/10], Batch [686/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [687/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [688/938], Loss: 0.5837\n",
      "Epoch [1/10], Batch [689/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [690/938], Loss: 0.6230\n",
      "Epoch [1/10], Batch [691/938], Loss: 0.6287\n",
      "Epoch [1/10], Batch [692/938], Loss: 0.6152\n",
      "Epoch [1/10], Batch [693/938], Loss: 0.5668\n",
      "Epoch [1/10], Batch [694/938], Loss: 0.6286\n",
      "Epoch [1/10], Batch [695/938], Loss: 0.5816\n",
      "Epoch [1/10], Batch [696/938], Loss: 0.5986\n",
      "Epoch [1/10], Batch [697/938], Loss: 0.6113\n",
      "Epoch [1/10], Batch [698/938], Loss: 0.5970\n",
      "Epoch [1/10], Batch [699/938], Loss: 0.6024\n",
      "Epoch [1/10], Batch [700/938], Loss: 0.5744\n",
      "Epoch [1/10], Batch [701/938], Loss: 0.6112\n",
      "Epoch [1/10], Batch [702/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [703/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [704/938], Loss: 0.5944\n",
      "Epoch [1/10], Batch [705/938], Loss: 0.6644\n",
      "Epoch [1/10], Batch [706/938], Loss: 0.6024\n",
      "Epoch [1/10], Batch [707/938], Loss: 0.6492\n",
      "Epoch [1/10], Batch [708/938], Loss: 0.6109\n",
      "Epoch [1/10], Batch [709/938], Loss: 0.6211\n",
      "Epoch [1/10], Batch [710/938], Loss: 0.6172\n",
      "Epoch [1/10], Batch [711/938], Loss: 0.5902\n",
      "Epoch [1/10], Batch [712/938], Loss: 0.5963\n",
      "Epoch [1/10], Batch [713/938], Loss: 0.6016\n",
      "Epoch [1/10], Batch [714/938], Loss: 0.6311\n",
      "Epoch [1/10], Batch [715/938], Loss: 0.6242\n",
      "Epoch [1/10], Batch [716/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [717/938], Loss: 0.6138\n",
      "Epoch [1/10], Batch [718/938], Loss: 0.5762\n",
      "Epoch [1/10], Batch [719/938], Loss: 0.5882\n",
      "Epoch [1/10], Batch [720/938], Loss: 0.6283\n",
      "Epoch [1/10], Batch [721/938], Loss: 0.6390\n",
      "Epoch [1/10], Batch [722/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [723/938], Loss: 0.6014\n",
      "Epoch [1/10], Batch [724/938], Loss: 0.6296\n",
      "Epoch [1/10], Batch [725/938], Loss: 0.6250\n",
      "Epoch [1/10], Batch [726/938], Loss: 0.6250\n",
      "Epoch [1/10], Batch [727/938], Loss: 0.6379\n",
      "Epoch [1/10], Batch [728/938], Loss: 0.6027\n",
      "Epoch [1/10], Batch [729/938], Loss: 0.6227\n",
      "Epoch [1/10], Batch [730/938], Loss: 0.6172\n",
      "Epoch [1/10], Batch [731/938], Loss: 0.6274\n",
      "Epoch [1/10], Batch [732/938], Loss: 0.6043\n",
      "Epoch [1/10], Batch [733/938], Loss: 0.5939\n",
      "Epoch [1/10], Batch [734/938], Loss: 0.6314\n",
      "Epoch [1/10], Batch [735/938], Loss: 0.5817\n",
      "Epoch [1/10], Batch [736/938], Loss: 0.6058\n",
      "Epoch [1/10], Batch [737/938], Loss: 0.6018\n",
      "Epoch [1/10], Batch [738/938], Loss: 0.6425\n",
      "Epoch [1/10], Batch [739/938], Loss: 0.6254\n",
      "Epoch [1/10], Batch [740/938], Loss: 0.5738\n",
      "Epoch [1/10], Batch [741/938], Loss: 0.6048\n",
      "Epoch [1/10], Batch [742/938], Loss: 0.6252\n",
      "Epoch [1/10], Batch [743/938], Loss: 0.6097\n",
      "Epoch [1/10], Batch [744/938], Loss: 0.6017\n",
      "Epoch [1/10], Batch [745/938], Loss: 0.6216\n",
      "Epoch [1/10], Batch [746/938], Loss: 0.6292\n",
      "Epoch [1/10], Batch [747/938], Loss: 0.6080\n",
      "Epoch [1/10], Batch [748/938], Loss: 0.6228\n",
      "Epoch [1/10], Batch [749/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [750/938], Loss: 0.6230\n",
      "Epoch [1/10], Batch [751/938], Loss: 0.6287\n",
      "Epoch [1/10], Batch [752/938], Loss: 0.6042\n",
      "Epoch [1/10], Batch [753/938], Loss: 0.5923\n",
      "Epoch [1/10], Batch [754/938], Loss: 0.6300\n",
      "Epoch [1/10], Batch [755/938], Loss: 0.5735\n",
      "Epoch [1/10], Batch [756/938], Loss: 0.6360\n",
      "Epoch [1/10], Batch [757/938], Loss: 0.6300\n",
      "Epoch [1/10], Batch [758/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [759/938], Loss: 0.6023\n",
      "Epoch [1/10], Batch [760/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [761/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [762/938], Loss: 0.6230\n",
      "Epoch [1/10], Batch [763/938], Loss: 0.6087\n",
      "Epoch [1/10], Batch [764/938], Loss: 0.5987\n",
      "Epoch [1/10], Batch [765/938], Loss: 0.5897\n",
      "Epoch [1/10], Batch [766/938], Loss: 0.6153\n",
      "Epoch [1/10], Batch [767/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [768/938], Loss: 0.6290\n",
      "Epoch [1/10], Batch [769/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [770/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [771/938], Loss: 0.6178\n",
      "Epoch [1/10], Batch [772/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [773/938], Loss: 0.6290\n",
      "Epoch [1/10], Batch [774/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [775/938], Loss: 0.6290\n",
      "Epoch [1/10], Batch [776/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [777/938], Loss: 0.6359\n",
      "Epoch [1/10], Batch [778/938], Loss: 0.5874\n",
      "Epoch [1/10], Batch [779/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [780/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [781/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [782/938], Loss: 0.6118\n",
      "Epoch [1/10], Batch [783/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [784/938], Loss: 0.6291\n",
      "Epoch [1/10], Batch [785/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [786/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [787/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [788/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [789/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [790/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [791/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [792/938], Loss: 0.6029\n",
      "Epoch [1/10], Batch [793/938], Loss: 0.6317\n",
      "Epoch [1/10], Batch [794/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [795/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [796/938], Loss: 0.5735\n",
      "Epoch [1/10], Batch [797/938], Loss: 0.6040\n",
      "Epoch [1/10], Batch [798/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [799/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [800/938], Loss: 0.6039\n",
      "Epoch [1/10], Batch [801/938], Loss: 0.6894\n",
      "Epoch [1/10], Batch [802/938], Loss: 0.6094\n",
      "Epoch [1/10], Batch [803/938], Loss: 0.6038\n",
      "Epoch [1/10], Batch [804/938], Loss: 0.6429\n",
      "Epoch [1/10], Batch [805/938], Loss: 0.6429\n",
      "Epoch [1/10], Batch [806/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [807/938], Loss: 0.6052\n",
      "Epoch [1/10], Batch [808/938], Loss: 0.6039\n",
      "Epoch [1/10], Batch [809/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [810/938], Loss: 0.6369\n",
      "Epoch [1/10], Batch [811/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [812/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [813/938], Loss: 0.6496\n",
      "Epoch [1/10], Batch [814/938], Loss: 0.6076\n",
      "Epoch [1/10], Batch [815/938], Loss: 0.6108\n",
      "Epoch [1/10], Batch [816/938], Loss: 0.6372\n",
      "Epoch [1/10], Batch [817/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [818/938], Loss: 0.6160\n",
      "Epoch [1/10], Batch [819/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [820/938], Loss: 0.6108\n",
      "Epoch [1/10], Batch [821/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [822/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [823/938], Loss: 0.5966\n",
      "Epoch [1/10], Batch [824/938], Loss: 0.5955\n",
      "Epoch [1/10], Batch [825/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [826/938], Loss: 0.6508\n",
      "Epoch [1/10], Batch [827/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [828/938], Loss: 0.6174\n",
      "Epoch [1/10], Batch [829/938], Loss: 0.6160\n",
      "Epoch [1/10], Batch [830/938], Loss: 0.5831\n",
      "Epoch [1/10], Batch [831/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [832/938], Loss: 0.6033\n",
      "Epoch [1/10], Batch [833/938], Loss: 0.5983\n",
      "Epoch [1/10], Batch [834/938], Loss: 0.5996\n",
      "Epoch [1/10], Batch [835/938], Loss: 0.6202\n",
      "Epoch [1/10], Batch [836/938], Loss: 0.6017\n",
      "Epoch [1/10], Batch [837/938], Loss: 0.5804\n",
      "Epoch [1/10], Batch [838/938], Loss: 0.6094\n",
      "Epoch [1/10], Batch [839/938], Loss: 0.6119\n",
      "Epoch [1/10], Batch [840/938], Loss: 0.6071\n",
      "Epoch [1/10], Batch [841/938], Loss: 0.5868\n",
      "Epoch [1/10], Batch [842/938], Loss: 0.6001\n",
      "Epoch [1/10], Batch [843/938], Loss: 0.6250\n",
      "Epoch [1/10], Batch [844/938], Loss: 0.6040\n",
      "Epoch [1/10], Batch [845/938], Loss: 0.5952\n",
      "Epoch [1/10], Batch [846/938], Loss: 0.6279\n",
      "Epoch [1/10], Batch [847/938], Loss: 0.6294\n",
      "Epoch [1/10], Batch [848/938], Loss: 0.5872\n",
      "Epoch [1/10], Batch [849/938], Loss: 0.6398\n",
      "Epoch [1/10], Batch [850/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [851/938], Loss: 0.6078\n",
      "Epoch [1/10], Batch [852/938], Loss: 0.6294\n",
      "Epoch [1/10], Batch [853/938], Loss: 0.6186\n",
      "Epoch [1/10], Batch [854/938], Loss: 0.6105\n",
      "Epoch [1/10], Batch [855/938], Loss: 0.6370\n",
      "Epoch [1/10], Batch [856/938], Loss: 0.5968\n",
      "Epoch [1/10], Batch [857/938], Loss: 0.6036\n",
      "Epoch [1/10], Batch [858/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [859/938], Loss: 0.6027\n",
      "Epoch [1/10], Batch [860/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [861/938], Loss: 0.6299\n",
      "Epoch [1/10], Batch [862/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [863/938], Loss: 0.6111\n",
      "Epoch [1/10], Batch [864/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [865/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [866/938], Loss: 0.6232\n",
      "Epoch [1/10], Batch [867/938], Loss: 0.5875\n",
      "Epoch [1/10], Batch [868/938], Loss: 0.6619\n",
      "Epoch [1/10], Batch [869/938], Loss: 0.6209\n",
      "Epoch [1/10], Batch [870/938], Loss: 0.6300\n",
      "Epoch [1/10], Batch [871/938], Loss: 0.6199\n",
      "Epoch [1/10], Batch [872/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [873/938], Loss: 0.5715\n",
      "Epoch [1/10], Batch [874/938], Loss: 0.5746\n",
      "Epoch [1/10], Batch [875/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [876/938], Loss: 0.6016\n",
      "Epoch [1/10], Batch [877/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [878/938], Loss: 0.6115\n",
      "Epoch [1/10], Batch [879/938], Loss: 0.5883\n",
      "Epoch [1/10], Batch [880/938], Loss: 0.5890\n",
      "Epoch [1/10], Batch [881/938], Loss: 0.6022\n",
      "Epoch [1/10], Batch [882/938], Loss: 0.6009\n",
      "Epoch [1/10], Batch [883/938], Loss: 0.6108\n",
      "Epoch [1/10], Batch [884/938], Loss: 0.6185\n",
      "Epoch [1/10], Batch [885/938], Loss: 0.6213\n",
      "Epoch [1/10], Batch [886/938], Loss: 0.5953\n",
      "Epoch [1/10], Batch [887/938], Loss: 0.6105\n",
      "Epoch [1/10], Batch [888/938], Loss: 0.5998\n",
      "Epoch [1/10], Batch [889/938], Loss: 0.6011\n",
      "Epoch [1/10], Batch [890/938], Loss: 0.5804\n",
      "Epoch [1/10], Batch [891/938], Loss: 0.6200\n",
      "Epoch [1/10], Batch [892/938], Loss: 0.6195\n",
      "Epoch [1/10], Batch [893/938], Loss: 0.6015\n",
      "Epoch [1/10], Batch [894/938], Loss: 0.5934\n",
      "Epoch [1/10], Batch [895/938], Loss: 0.5850\n",
      "Epoch [1/10], Batch [896/938], Loss: 0.6146\n",
      "Epoch [1/10], Batch [897/938], Loss: 0.5910\n",
      "Epoch [1/10], Batch [898/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [899/938], Loss: 0.6013\n",
      "Epoch [1/10], Batch [900/938], Loss: 0.6178\n",
      "Epoch [1/10], Batch [901/938], Loss: 0.6198\n",
      "Epoch [1/10], Batch [902/938], Loss: 0.6161\n",
      "Epoch [1/10], Batch [903/938], Loss: 0.6257\n",
      "Epoch [1/10], Batch [904/938], Loss: 0.6025\n",
      "Epoch [1/10], Batch [905/938], Loss: 0.6114\n",
      "Epoch [1/10], Batch [906/938], Loss: 0.6245\n",
      "Epoch [1/10], Batch [907/938], Loss: 0.6300\n",
      "Epoch [1/10], Batch [908/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [909/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [910/938], Loss: 0.5996\n",
      "Epoch [1/10], Batch [911/938], Loss: 0.6112\n",
      "Epoch [1/10], Batch [912/938], Loss: 0.6295\n",
      "Epoch [1/10], Batch [913/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [914/938], Loss: 0.5961\n",
      "Epoch [1/10], Batch [915/938], Loss: 0.6451\n",
      "Epoch [1/10], Batch [916/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [917/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [918/938], Loss: 0.6223\n",
      "Epoch [1/10], Batch [919/938], Loss: 0.5930\n",
      "Epoch [1/10], Batch [920/938], Loss: 0.6221\n",
      "Epoch [1/10], Batch [921/938], Loss: 0.5871\n",
      "Epoch [1/10], Batch [922/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [923/938], Loss: 0.6082\n",
      "Epoch [1/10], Batch [924/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [925/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [926/938], Loss: 0.6439\n",
      "Epoch [1/10], Batch [927/938], Loss: 0.5943\n",
      "Epoch [1/10], Batch [928/938], Loss: 0.5804\n",
      "Epoch [1/10], Batch [929/938], Loss: 0.6147\n",
      "Epoch [1/10], Batch [930/938], Loss: 0.6204\n",
      "Epoch [1/10], Batch [931/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [932/938], Loss: 0.6151\n",
      "Epoch [1/10], Batch [933/938], Loss: 0.6092\n",
      "Epoch [1/10], Batch [934/938], Loss: 0.6432\n",
      "Epoch [1/10], Batch [935/938], Loss: 0.5996\n",
      "Epoch [1/10], Batch [936/938], Loss: 0.6138\n",
      "Epoch [1/10], Batch [937/938], Loss: 0.6012\n",
      "Epoch [1/10], Batch [938/938], Loss: 0.5665\n",
      "Epoch [1/10], Loss: 0.5665\n",
      "Epoch [2/10], Batch [1/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [2/938], Loss: 0.5867\n",
      "Epoch [2/10], Batch [3/938], Loss: 0.6022\n",
      "Epoch [2/10], Batch [4/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [5/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [6/938], Loss: 0.6290\n",
      "Epoch [2/10], Batch [7/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [8/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [9/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [10/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [11/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [12/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [13/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [14/938], Loss: 0.6032\n",
      "Epoch [2/10], Batch [15/938], Loss: 0.5919\n",
      "Epoch [2/10], Batch [16/938], Loss: 0.6426\n",
      "Epoch [2/10], Batch [17/938], Loss: 0.6014\n",
      "Epoch [2/10], Batch [18/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [19/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [20/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [21/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [22/938], Loss: 0.6022\n",
      "Epoch [2/10], Batch [23/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [24/938], Loss: 0.6009\n",
      "Epoch [2/10], Batch [25/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [26/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [27/938], Loss: 0.5953\n",
      "Epoch [2/10], Batch [28/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [29/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [30/938], Loss: 0.6022\n",
      "Epoch [2/10], Batch [31/938], Loss: 0.6443\n",
      "Epoch [2/10], Batch [32/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [33/938], Loss: 0.6168\n",
      "Epoch [2/10], Batch [34/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [35/938], Loss: 0.6170\n",
      "Epoch [2/10], Batch [36/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [37/938], Loss: 0.6290\n",
      "Epoch [2/10], Batch [38/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [39/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [40/938], Loss: 0.5996\n",
      "Epoch [2/10], Batch [41/938], Loss: 0.6229\n",
      "Epoch [2/10], Batch [42/938], Loss: 0.6135\n",
      "Epoch [2/10], Batch [43/938], Loss: 0.5949\n",
      "Epoch [2/10], Batch [44/938], Loss: 0.5914\n",
      "Epoch [2/10], Batch [45/938], Loss: 0.5735\n",
      "Epoch [2/10], Batch [46/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [47/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [48/938], Loss: 0.6368\n",
      "Epoch [2/10], Batch [49/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [50/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [51/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [52/938], Loss: 0.6242\n",
      "Epoch [2/10], Batch [53/938], Loss: 0.5873\n",
      "Epoch [2/10], Batch [54/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [55/938], Loss: 0.6176\n",
      "Epoch [2/10], Batch [56/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [57/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [58/938], Loss: 0.6360\n",
      "Epoch [2/10], Batch [59/938], Loss: 0.6013\n",
      "Epoch [2/10], Batch [60/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [61/938], Loss: 0.6498\n",
      "Epoch [2/10], Batch [62/938], Loss: 0.6290\n",
      "Epoch [2/10], Batch [63/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [64/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [65/938], Loss: 0.5873\n",
      "Epoch [2/10], Batch [66/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [67/938], Loss: 0.6287\n",
      "Epoch [2/10], Batch [68/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [69/938], Loss: 0.6298\n",
      "Epoch [2/10], Batch [70/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [71/938], Loss: 0.6401\n",
      "Epoch [2/10], Batch [72/938], Loss: 0.5900\n",
      "Epoch [2/10], Batch [73/938], Loss: 0.6288\n",
      "Epoch [2/10], Batch [74/938], Loss: 0.6081\n",
      "Epoch [2/10], Batch [75/938], Loss: 0.6011\n",
      "Epoch [2/10], Batch [76/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [77/938], Loss: 0.6218\n",
      "Epoch [2/10], Batch [78/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [79/938], Loss: 0.5876\n",
      "Epoch [2/10], Batch [80/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [81/938], Loss: 0.6014\n",
      "Epoch [2/10], Batch [82/938], Loss: 0.5793\n",
      "Epoch [2/10], Batch [83/938], Loss: 0.6164\n",
      "Epoch [2/10], Batch [84/938], Loss: 0.6258\n",
      "Epoch [2/10], Batch [85/938], Loss: 0.5914\n",
      "Epoch [2/10], Batch [86/938], Loss: 0.6341\n",
      "Epoch [2/10], Batch [87/938], Loss: 0.6058\n",
      "Epoch [2/10], Batch [88/938], Loss: 0.6192\n",
      "Epoch [2/10], Batch [89/938], Loss: 0.5782\n",
      "Epoch [2/10], Batch [90/938], Loss: 0.6043\n",
      "Epoch [2/10], Batch [91/938], Loss: 0.6227\n",
      "Epoch [2/10], Batch [92/938], Loss: 0.6238\n",
      "Epoch [2/10], Batch [93/938], Loss: 0.6104\n",
      "Epoch [2/10], Batch [94/938], Loss: 0.6066\n",
      "Epoch [2/10], Batch [95/938], Loss: 0.6654\n",
      "Epoch [2/10], Batch [96/938], Loss: 0.6566\n",
      "Epoch [2/10], Batch [97/938], Loss: 0.6211\n",
      "Epoch [2/10], Batch [98/938], Loss: 0.6153\n",
      "Epoch [2/10], Batch [99/938], Loss: 0.5659\n",
      "Epoch [2/10], Batch [100/938], Loss: 0.6216\n",
      "Epoch [2/10], Batch [101/938], Loss: 0.5803\n",
      "Epoch [2/10], Batch [102/938], Loss: 0.5817\n",
      "Epoch [2/10], Batch [103/938], Loss: 0.6254\n",
      "Epoch [2/10], Batch [104/938], Loss: 0.6530\n",
      "Epoch [2/10], Batch [105/938], Loss: 0.6305\n",
      "Epoch [2/10], Batch [106/938], Loss: 0.5926\n",
      "Epoch [2/10], Batch [107/938], Loss: 0.6147\n",
      "Epoch [2/10], Batch [108/938], Loss: 0.6413\n",
      "Epoch [2/10], Batch [109/938], Loss: 0.5970\n",
      "Epoch [2/10], Batch [110/938], Loss: 0.6093\n",
      "Epoch [2/10], Batch [111/938], Loss: 0.5993\n",
      "Epoch [2/10], Batch [112/938], Loss: 0.6152\n",
      "Epoch [2/10], Batch [113/938], Loss: 0.6405\n",
      "Epoch [2/10], Batch [114/938], Loss: 0.6061\n",
      "Epoch [2/10], Batch [115/938], Loss: 0.6087\n",
      "Epoch [2/10], Batch [116/938], Loss: 0.6155\n",
      "Epoch [2/10], Batch [117/938], Loss: 0.5873\n",
      "Epoch [2/10], Batch [118/938], Loss: 0.6091\n",
      "Epoch [2/10], Batch [119/938], Loss: 0.6211\n",
      "Epoch [2/10], Batch [120/938], Loss: 0.6139\n",
      "Epoch [2/10], Batch [121/938], Loss: 0.6069\n",
      "Epoch [2/10], Batch [122/938], Loss: 0.6262\n",
      "Epoch [2/10], Batch [123/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [124/938], Loss: 0.6137\n",
      "Epoch [2/10], Batch [125/938], Loss: 0.5925\n",
      "Epoch [2/10], Batch [126/938], Loss: 0.6252\n",
      "Epoch [2/10], Batch [127/938], Loss: 0.6222\n",
      "Epoch [2/10], Batch [128/938], Loss: 0.6193\n",
      "Epoch [2/10], Batch [129/938], Loss: 0.6119\n",
      "Epoch [2/10], Batch [130/938], Loss: 0.6152\n",
      "Epoch [2/10], Batch [131/938], Loss: 0.6022\n",
      "Epoch [2/10], Batch [132/938], Loss: 0.6109\n",
      "Epoch [2/10], Batch [133/938], Loss: 0.6248\n",
      "Epoch [2/10], Batch [134/938], Loss: 0.6240\n",
      "Epoch [2/10], Batch [135/938], Loss: 0.6318\n",
      "Epoch [2/10], Batch [136/938], Loss: 0.6286\n",
      "Epoch [2/10], Batch [137/938], Loss: 0.6161\n",
      "Epoch [2/10], Batch [138/938], Loss: 0.5996\n",
      "Epoch [2/10], Batch [139/938], Loss: 0.6230\n",
      "Epoch [2/10], Batch [140/938], Loss: 0.6233\n",
      "Epoch [2/10], Batch [141/938], Loss: 0.6257\n",
      "Epoch [2/10], Batch [142/938], Loss: 0.6146\n",
      "Epoch [2/10], Batch [143/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [144/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [145/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [146/938], Loss: 0.6175\n",
      "Epoch [2/10], Batch [147/938], Loss: 0.5857\n",
      "Epoch [2/10], Batch [148/938], Loss: 0.5953\n",
      "Epoch [2/10], Batch [149/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [150/938], Loss: 0.6007\n",
      "Epoch [2/10], Batch [151/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [152/938], Loss: 0.6462\n",
      "Epoch [2/10], Batch [153/938], Loss: 0.6009\n",
      "Epoch [2/10], Batch [154/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [155/938], Loss: 0.5954\n",
      "Epoch [2/10], Batch [156/938], Loss: 0.6161\n",
      "Epoch [2/10], Batch [157/938], Loss: 0.6066\n",
      "Epoch [2/10], Batch [158/938], Loss: 0.5996\n",
      "Epoch [2/10], Batch [159/938], Loss: 0.6164\n",
      "Epoch [2/10], Batch [160/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [161/938], Loss: 0.6161\n",
      "Epoch [2/10], Batch [162/938], Loss: 0.6066\n",
      "Epoch [2/10], Batch [163/938], Loss: 0.6124\n",
      "Epoch [2/10], Batch [164/938], Loss: 0.6101\n",
      "Epoch [2/10], Batch [165/938], Loss: 0.6013\n",
      "Epoch [2/10], Batch [166/938], Loss: 0.6006\n",
      "Epoch [2/10], Batch [167/938], Loss: 0.6439\n",
      "Epoch [2/10], Batch [168/938], Loss: 0.6463\n",
      "Epoch [2/10], Batch [169/938], Loss: 0.6091\n",
      "Epoch [2/10], Batch [170/938], Loss: 0.6231\n",
      "Epoch [2/10], Batch [171/938], Loss: 0.5969\n",
      "Epoch [2/10], Batch [172/938], Loss: 0.6019\n",
      "Epoch [2/10], Batch [173/938], Loss: 0.6227\n",
      "Epoch [2/10], Batch [174/938], Loss: 0.6357\n",
      "Epoch [2/10], Batch [175/938], Loss: 0.6138\n",
      "Epoch [2/10], Batch [176/938], Loss: 0.5952\n",
      "Epoch [2/10], Batch [177/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [178/938], Loss: 0.6230\n",
      "Epoch [2/10], Batch [179/938], Loss: 0.6109\n",
      "Epoch [2/10], Batch [180/938], Loss: 0.6219\n",
      "Epoch [2/10], Batch [181/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [182/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [183/938], Loss: 0.6204\n",
      "Epoch [2/10], Batch [184/938], Loss: 0.6120\n",
      "Epoch [2/10], Batch [185/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [186/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [187/938], Loss: 0.6250\n",
      "Epoch [2/10], Batch [188/938], Loss: 0.6043\n",
      "Epoch [2/10], Batch [189/938], Loss: 0.6255\n",
      "Epoch [2/10], Batch [190/938], Loss: 0.6028\n",
      "Epoch [2/10], Batch [191/938], Loss: 0.6075\n",
      "Epoch [2/10], Batch [192/938], Loss: 0.5883\n",
      "Epoch [2/10], Batch [193/938], Loss: 0.6002\n",
      "Epoch [2/10], Batch [194/938], Loss: 0.6178\n",
      "Epoch [2/10], Batch [195/938], Loss: 0.6101\n",
      "Epoch [2/10], Batch [196/938], Loss: 0.6227\n",
      "Epoch [2/10], Batch [197/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [198/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [199/938], Loss: 0.6461\n",
      "Epoch [2/10], Batch [200/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [201/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [202/938], Loss: 0.6250\n",
      "Epoch [2/10], Batch [203/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [204/938], Loss: 0.6300\n",
      "Epoch [2/10], Batch [205/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [206/938], Loss: 0.5814\n",
      "Epoch [2/10], Batch [207/938], Loss: 0.5953\n",
      "Epoch [2/10], Batch [208/938], Loss: 0.6393\n",
      "Epoch [2/10], Batch [209/938], Loss: 0.6646\n",
      "Epoch [2/10], Batch [210/938], Loss: 0.5875\n",
      "Epoch [2/10], Batch [211/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [212/938], Loss: 0.6075\n",
      "Epoch [2/10], Batch [213/938], Loss: 0.6439\n",
      "Epoch [2/10], Batch [214/938], Loss: 0.6065\n",
      "Epoch [2/10], Batch [215/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [216/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [217/938], Loss: 0.6173\n",
      "Epoch [2/10], Batch [218/938], Loss: 0.5953\n",
      "Epoch [2/10], Batch [219/938], Loss: 0.6032\n",
      "Epoch [2/10], Batch [220/938], Loss: 0.6079\n",
      "Epoch [2/10], Batch [221/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [222/938], Loss: 0.5951\n",
      "Epoch [2/10], Batch [223/938], Loss: 0.6186\n",
      "Epoch [2/10], Batch [224/938], Loss: 0.5944\n",
      "Epoch [2/10], Batch [225/938], Loss: 0.5735\n",
      "Epoch [2/10], Batch [226/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [227/938], Loss: 0.5867\n",
      "Epoch [2/10], Batch [228/938], Loss: 0.5883\n",
      "Epoch [2/10], Batch [229/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [230/938], Loss: 0.5952\n",
      "Epoch [2/10], Batch [231/938], Loss: 0.6032\n",
      "Epoch [2/10], Batch [232/938], Loss: 0.6107\n",
      "Epoch [2/10], Batch [233/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [234/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [235/938], Loss: 0.6172\n",
      "Epoch [2/10], Batch [236/938], Loss: 0.5937\n",
      "Epoch [2/10], Batch [237/938], Loss: 0.6095\n",
      "Epoch [2/10], Batch [238/938], Loss: 0.6404\n",
      "Epoch [2/10], Batch [239/938], Loss: 0.6025\n",
      "Epoch [2/10], Batch [240/938], Loss: 0.6211\n",
      "Epoch [2/10], Batch [241/938], Loss: 0.6311\n",
      "Epoch [2/10], Batch [242/938], Loss: 0.6339\n",
      "Epoch [2/10], Batch [243/938], Loss: 0.6022\n",
      "Epoch [2/10], Batch [244/938], Loss: 0.6008\n",
      "Epoch [2/10], Batch [245/938], Loss: 0.6269\n",
      "Epoch [2/10], Batch [246/938], Loss: 0.6231\n",
      "Epoch [2/10], Batch [247/938], Loss: 0.6191\n",
      "Epoch [2/10], Batch [248/938], Loss: 0.6369\n",
      "Epoch [2/10], Batch [249/938], Loss: 0.6148\n",
      "Epoch [2/10], Batch [250/938], Loss: 0.6493\n",
      "Epoch [2/10], Batch [251/938], Loss: 0.6427\n",
      "Epoch [2/10], Batch [252/938], Loss: 0.6007\n",
      "Epoch [2/10], Batch [253/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [254/938], Loss: 0.6063\n",
      "Epoch [2/10], Batch [255/938], Loss: 0.6128\n",
      "Epoch [2/10], Batch [256/938], Loss: 0.6273\n",
      "Epoch [2/10], Batch [257/938], Loss: 0.6396\n",
      "Epoch [2/10], Batch [258/938], Loss: 0.6171\n",
      "Epoch [2/10], Batch [259/938], Loss: 0.6230\n",
      "Epoch [2/10], Batch [260/938], Loss: 0.6107\n",
      "Epoch [2/10], Batch [261/938], Loss: 0.6161\n",
      "Epoch [2/10], Batch [262/938], Loss: 0.6118\n",
      "Epoch [2/10], Batch [263/938], Loss: 0.6254\n",
      "Epoch [2/10], Batch [264/938], Loss: 0.6116\n",
      "Epoch [2/10], Batch [265/938], Loss: 0.5921\n",
      "Epoch [2/10], Batch [266/938], Loss: 0.6128\n",
      "Epoch [2/10], Batch [267/938], Loss: 0.6098\n",
      "Epoch [2/10], Batch [268/938], Loss: 0.6304\n",
      "Epoch [2/10], Batch [269/938], Loss: 0.6056\n",
      "Epoch [2/10], Batch [270/938], Loss: 0.5906\n",
      "Epoch [2/10], Batch [271/938], Loss: 0.5822\n",
      "Epoch [2/10], Batch [272/938], Loss: 0.6100\n",
      "Epoch [2/10], Batch [273/938], Loss: 0.5949\n",
      "Epoch [2/10], Batch [274/938], Loss: 0.5880\n",
      "Epoch [2/10], Batch [275/938], Loss: 0.6247\n",
      "Epoch [2/10], Batch [276/938], Loss: 0.6536\n",
      "Epoch [2/10], Batch [277/938], Loss: 0.5976\n",
      "Epoch [2/10], Batch [278/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [279/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [280/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [281/938], Loss: 0.6387\n",
      "Epoch [2/10], Batch [282/938], Loss: 0.5996\n",
      "Epoch [2/10], Batch [283/938], Loss: 0.6345\n",
      "Epoch [2/10], Batch [284/938], Loss: 0.6369\n",
      "Epoch [2/10], Batch [285/938], Loss: 0.6093\n",
      "Epoch [2/10], Batch [286/938], Loss: 0.6350\n",
      "Epoch [2/10], Batch [287/938], Loss: 0.6022\n",
      "Epoch [2/10], Batch [288/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [289/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [290/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [291/938], Loss: 0.6317\n",
      "Epoch [2/10], Batch [292/938], Loss: 0.6089\n",
      "Epoch [2/10], Batch [293/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [294/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [295/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [296/938], Loss: 0.6273\n",
      "Epoch [2/10], Batch [297/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [298/938], Loss: 0.6022\n",
      "Epoch [2/10], Batch [299/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [300/938], Loss: 0.6360\n",
      "Epoch [2/10], Batch [301/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [302/938], Loss: 0.6463\n",
      "Epoch [2/10], Batch [303/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [304/938], Loss: 0.5735\n",
      "Epoch [2/10], Batch [305/938], Loss: 0.5883\n",
      "Epoch [2/10], Batch [306/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [307/938], Loss: 0.6286\n",
      "Epoch [2/10], Batch [308/938], Loss: 0.6482\n",
      "Epoch [2/10], Batch [309/938], Loss: 0.6039\n",
      "Epoch [2/10], Batch [310/938], Loss: 0.6210\n",
      "Epoch [2/10], Batch [311/938], Loss: 0.6490\n",
      "Epoch [2/10], Batch [312/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [313/938], Loss: 0.6230\n",
      "Epoch [2/10], Batch [314/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [315/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [316/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [317/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [318/938], Loss: 0.5735\n",
      "Epoch [2/10], Batch [319/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [320/938], Loss: 0.6101\n",
      "Epoch [2/10], Batch [321/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [322/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [323/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [324/938], Loss: 0.6056\n",
      "Epoch [2/10], Batch [325/938], Loss: 0.6198\n",
      "Epoch [2/10], Batch [326/938], Loss: 0.6022\n",
      "Epoch [2/10], Batch [327/938], Loss: 0.6290\n",
      "Epoch [2/10], Batch [328/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [329/938], Loss: 0.6290\n",
      "Epoch [2/10], Batch [330/938], Loss: 0.6182\n",
      "Epoch [2/10], Batch [331/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [332/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [333/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [334/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [335/938], Loss: 0.6290\n",
      "Epoch [2/10], Batch [336/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [337/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [338/938], Loss: 0.6136\n",
      "Epoch [2/10], Batch [339/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [340/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [341/938], Loss: 0.6317\n",
      "Epoch [2/10], Batch [342/938], Loss: 0.6062\n",
      "Epoch [2/10], Batch [343/938], Loss: 0.6328\n",
      "Epoch [2/10], Batch [344/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [345/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [346/938], Loss: 0.6039\n",
      "Epoch [2/10], Batch [347/938], Loss: 0.6231\n",
      "Epoch [2/10], Batch [348/938], Loss: 0.6124\n",
      "Epoch [2/10], Batch [349/938], Loss: 0.5918\n",
      "Epoch [2/10], Batch [350/938], Loss: 0.5969\n",
      "Epoch [2/10], Batch [351/938], Loss: 0.6161\n",
      "Epoch [2/10], Batch [352/938], Loss: 0.5902\n",
      "Epoch [2/10], Batch [353/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [354/938], Loss: 0.5857\n",
      "Epoch [2/10], Batch [355/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [356/938], Loss: 0.6174\n",
      "Epoch [2/10], Batch [357/938], Loss: 0.6169\n",
      "Epoch [2/10], Batch [358/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [359/938], Loss: 0.6089\n",
      "Epoch [2/10], Batch [360/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [361/938], Loss: 0.6158\n",
      "Epoch [2/10], Batch [362/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [363/938], Loss: 0.6170\n",
      "Epoch [2/10], Batch [364/938], Loss: 0.6268\n",
      "Epoch [2/10], Batch [365/938], Loss: 0.6305\n",
      "Epoch [2/10], Batch [366/938], Loss: 0.6177\n",
      "Epoch [2/10], Batch [367/938], Loss: 0.5900\n",
      "Epoch [2/10], Batch [368/938], Loss: 0.6079\n",
      "Epoch [2/10], Batch [369/938], Loss: 0.6191\n",
      "Epoch [2/10], Batch [370/938], Loss: 0.6053\n",
      "Epoch [2/10], Batch [371/938], Loss: 0.6302\n",
      "Epoch [2/10], Batch [372/938], Loss: 0.6018\n",
      "Epoch [2/10], Batch [373/938], Loss: 0.5996\n",
      "Epoch [2/10], Batch [374/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [375/938], Loss: 0.6131\n",
      "Epoch [2/10], Batch [376/938], Loss: 0.6230\n",
      "Epoch [2/10], Batch [377/938], Loss: 0.6310\n",
      "Epoch [2/10], Batch [378/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [379/938], Loss: 0.6178\n",
      "Epoch [2/10], Batch [380/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [381/938], Loss: 0.6300\n",
      "Epoch [2/10], Batch [382/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [383/938], Loss: 0.6657\n",
      "Epoch [2/10], Batch [384/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [385/938], Loss: 0.6247\n",
      "Epoch [2/10], Batch [386/938], Loss: 0.6200\n",
      "Epoch [2/10], Batch [387/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [388/938], Loss: 0.5969\n",
      "Epoch [2/10], Batch [389/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [390/938], Loss: 0.6360\n",
      "Epoch [2/10], Batch [391/938], Loss: 0.6007\n",
      "Epoch [2/10], Batch [392/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [393/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [394/938], Loss: 0.5962\n",
      "Epoch [2/10], Batch [395/938], Loss: 0.6339\n",
      "Epoch [2/10], Batch [396/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [397/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [398/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [399/938], Loss: 0.5952\n",
      "Epoch [2/10], Batch [400/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [401/938], Loss: 0.6264\n",
      "Epoch [2/10], Batch [402/938], Loss: 0.5952\n",
      "Epoch [2/10], Batch [403/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [404/938], Loss: 0.5856\n",
      "Epoch [2/10], Batch [405/938], Loss: 0.6295\n",
      "Epoch [2/10], Batch [406/938], Loss: 0.5981\n",
      "Epoch [2/10], Batch [407/938], Loss: 0.6108\n",
      "Epoch [2/10], Batch [408/938], Loss: 0.6332\n",
      "Epoch [2/10], Batch [409/938], Loss: 0.6161\n",
      "Epoch [2/10], Batch [410/938], Loss: 0.6049\n",
      "Epoch [2/10], Batch [411/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [412/938], Loss: 0.6312\n",
      "Epoch [2/10], Batch [413/938], Loss: 0.6073\n",
      "Epoch [2/10], Batch [414/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [415/938], Loss: 0.6066\n",
      "Epoch [2/10], Batch [416/938], Loss: 0.5997\n",
      "Epoch [2/10], Batch [417/938], Loss: 0.6109\n",
      "Epoch [2/10], Batch [418/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [419/938], Loss: 0.6198\n",
      "Epoch [2/10], Batch [420/938], Loss: 0.6409\n",
      "Epoch [2/10], Batch [421/938], Loss: 0.6130\n",
      "Epoch [2/10], Batch [422/938], Loss: 0.6318\n",
      "Epoch [2/10], Batch [423/938], Loss: 0.6017\n",
      "Epoch [2/10], Batch [424/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [425/938], Loss: 0.5857\n",
      "Epoch [2/10], Batch [426/938], Loss: 0.6249\n",
      "Epoch [2/10], Batch [427/938], Loss: 0.5953\n",
      "Epoch [2/10], Batch [428/938], Loss: 0.6006\n",
      "Epoch [2/10], Batch [429/938], Loss: 0.6156\n",
      "Epoch [2/10], Batch [430/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [431/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [432/938], Loss: 0.5866\n",
      "Epoch [2/10], Batch [433/938], Loss: 0.5990\n",
      "Epoch [2/10], Batch [434/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [435/938], Loss: 0.6382\n",
      "Epoch [2/10], Batch [436/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [437/938], Loss: 0.6114\n",
      "Epoch [2/10], Batch [438/938], Loss: 0.6133\n",
      "Epoch [2/10], Batch [439/938], Loss: 0.6119\n",
      "Epoch [2/10], Batch [440/938], Loss: 0.6022\n",
      "Epoch [2/10], Batch [441/938], Loss: 0.6246\n",
      "Epoch [2/10], Batch [442/938], Loss: 0.6308\n",
      "Epoch [2/10], Batch [443/938], Loss: 0.6253\n",
      "Epoch [2/10], Batch [444/938], Loss: 0.6177\n",
      "Epoch [2/10], Batch [445/938], Loss: 0.6218\n",
      "Epoch [2/10], Batch [446/938], Loss: 0.5929\n",
      "Epoch [2/10], Batch [447/938], Loss: 0.6187\n",
      "Epoch [2/10], Batch [448/938], Loss: 0.5993\n",
      "Epoch [2/10], Batch [449/938], Loss: 0.5875\n",
      "Epoch [2/10], Batch [450/938], Loss: 0.6163\n",
      "Epoch [2/10], Batch [451/938], Loss: 0.6058\n",
      "Epoch [2/10], Batch [452/938], Loss: 0.5913\n",
      "Epoch [2/10], Batch [453/938], Loss: 0.6021\n",
      "Epoch [2/10], Batch [454/938], Loss: 0.6113\n",
      "Epoch [2/10], Batch [455/938], Loss: 0.6106\n",
      "Epoch [2/10], Batch [456/938], Loss: 0.6047\n",
      "Epoch [2/10], Batch [457/938], Loss: 0.6263\n",
      "Epoch [2/10], Batch [458/938], Loss: 0.6138\n",
      "Epoch [2/10], Batch [459/938], Loss: 0.6016\n",
      "Epoch [2/10], Batch [460/938], Loss: 0.5966\n",
      "Epoch [2/10], Batch [461/938], Loss: 0.5934\n",
      "Epoch [2/10], Batch [462/938], Loss: 0.6254\n",
      "Epoch [2/10], Batch [463/938], Loss: 0.6033\n",
      "Epoch [2/10], Batch [464/938], Loss: 0.6248\n",
      "Epoch [2/10], Batch [465/938], Loss: 0.5936\n",
      "Epoch [2/10], Batch [466/938], Loss: 0.5807\n",
      "Epoch [2/10], Batch [467/938], Loss: 0.6069\n",
      "Epoch [2/10], Batch [468/938], Loss: 0.6230\n",
      "Epoch [2/10], Batch [469/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [470/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [471/938], Loss: 0.5955\n",
      "Epoch [2/10], Batch [472/938], Loss: 0.6256\n",
      "Epoch [2/10], Batch [473/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [474/938], Loss: 0.6170\n",
      "Epoch [2/10], Batch [475/938], Loss: 0.6086\n",
      "Epoch [2/10], Batch [476/938], Loss: 0.6041\n",
      "Epoch [2/10], Batch [477/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [478/938], Loss: 0.6429\n",
      "Epoch [2/10], Batch [479/938], Loss: 0.6429\n",
      "Epoch [2/10], Batch [480/938], Loss: 0.6349\n",
      "Epoch [2/10], Batch [481/938], Loss: 0.6133\n",
      "Epoch [2/10], Batch [482/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [483/938], Loss: 0.6149\n",
      "Epoch [2/10], Batch [484/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [485/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [486/938], Loss: 0.5856\n",
      "Epoch [2/10], Batch [487/938], Loss: 0.6130\n",
      "Epoch [2/10], Batch [488/938], Loss: 0.6426\n",
      "Epoch [2/10], Batch [489/938], Loss: 0.6137\n",
      "Epoch [2/10], Batch [490/938], Loss: 0.6009\n",
      "Epoch [2/10], Batch [491/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [492/938], Loss: 0.5953\n",
      "Epoch [2/10], Batch [493/938], Loss: 0.6188\n",
      "Epoch [2/10], Batch [494/938], Loss: 0.6005\n",
      "Epoch [2/10], Batch [495/938], Loss: 0.6138\n",
      "Epoch [2/10], Batch [496/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [497/938], Loss: 0.5901\n",
      "Epoch [2/10], Batch [498/938], Loss: 0.6283\n",
      "Epoch [2/10], Batch [499/938], Loss: 0.6183\n",
      "Epoch [2/10], Batch [500/938], Loss: 0.5876\n",
      "Epoch [2/10], Batch [501/938], Loss: 0.6135\n",
      "Epoch [2/10], Batch [502/938], Loss: 0.5884\n",
      "Epoch [2/10], Batch [503/938], Loss: 0.6334\n",
      "Epoch [2/10], Batch [504/938], Loss: 0.6250\n",
      "Epoch [2/10], Batch [505/938], Loss: 0.6026\n",
      "Epoch [2/10], Batch [506/938], Loss: 0.6010\n",
      "Epoch [2/10], Batch [507/938], Loss: 0.5972\n",
      "Epoch [2/10], Batch [508/938], Loss: 0.6156\n",
      "Epoch [2/10], Batch [509/938], Loss: 0.5956\n",
      "Epoch [2/10], Batch [510/938], Loss: 0.6004\n",
      "Epoch [2/10], Batch [511/938], Loss: 0.6457\n",
      "Epoch [2/10], Batch [512/938], Loss: 0.6018\n",
      "Epoch [2/10], Batch [513/938], Loss: 0.6468\n",
      "Epoch [2/10], Batch [514/938], Loss: 0.5954\n",
      "Epoch [2/10], Batch [515/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [516/938], Loss: 0.5795\n",
      "Epoch [2/10], Batch [517/938], Loss: 0.5855\n",
      "Epoch [2/10], Batch [518/938], Loss: 0.6269\n",
      "Epoch [2/10], Batch [519/938], Loss: 0.6047\n",
      "Epoch [2/10], Batch [520/938], Loss: 0.6023\n",
      "Epoch [2/10], Batch [521/938], Loss: 0.6091\n",
      "Epoch [2/10], Batch [522/938], Loss: 0.6253\n",
      "Epoch [2/10], Batch [523/938], Loss: 0.6197\n",
      "Epoch [2/10], Batch [524/938], Loss: 0.6213\n",
      "Epoch [2/10], Batch [525/938], Loss: 0.6156\n",
      "Epoch [2/10], Batch [526/938], Loss: 0.6145\n",
      "Epoch [2/10], Batch [527/938], Loss: 0.6297\n",
      "Epoch [2/10], Batch [528/938], Loss: 0.6459\n",
      "Epoch [2/10], Batch [529/938], Loss: 0.6423\n",
      "Epoch [2/10], Batch [530/938], Loss: 0.6257\n",
      "Epoch [2/10], Batch [531/938], Loss: 0.6044\n",
      "Epoch [2/10], Batch [532/938], Loss: 0.5951\n",
      "Epoch [2/10], Batch [533/938], Loss: 0.6041\n",
      "Epoch [2/10], Batch [534/938], Loss: 0.6243\n",
      "Epoch [2/10], Batch [535/938], Loss: 0.6128\n",
      "Epoch [2/10], Batch [536/938], Loss: 0.6185\n",
      "Epoch [2/10], Batch [537/938], Loss: 0.6052\n",
      "Epoch [2/10], Batch [538/938], Loss: 0.6175\n",
      "Epoch [2/10], Batch [539/938], Loss: 0.6054\n",
      "Epoch [2/10], Batch [540/938], Loss: 0.6122\n",
      "Epoch [2/10], Batch [541/938], Loss: 0.6329\n",
      "Epoch [2/10], Batch [542/938], Loss: 0.5929\n",
      "Epoch [2/10], Batch [543/938], Loss: 0.6214\n",
      "Epoch [2/10], Batch [544/938], Loss: 0.6310\n",
      "Epoch [2/10], Batch [545/938], Loss: 0.5926\n",
      "Epoch [2/10], Batch [546/938], Loss: 0.6168\n",
      "Epoch [2/10], Batch [547/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [548/938], Loss: 0.6188\n",
      "Epoch [2/10], Batch [549/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [550/938], Loss: 0.6200\n",
      "Epoch [2/10], Batch [551/938], Loss: 0.5900\n",
      "Epoch [2/10], Batch [552/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [553/938], Loss: 0.6412\n",
      "Epoch [2/10], Batch [554/938], Loss: 0.5965\n",
      "Epoch [2/10], Batch [555/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [556/938], Loss: 0.6143\n",
      "Epoch [2/10], Batch [557/938], Loss: 0.6034\n",
      "Epoch [2/10], Batch [558/938], Loss: 0.6296\n",
      "Epoch [2/10], Batch [559/938], Loss: 0.5923\n",
      "Epoch [2/10], Batch [560/938], Loss: 0.6230\n",
      "Epoch [2/10], Batch [561/938], Loss: 0.6268\n",
      "Epoch [2/10], Batch [562/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [563/938], Loss: 0.6312\n",
      "Epoch [2/10], Batch [564/938], Loss: 0.6465\n",
      "Epoch [2/10], Batch [565/938], Loss: 0.6111\n",
      "Epoch [2/10], Batch [566/938], Loss: 0.5913\n",
      "Epoch [2/10], Batch [567/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [568/938], Loss: 0.6187\n",
      "Epoch [2/10], Batch [569/938], Loss: 0.6039\n",
      "Epoch [2/10], Batch [570/938], Loss: 0.6118\n",
      "Epoch [2/10], Batch [571/938], Loss: 0.6326\n",
      "Epoch [2/10], Batch [572/938], Loss: 0.6034\n",
      "Epoch [2/10], Batch [573/938], Loss: 0.6057\n",
      "Epoch [2/10], Batch [574/938], Loss: 0.6195\n",
      "Epoch [2/10], Batch [575/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [576/938], Loss: 0.5944\n",
      "Epoch [2/10], Batch [577/938], Loss: 0.6106\n",
      "Epoch [2/10], Batch [578/938], Loss: 0.6225\n",
      "Epoch [2/10], Batch [579/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [580/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [581/938], Loss: 0.6268\n",
      "Epoch [2/10], Batch [582/938], Loss: 0.6451\n",
      "Epoch [2/10], Batch [583/938], Loss: 0.6232\n",
      "Epoch [2/10], Batch [584/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [585/938], Loss: 0.6080\n",
      "Epoch [2/10], Batch [586/938], Loss: 0.6359\n",
      "Epoch [2/10], Batch [587/938], Loss: 0.5995\n",
      "Epoch [2/10], Batch [588/938], Loss: 0.6369\n",
      "Epoch [2/10], Batch [589/938], Loss: 0.6109\n",
      "Epoch [2/10], Batch [590/938], Loss: 0.6174\n",
      "Epoch [2/10], Batch [591/938], Loss: 0.5926\n",
      "Epoch [2/10], Batch [592/938], Loss: 0.6272\n",
      "Epoch [2/10], Batch [593/938], Loss: 0.5732\n",
      "Epoch [2/10], Batch [594/938], Loss: 0.5910\n",
      "Epoch [2/10], Batch [595/938], Loss: 0.5870\n",
      "Epoch [2/10], Batch [596/938], Loss: 0.5814\n",
      "Epoch [2/10], Batch [597/938], Loss: 0.6067\n",
      "Epoch [2/10], Batch [598/938], Loss: 0.6118\n",
      "Epoch [2/10], Batch [599/938], Loss: 0.5916\n",
      "Epoch [2/10], Batch [600/938], Loss: 0.5677\n",
      "Epoch [2/10], Batch [601/938], Loss: 0.6209\n",
      "Epoch [2/10], Batch [602/938], Loss: 0.6074\n",
      "Epoch [2/10], Batch [603/938], Loss: 0.6127\n",
      "Epoch [2/10], Batch [604/938], Loss: 0.6048\n",
      "Epoch [2/10], Batch [605/938], Loss: 0.6024\n",
      "Epoch [2/10], Batch [606/938], Loss: 0.6262\n",
      "Epoch [2/10], Batch [607/938], Loss: 0.6217\n",
      "Epoch [2/10], Batch [608/938], Loss: 0.6172\n",
      "Epoch [2/10], Batch [609/938], Loss: 0.6240\n",
      "Epoch [2/10], Batch [610/938], Loss: 0.6013\n",
      "Epoch [2/10], Batch [611/938], Loss: 0.6261\n",
      "Epoch [2/10], Batch [612/938], Loss: 0.5853\n",
      "Epoch [2/10], Batch [613/938], Loss: 0.5895\n",
      "Epoch [2/10], Batch [614/938], Loss: 0.5851\n",
      "Epoch [2/10], Batch [615/938], Loss: 0.5908\n",
      "Epoch [2/10], Batch [616/938], Loss: 0.6041\n",
      "Epoch [2/10], Batch [617/938], Loss: 0.6123\n",
      "Epoch [2/10], Batch [618/938], Loss: 0.6228\n",
      "Epoch [2/10], Batch [619/938], Loss: 0.5955\n",
      "Epoch [2/10], Batch [620/938], Loss: 0.6175\n",
      "Epoch [2/10], Batch [621/938], Loss: 0.6170\n",
      "Epoch [2/10], Batch [622/938], Loss: 0.5987\n",
      "Epoch [2/10], Batch [623/938], Loss: 0.5944\n",
      "Epoch [2/10], Batch [624/938], Loss: 0.6078\n",
      "Epoch [2/10], Batch [625/938], Loss: 0.6313\n",
      "Epoch [2/10], Batch [626/938], Loss: 0.6481\n",
      "Epoch [2/10], Batch [627/938], Loss: 0.5979\n",
      "Epoch [2/10], Batch [628/938], Loss: 0.6009\n",
      "Epoch [2/10], Batch [629/938], Loss: 0.6064\n",
      "Epoch [2/10], Batch [630/938], Loss: 0.6039\n",
      "Epoch [2/10], Batch [631/938], Loss: 0.6102\n",
      "Epoch [2/10], Batch [632/938], Loss: 0.5922\n",
      "Epoch [2/10], Batch [633/938], Loss: 0.5764\n",
      "Epoch [2/10], Batch [634/938], Loss: 0.5877\n",
      "Epoch [2/10], Batch [635/938], Loss: 0.5990\n",
      "Epoch [2/10], Batch [636/938], Loss: 0.5972\n",
      "Epoch [2/10], Batch [637/938], Loss: 0.6155\n",
      "Epoch [2/10], Batch [638/938], Loss: 0.5930\n",
      "Epoch [2/10], Batch [639/938], Loss: 0.5942\n",
      "Epoch [2/10], Batch [640/938], Loss: 0.5902\n",
      "Epoch [2/10], Batch [641/938], Loss: 0.5792\n",
      "Epoch [2/10], Batch [642/938], Loss: 0.6132\n",
      "Epoch [2/10], Batch [643/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [644/938], Loss: 0.5985\n",
      "Epoch [2/10], Batch [645/938], Loss: 0.5839\n",
      "Epoch [2/10], Batch [646/938], Loss: 0.5919\n",
      "Epoch [2/10], Batch [647/938], Loss: 0.6090\n",
      "Epoch [2/10], Batch [648/938], Loss: 0.5910\n",
      "Epoch [2/10], Batch [649/938], Loss: 0.6134\n",
      "Epoch [2/10], Batch [650/938], Loss: 0.5981\n",
      "Epoch [2/10], Batch [651/938], Loss: 0.5792\n",
      "Epoch [2/10], Batch [652/938], Loss: 0.5978\n",
      "Epoch [2/10], Batch [653/938], Loss: 0.5862\n",
      "Epoch [2/10], Batch [654/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [655/938], Loss: 0.5707\n",
      "Epoch [2/10], Batch [656/938], Loss: 0.5653\n",
      "Epoch [2/10], Batch [657/938], Loss: 0.5974\n",
      "Epoch [2/10], Batch [658/938], Loss: 0.5798\n",
      "Epoch [2/10], Batch [659/938], Loss: 0.5966\n",
      "Epoch [2/10], Batch [660/938], Loss: 0.5881\n",
      "Epoch [2/10], Batch [661/938], Loss: 0.5763\n",
      "Epoch [2/10], Batch [662/938], Loss: 0.6054\n",
      "Epoch [2/10], Batch [663/938], Loss: 0.5862\n",
      "Epoch [2/10], Batch [664/938], Loss: 0.5951\n",
      "Epoch [2/10], Batch [665/938], Loss: 0.6040\n",
      "Epoch [2/10], Batch [666/938], Loss: 0.5757\n",
      "Epoch [2/10], Batch [667/938], Loss: 0.5989\n",
      "Epoch [2/10], Batch [668/938], Loss: 0.6245\n",
      "Epoch [2/10], Batch [669/938], Loss: 0.5747\n",
      "Epoch [2/10], Batch [670/938], Loss: 0.5507\n",
      "Epoch [2/10], Batch [671/938], Loss: 0.6205\n",
      "Epoch [2/10], Batch [672/938], Loss: 0.5652\n",
      "Epoch [2/10], Batch [673/938], Loss: 0.5745\n",
      "Epoch [2/10], Batch [674/938], Loss: 0.5910\n",
      "Epoch [2/10], Batch [675/938], Loss: 0.5842\n",
      "Epoch [2/10], Batch [676/938], Loss: 0.6244\n",
      "Epoch [2/10], Batch [677/938], Loss: 0.6028\n",
      "Epoch [2/10], Batch [678/938], Loss: 0.6131\n",
      "Epoch [2/10], Batch [679/938], Loss: 0.6099\n",
      "Epoch [2/10], Batch [680/938], Loss: 0.6191\n",
      "Epoch [2/10], Batch [681/938], Loss: 0.5770\n",
      "Epoch [2/10], Batch [682/938], Loss: 0.5993\n",
      "Epoch [2/10], Batch [683/938], Loss: 0.6397\n",
      "Epoch [2/10], Batch [684/938], Loss: 0.5820\n",
      "Epoch [2/10], Batch [685/938], Loss: 0.5982\n",
      "Epoch [2/10], Batch [686/938], Loss: 0.6159\n",
      "Epoch [2/10], Batch [687/938], Loss: 0.5750\n",
      "Epoch [2/10], Batch [688/938], Loss: 0.6070\n",
      "Epoch [2/10], Batch [689/938], Loss: 0.6238\n",
      "Epoch [2/10], Batch [690/938], Loss: 0.6158\n",
      "Epoch [2/10], Batch [691/938], Loss: 0.5800\n",
      "Epoch [2/10], Batch [692/938], Loss: 0.5923\n",
      "Epoch [2/10], Batch [693/938], Loss: 0.5990\n",
      "Epoch [2/10], Batch [694/938], Loss: 0.6321\n",
      "Epoch [2/10], Batch [695/938], Loss: 0.5875\n",
      "Epoch [2/10], Batch [696/938], Loss: 0.5769\n",
      "Epoch [2/10], Batch [697/938], Loss: 0.6003\n",
      "Epoch [2/10], Batch [698/938], Loss: 0.6222\n",
      "Epoch [2/10], Batch [699/938], Loss: 0.5644\n",
      "Epoch [2/10], Batch [700/938], Loss: 0.6010\n",
      "Epoch [2/10], Batch [701/938], Loss: 0.6218\n",
      "Epoch [2/10], Batch [702/938], Loss: 0.6058\n",
      "Epoch [2/10], Batch [703/938], Loss: 0.6246\n",
      "Epoch [2/10], Batch [704/938], Loss: 0.5734\n",
      "Epoch [2/10], Batch [705/938], Loss: 0.5829\n",
      "Epoch [2/10], Batch [706/938], Loss: 0.6293\n",
      "Epoch [2/10], Batch [707/938], Loss: 0.6057\n",
      "Epoch [2/10], Batch [708/938], Loss: 0.6143\n",
      "Epoch [2/10], Batch [709/938], Loss: 0.6258\n",
      "Epoch [2/10], Batch [710/938], Loss: 0.6178\n",
      "Epoch [2/10], Batch [711/938], Loss: 0.6326\n",
      "Epoch [2/10], Batch [712/938], Loss: 0.6291\n",
      "Epoch [2/10], Batch [713/938], Loss: 0.5939\n",
      "Epoch [2/10], Batch [714/938], Loss: 0.6017\n",
      "Epoch [2/10], Batch [715/938], Loss: 0.5937\n",
      "Epoch [2/10], Batch [716/938], Loss: 0.6093\n",
      "Epoch [2/10], Batch [717/938], Loss: 0.6269\n",
      "Epoch [2/10], Batch [718/938], Loss: 0.6160\n",
      "Epoch [2/10], Batch [719/938], Loss: 0.6050\n",
      "Epoch [2/10], Batch [720/938], Loss: 0.6230\n",
      "Epoch [2/10], Batch [721/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [722/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [723/938], Loss: 0.6031\n",
      "Epoch [2/10], Batch [724/938], Loss: 0.5812\n",
      "Epoch [2/10], Batch [725/938], Loss: 0.5892\n",
      "Epoch [2/10], Batch [726/938], Loss: 0.6107\n",
      "Epoch [2/10], Batch [727/938], Loss: 0.6101\n",
      "Epoch [2/10], Batch [728/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [729/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [730/938], Loss: 0.6247\n",
      "Epoch [2/10], Batch [731/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [732/938], Loss: 0.6161\n",
      "Epoch [2/10], Batch [733/938], Loss: 0.5883\n",
      "Epoch [2/10], Batch [734/938], Loss: 0.6045\n",
      "Epoch [2/10], Batch [735/938], Loss: 0.6130\n",
      "Epoch [2/10], Batch [736/938], Loss: 0.6247\n",
      "Epoch [2/10], Batch [737/938], Loss: 0.5901\n",
      "Epoch [2/10], Batch [738/938], Loss: 0.6087\n",
      "Epoch [2/10], Batch [739/938], Loss: 0.6111\n",
      "Epoch [2/10], Batch [740/938], Loss: 0.6050\n",
      "Epoch [2/10], Batch [741/938], Loss: 0.6066\n",
      "Epoch [2/10], Batch [742/938], Loss: 0.6300\n",
      "Epoch [2/10], Batch [743/938], Loss: 0.5988\n",
      "Epoch [2/10], Batch [744/938], Loss: 0.5995\n",
      "Epoch [2/10], Batch [745/938], Loss: 0.6250\n",
      "Epoch [2/10], Batch [746/938], Loss: 0.6041\n",
      "Epoch [2/10], Batch [747/938], Loss: 0.6101\n",
      "Epoch [2/10], Batch [748/938], Loss: 0.6170\n",
      "Epoch [2/10], Batch [749/938], Loss: 0.6083\n",
      "Epoch [2/10], Batch [750/938], Loss: 0.6291\n",
      "Epoch [2/10], Batch [751/938], Loss: 0.5894\n",
      "Epoch [2/10], Batch [752/938], Loss: 0.5935\n",
      "Epoch [2/10], Batch [753/938], Loss: 0.6113\n",
      "Epoch [2/10], Batch [754/938], Loss: 0.6063\n",
      "Epoch [2/10], Batch [755/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [756/938], Loss: 0.5950\n",
      "Epoch [2/10], Batch [757/938], Loss: 0.5876\n",
      "Epoch [2/10], Batch [758/938], Loss: 0.6046\n",
      "Epoch [2/10], Batch [759/938], Loss: 0.5908\n",
      "Epoch [2/10], Batch [760/938], Loss: 0.5855\n",
      "Epoch [2/10], Batch [761/938], Loss: 0.6507\n",
      "Epoch [2/10], Batch [762/938], Loss: 0.6296\n",
      "Epoch [2/10], Batch [763/938], Loss: 0.5920\n",
      "Epoch [2/10], Batch [764/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [765/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [766/938], Loss: 0.6144\n",
      "Epoch [2/10], Batch [767/938], Loss: 0.6318\n",
      "Epoch [2/10], Batch [768/938], Loss: 0.5981\n",
      "Epoch [2/10], Batch [769/938], Loss: 0.6193\n",
      "Epoch [2/10], Batch [770/938], Loss: 0.6007\n",
      "Epoch [2/10], Batch [771/938], Loss: 0.6422\n",
      "Epoch [2/10], Batch [772/938], Loss: 0.6016\n",
      "Epoch [2/10], Batch [773/938], Loss: 0.5875\n",
      "Epoch [2/10], Batch [774/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [775/938], Loss: 0.6083\n",
      "Epoch [2/10], Batch [776/938], Loss: 0.5959\n",
      "Epoch [2/10], Batch [777/938], Loss: 0.6204\n",
      "Epoch [2/10], Batch [778/938], Loss: 0.5948\n",
      "Epoch [2/10], Batch [779/938], Loss: 0.6101\n",
      "Epoch [2/10], Batch [780/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [781/938], Loss: 0.6180\n",
      "Epoch [2/10], Batch [782/938], Loss: 0.6199\n",
      "Epoch [2/10], Batch [783/938], Loss: 0.5996\n",
      "Epoch [2/10], Batch [784/938], Loss: 0.6006\n",
      "Epoch [2/10], Batch [785/938], Loss: 0.6108\n",
      "Epoch [2/10], Batch [786/938], Loss: 0.6161\n",
      "Epoch [2/10], Batch [787/938], Loss: 0.5979\n",
      "Epoch [2/10], Batch [788/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [789/938], Loss: 0.5854\n",
      "Epoch [2/10], Batch [790/938], Loss: 0.6168\n",
      "Epoch [2/10], Batch [791/938], Loss: 0.6268\n",
      "Epoch [2/10], Batch [792/938], Loss: 0.6096\n",
      "Epoch [2/10], Batch [793/938], Loss: 0.6207\n",
      "Epoch [2/10], Batch [794/938], Loss: 0.6316\n",
      "Epoch [2/10], Batch [795/938], Loss: 0.6264\n",
      "Epoch [2/10], Batch [796/938], Loss: 0.6072\n",
      "Epoch [2/10], Batch [797/938], Loss: 0.6164\n",
      "Epoch [2/10], Batch [798/938], Loss: 0.6222\n",
      "Epoch [2/10], Batch [799/938], Loss: 0.5844\n",
      "Epoch [2/10], Batch [800/938], Loss: 0.5834\n",
      "Epoch [2/10], Batch [801/938], Loss: 0.5741\n",
      "Epoch [2/10], Batch [802/938], Loss: 0.6251\n",
      "Epoch [2/10], Batch [803/938], Loss: 0.6065\n",
      "Epoch [2/10], Batch [804/938], Loss: 0.5978\n",
      "Epoch [2/10], Batch [805/938], Loss: 0.6023\n",
      "Epoch [2/10], Batch [806/938], Loss: 0.6161\n",
      "Epoch [2/10], Batch [807/938], Loss: 0.6422\n",
      "Epoch [2/10], Batch [808/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [809/938], Loss: 0.6499\n",
      "Epoch [2/10], Batch [810/938], Loss: 0.6369\n",
      "Epoch [2/10], Batch [811/938], Loss: 0.6178\n",
      "Epoch [2/10], Batch [812/938], Loss: 0.6102\n",
      "Epoch [2/10], Batch [813/938], Loss: 0.5883\n",
      "Epoch [2/10], Batch [814/938], Loss: 0.6092\n",
      "Epoch [2/10], Batch [815/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [816/938], Loss: 0.6080\n",
      "Epoch [2/10], Batch [817/938], Loss: 0.6178\n",
      "Epoch [2/10], Batch [818/938], Loss: 0.6022\n",
      "Epoch [2/10], Batch [819/938], Loss: 0.6018\n",
      "Epoch [2/10], Batch [820/938], Loss: 0.6101\n",
      "Epoch [2/10], Batch [821/938], Loss: 0.5857\n",
      "Epoch [2/10], Batch [822/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [823/938], Loss: 0.6300\n",
      "Epoch [2/10], Batch [824/938], Loss: 0.5761\n",
      "Epoch [2/10], Batch [825/938], Loss: 0.5969\n",
      "Epoch [2/10], Batch [826/938], Loss: 0.5946\n",
      "Epoch [2/10], Batch [827/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [828/938], Loss: 0.6065\n",
      "Epoch [2/10], Batch [829/938], Loss: 0.6093\n",
      "Epoch [2/10], Batch [830/938], Loss: 0.6114\n",
      "Epoch [2/10], Batch [831/938], Loss: 0.6039\n",
      "Epoch [2/10], Batch [832/938], Loss: 0.6070\n",
      "Epoch [2/10], Batch [833/938], Loss: 0.6135\n",
      "Epoch [2/10], Batch [834/938], Loss: 0.5955\n",
      "Epoch [2/10], Batch [835/938], Loss: 0.6116\n",
      "Epoch [2/10], Batch [836/938], Loss: 0.6143\n",
      "Epoch [2/10], Batch [837/938], Loss: 0.5927\n",
      "Epoch [2/10], Batch [838/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [839/938], Loss: 0.6247\n",
      "Epoch [2/10], Batch [840/938], Loss: 0.6022\n",
      "Epoch [2/10], Batch [841/938], Loss: 0.6039\n",
      "Epoch [2/10], Batch [842/938], Loss: 0.6132\n",
      "Epoch [2/10], Batch [843/938], Loss: 0.6135\n",
      "Epoch [2/10], Batch [844/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [845/938], Loss: 0.6144\n",
      "Epoch [2/10], Batch [846/938], Loss: 0.6290\n",
      "Epoch [2/10], Batch [847/938], Loss: 0.5761\n",
      "Epoch [2/10], Batch [848/938], Loss: 0.6006\n",
      "Epoch [2/10], Batch [849/938], Loss: 0.5926\n",
      "Epoch [2/10], Batch [850/938], Loss: 0.6309\n",
      "Epoch [2/10], Batch [851/938], Loss: 0.6111\n",
      "Epoch [2/10], Batch [852/938], Loss: 0.6151\n",
      "Epoch [2/10], Batch [853/938], Loss: 0.6023\n",
      "Epoch [2/10], Batch [854/938], Loss: 0.6221\n",
      "Epoch [2/10], Batch [855/938], Loss: 0.6407\n",
      "Epoch [2/10], Batch [856/938], Loss: 0.6225\n",
      "Epoch [2/10], Batch [857/938], Loss: 0.5987\n",
      "Epoch [2/10], Batch [858/938], Loss: 0.6107\n",
      "Epoch [2/10], Batch [859/938], Loss: 0.5782\n",
      "Epoch [2/10], Batch [860/938], Loss: 0.6204\n",
      "Epoch [2/10], Batch [861/938], Loss: 0.5814\n",
      "Epoch [2/10], Batch [862/938], Loss: 0.6135\n",
      "Epoch [2/10], Batch [863/938], Loss: 0.6014\n",
      "Epoch [2/10], Batch [864/938], Loss: 0.6232\n",
      "Epoch [2/10], Batch [865/938], Loss: 0.6409\n",
      "Epoch [2/10], Batch [866/938], Loss: 0.5900\n",
      "Epoch [2/10], Batch [867/938], Loss: 0.6023\n",
      "Epoch [2/10], Batch [868/938], Loss: 0.5900\n",
      "Epoch [2/10], Batch [869/938], Loss: 0.6309\n",
      "Epoch [2/10], Batch [870/938], Loss: 0.6452\n",
      "Epoch [2/10], Batch [871/938], Loss: 0.6259\n",
      "Epoch [2/10], Batch [872/938], Loss: 0.6318\n",
      "Epoch [2/10], Batch [873/938], Loss: 0.6099\n",
      "Epoch [2/10], Batch [874/938], Loss: 0.6465\n",
      "Epoch [2/10], Batch [875/938], Loss: 0.6001\n",
      "Epoch [2/10], Batch [876/938], Loss: 0.6283\n",
      "Epoch [2/10], Batch [877/938], Loss: 0.6219\n",
      "Epoch [2/10], Batch [878/938], Loss: 0.5980\n",
      "Epoch [2/10], Batch [879/938], Loss: 0.6263\n",
      "Epoch [2/10], Batch [880/938], Loss: 0.5943\n",
      "Epoch [2/10], Batch [881/938], Loss: 0.6226\n",
      "Epoch [2/10], Batch [882/938], Loss: 0.5925\n",
      "Epoch [2/10], Batch [883/938], Loss: 0.6094\n",
      "Epoch [2/10], Batch [884/938], Loss: 0.5953\n",
      "Epoch [2/10], Batch [885/938], Loss: 0.6003\n",
      "Epoch [2/10], Batch [886/938], Loss: 0.5969\n",
      "Epoch [2/10], Batch [887/938], Loss: 0.5985\n",
      "Epoch [2/10], Batch [888/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [889/938], Loss: 0.5805\n",
      "Epoch [2/10], Batch [890/938], Loss: 0.5900\n",
      "Epoch [2/10], Batch [891/938], Loss: 0.5910\n",
      "Epoch [2/10], Batch [892/938], Loss: 0.5814\n",
      "Epoch [2/10], Batch [893/938], Loss: 0.6131\n",
      "Epoch [2/10], Batch [894/938], Loss: 0.6161\n",
      "Epoch [2/10], Batch [895/938], Loss: 0.6187\n",
      "Epoch [2/10], Batch [896/938], Loss: 0.5926\n",
      "Epoch [2/10], Batch [897/938], Loss: 0.6058\n",
      "Epoch [2/10], Batch [898/938], Loss: 0.6174\n",
      "Epoch [2/10], Batch [899/938], Loss: 0.6268\n",
      "Epoch [2/10], Batch [900/938], Loss: 0.5958\n",
      "Epoch [2/10], Batch [901/938], Loss: 0.5926\n",
      "Epoch [2/10], Batch [902/938], Loss: 0.5910\n",
      "Epoch [2/10], Batch [903/938], Loss: 0.6443\n",
      "Epoch [2/10], Batch [904/938], Loss: 0.5969\n",
      "Epoch [2/10], Batch [905/938], Loss: 0.5804\n",
      "Epoch [2/10], Batch [906/938], Loss: 0.6422\n",
      "Epoch [2/10], Batch [907/938], Loss: 0.6005\n",
      "Epoch [2/10], Batch [908/938], Loss: 0.6070\n",
      "Epoch [2/10], Batch [909/938], Loss: 0.5902\n",
      "Epoch [2/10], Batch [910/938], Loss: 0.6017\n",
      "Epoch [2/10], Batch [911/938], Loss: 0.6129\n",
      "Epoch [2/10], Batch [912/938], Loss: 0.5857\n",
      "Epoch [2/10], Batch [913/938], Loss: 0.6090\n",
      "Epoch [2/10], Batch [914/938], Loss: 0.6154\n",
      "Epoch [2/10], Batch [915/938], Loss: 0.5916\n",
      "Epoch [2/10], Batch [916/938], Loss: 0.5926\n",
      "Epoch [2/10], Batch [917/938], Loss: 0.6055\n",
      "Epoch [2/10], Batch [918/938], Loss: 0.5974\n",
      "Epoch [2/10], Batch [919/938], Loss: 0.6283\n",
      "Epoch [2/10], Batch [920/938], Loss: 0.6322\n",
      "Epoch [2/10], Batch [921/938], Loss: 0.6031\n",
      "Epoch [2/10], Batch [922/938], Loss: 0.5996\n",
      "Epoch [2/10], Batch [923/938], Loss: 0.5953\n",
      "Epoch [2/10], Batch [924/938], Loss: 0.6178\n",
      "Epoch [2/10], Batch [925/938], Loss: 0.6091\n",
      "Epoch [2/10], Batch [926/938], Loss: 0.6204\n",
      "Epoch [2/10], Batch [927/938], Loss: 0.6231\n",
      "Epoch [2/10], Batch [928/938], Loss: 0.6079\n",
      "Epoch [2/10], Batch [929/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [930/938], Loss: 0.6115\n",
      "Epoch [2/10], Batch [931/938], Loss: 0.6360\n",
      "Epoch [2/10], Batch [932/938], Loss: 0.6277\n",
      "Epoch [2/10], Batch [933/938], Loss: 0.6082\n",
      "Epoch [2/10], Batch [934/938], Loss: 0.6102\n",
      "Epoch [2/10], Batch [935/938], Loss: 0.6012\n",
      "Epoch [2/10], Batch [936/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [937/938], Loss: 0.5874\n",
      "Epoch [2/10], Batch [938/938], Loss: 0.5665\n",
      "Epoch [2/10], Loss: 0.5665\n",
      "Epoch [3/10], Batch [1/938], Loss: 0.6068\n",
      "Epoch [3/10], Batch [2/938], Loss: 0.6152\n",
      "Epoch [3/10], Batch [3/938], Loss: 0.6012\n",
      "Epoch [3/10], Batch [4/938], Loss: 0.6012\n",
      "Epoch [3/10], Batch [5/938], Loss: 0.6401\n",
      "Epoch [3/10], Batch [6/938], Loss: 0.6468\n",
      "Epoch [3/10], Batch [7/938], Loss: 0.6290\n",
      "Epoch [3/10], Batch [8/938], Loss: 0.6012\n",
      "Epoch [3/10], Batch [9/938], Loss: 0.6134\n",
      "Epoch [3/10], Batch [10/938], Loss: 0.5834\n",
      "Epoch [3/10], Batch [11/938], Loss: 0.6029\n",
      "Epoch [3/10], Batch [12/938], Loss: 0.6157\n",
      "Epoch [3/10], Batch [13/938], Loss: 0.6292\n",
      "Epoch [3/10], Batch [14/938], Loss: 0.6245\n",
      "Epoch [3/10], Batch [15/938], Loss: 0.5852\n",
      "Epoch [3/10], Batch [16/938], Loss: 0.6178\n",
      "Epoch [3/10], Batch [17/938], Loss: 0.6082\n",
      "Epoch [3/10], Batch [18/938], Loss: 0.6257\n",
      "Epoch [3/10], Batch [19/938], Loss: 0.5943\n",
      "Epoch [3/10], Batch [20/938], Loss: 0.5895\n",
      "Epoch [3/10], Batch [21/938], Loss: 0.5910\n",
      "Epoch [3/10], Batch [22/938], Loss: 0.6329\n",
      "Epoch [3/10], Batch [23/938], Loss: 0.6304\n",
      "Epoch [3/10], Batch [24/938], Loss: 0.6191\n",
      "Epoch [3/10], Batch [25/938], Loss: 0.6323\n",
      "Epoch [3/10], Batch [26/938], Loss: 0.6230\n",
      "Epoch [3/10], Batch [27/938], Loss: 0.6041\n",
      "Epoch [3/10], Batch [28/938], Loss: 0.6130\n",
      "Epoch [3/10], Batch [29/938], Loss: 0.5943\n",
      "Epoch [3/10], Batch [30/938], Loss: 0.6151\n",
      "Epoch [3/10], Batch [31/938], Loss: 0.6600\n",
      "Epoch [3/10], Batch [32/938], Loss: 0.6045\n",
      "Epoch [3/10], Batch [33/938], Loss: 0.5902\n",
      "Epoch [3/10], Batch [34/938], Loss: 0.6323\n",
      "Epoch [3/10], Batch [35/938], Loss: 0.6170\n",
      "Epoch [3/10], Batch [36/938], Loss: 0.6017\n",
      "Epoch [3/10], Batch [37/938], Loss: 0.6781\n",
      "Epoch [3/10], Batch [38/938], Loss: 0.6202\n",
      "Epoch [3/10], Batch [39/938], Loss: 0.6037\n",
      "Epoch [3/10], Batch [40/938], Loss: 0.5918\n",
      "Epoch [3/10], Batch [41/938], Loss: 0.6135\n",
      "Epoch [3/10], Batch [42/938], Loss: 0.6142\n",
      "Epoch [3/10], Batch [43/938], Loss: 0.6047\n",
      "Epoch [3/10], Batch [44/938], Loss: 0.5821\n",
      "Epoch [3/10], Batch [45/938], Loss: 0.5924\n",
      "Epoch [3/10], Batch [46/938], Loss: 0.6136\n",
      "Epoch [3/10], Batch [47/938], Loss: 0.5891\n",
      "Epoch [3/10], Batch [48/938], Loss: 0.6421\n",
      "Epoch [3/10], Batch [49/938], Loss: 0.5869\n",
      "Epoch [3/10], Batch [50/938], Loss: 0.6119\n",
      "Epoch [3/10], Batch [51/938], Loss: 0.5815\n",
      "Epoch [3/10], Batch [52/938], Loss: 0.6311\n",
      "Epoch [3/10], Batch [53/938], Loss: 0.6304\n",
      "Epoch [3/10], Batch [54/938], Loss: 0.5999\n",
      "Epoch [3/10], Batch [55/938], Loss: 0.5878\n",
      "Epoch [3/10], Batch [56/938], Loss: 0.5813\n",
      "Epoch [3/10], Batch [57/938], Loss: 0.5988\n",
      "Epoch [3/10], Batch [58/938], Loss: 0.5982\n",
      "Epoch [3/10], Batch [59/938], Loss: 0.5835\n",
      "Epoch [3/10], Batch [60/938], Loss: 0.6294\n",
      "Epoch [3/10], Batch [61/938], Loss: 0.5813\n",
      "Epoch [3/10], Batch [62/938], Loss: 0.6163\n",
      "Epoch [3/10], Batch [63/938], Loss: 0.6209\n",
      "Epoch [3/10], Batch [64/938], Loss: 0.5946\n",
      "Epoch [3/10], Batch [65/938], Loss: 0.5824\n",
      "Epoch [3/10], Batch [66/938], Loss: 0.6252\n",
      "Epoch [3/10], Batch [67/938], Loss: 0.5897\n",
      "Epoch [3/10], Batch [68/938], Loss: 0.5910\n",
      "Epoch [3/10], Batch [69/938], Loss: 0.5752\n",
      "Epoch [3/10], Batch [70/938], Loss: 0.5858\n",
      "Epoch [3/10], Batch [71/938], Loss: 0.6133\n",
      "Epoch [3/10], Batch [72/938], Loss: 0.6156\n",
      "Epoch [3/10], Batch [73/938], Loss: 0.5643\n",
      "Epoch [3/10], Batch [74/938], Loss: 0.6169\n",
      "Epoch [3/10], Batch [75/938], Loss: 0.5689\n",
      "Epoch [3/10], Batch [76/938], Loss: 0.5889\n",
      "Epoch [3/10], Batch [77/938], Loss: 0.6158\n",
      "Epoch [3/10], Batch [78/938], Loss: 0.6005\n",
      "Epoch [3/10], Batch [79/938], Loss: 0.6162\n",
      "Epoch [3/10], Batch [80/938], Loss: 0.6033\n",
      "Epoch [3/10], Batch [81/938], Loss: 0.5766\n",
      "Epoch [3/10], Batch [82/938], Loss: 0.6091\n",
      "Epoch [3/10], Batch [83/938], Loss: 0.5848\n",
      "Epoch [3/10], Batch [84/938], Loss: 0.5950\n",
      "Epoch [3/10], Batch [85/938], Loss: 0.5939\n",
      "Epoch [3/10], Batch [86/938], Loss: 0.6040\n",
      "Epoch [3/10], Batch [87/938], Loss: 0.6087\n",
      "Epoch [3/10], Batch [88/938], Loss: 0.6168\n",
      "Epoch [3/10], Batch [89/938], Loss: 0.5812\n",
      "Epoch [3/10], Batch [90/938], Loss: 0.5895\n",
      "Epoch [3/10], Batch [91/938], Loss: 0.6073\n",
      "Epoch [3/10], Batch [92/938], Loss: 0.6228\n",
      "Epoch [3/10], Batch [93/938], Loss: 0.6309\n",
      "Epoch [3/10], Batch [94/938], Loss: 0.6040\n",
      "Epoch [3/10], Batch [95/938], Loss: 0.5922\n",
      "Epoch [3/10], Batch [96/938], Loss: 0.5941\n",
      "Epoch [3/10], Batch [97/938], Loss: 0.5662\n",
      "Epoch [3/10], Batch [98/938], Loss: 0.6098\n",
      "Epoch [3/10], Batch [99/938], Loss: 0.6058\n",
      "Epoch [3/10], Batch [100/938], Loss: 0.5891\n",
      "Epoch [3/10], Batch [101/938], Loss: 0.5812\n",
      "Epoch [3/10], Batch [102/938], Loss: 0.5883\n",
      "Epoch [3/10], Batch [103/938], Loss: 0.6076\n",
      "Epoch [3/10], Batch [104/938], Loss: 0.5912\n",
      "Epoch [3/10], Batch [105/938], Loss: 0.6317\n",
      "Epoch [3/10], Batch [106/938], Loss: 0.5957\n",
      "Epoch [3/10], Batch [107/938], Loss: 0.5914\n",
      "Epoch [3/10], Batch [108/938], Loss: 0.6084\n",
      "Epoch [3/10], Batch [109/938], Loss: 0.6232\n",
      "Epoch [3/10], Batch [110/938], Loss: 0.6110\n",
      "Epoch [3/10], Batch [111/938], Loss: 0.5851\n",
      "Epoch [3/10], Batch [112/938], Loss: 0.6022\n",
      "Epoch [3/10], Batch [113/938], Loss: 0.6012\n",
      "Epoch [3/10], Batch [114/938], Loss: 0.6092\n",
      "Epoch [3/10], Batch [115/938], Loss: 0.6300\n",
      "Epoch [3/10], Batch [116/938], Loss: 0.5846\n",
      "Epoch [3/10], Batch [117/938], Loss: 0.5942\n",
      "Epoch [3/10], Batch [118/938], Loss: 0.6092\n",
      "Epoch [3/10], Batch [119/938], Loss: 0.6264\n",
      "Epoch [3/10], Batch [120/938], Loss: 0.5900\n",
      "Epoch [3/10], Batch [121/938], Loss: 0.5849\n",
      "Epoch [3/10], Batch [122/938], Loss: 0.6247\n",
      "Epoch [3/10], Batch [123/938], Loss: 0.6132\n",
      "Epoch [3/10], Batch [124/938], Loss: 0.6240\n",
      "Epoch [3/10], Batch [125/938], Loss: 0.6119\n",
      "Epoch [3/10], Batch [126/938], Loss: 0.6225\n",
      "Epoch [3/10], Batch [127/938], Loss: 0.6213\n",
      "Epoch [3/10], Batch [128/938], Loss: 0.6406\n",
      "Epoch [3/10], Batch [129/938], Loss: 0.6362\n",
      "Epoch [3/10], Batch [130/938], Loss: 0.6136\n",
      "Epoch [3/10], Batch [131/938], Loss: 0.5922\n",
      "Epoch [3/10], Batch [132/938], Loss: 0.5927\n",
      "Epoch [3/10], Batch [133/938], Loss: 0.5915\n",
      "Epoch [3/10], Batch [134/938], Loss: 0.5906\n",
      "Epoch [3/10], Batch [135/938], Loss: 0.6108\n",
      "Epoch [3/10], Batch [136/938], Loss: 0.6083\n",
      "Epoch [3/10], Batch [137/938], Loss: 0.6176\n",
      "Epoch [3/10], Batch [138/938], Loss: 0.6244\n",
      "Epoch [3/10], Batch [139/938], Loss: 0.5982\n",
      "Epoch [3/10], Batch [140/938], Loss: 0.5803\n",
      "Epoch [3/10], Batch [141/938], Loss: 0.5776\n",
      "Epoch [3/10], Batch [142/938], Loss: 0.5905\n",
      "Epoch [3/10], Batch [143/938], Loss: 0.5643\n",
      "Epoch [3/10], Batch [144/938], Loss: 0.6269\n",
      "Epoch [3/10], Batch [145/938], Loss: 0.5846\n",
      "Epoch [3/10], Batch [146/938], Loss: 0.6008\n",
      "Epoch [3/10], Batch [147/938], Loss: 0.6131\n",
      "Epoch [3/10], Batch [148/938], Loss: 0.5761\n",
      "Epoch [3/10], Batch [149/938], Loss: 0.5851\n",
      "Epoch [3/10], Batch [150/938], Loss: 0.5684\n",
      "Epoch [3/10], Batch [151/938], Loss: 0.6356\n",
      "Epoch [3/10], Batch [152/938], Loss: 0.6033\n",
      "Epoch [3/10], Batch [153/938], Loss: 0.6116\n",
      "Epoch [3/10], Batch [154/938], Loss: 0.6149\n",
      "Epoch [3/10], Batch [155/938], Loss: 0.5943\n",
      "Epoch [3/10], Batch [156/938], Loss: 0.6105\n",
      "Epoch [3/10], Batch [157/938], Loss: 0.5942\n",
      "Epoch [3/10], Batch [158/938], Loss: 0.5839\n",
      "Epoch [3/10], Batch [159/938], Loss: 0.6289\n",
      "Epoch [3/10], Batch [160/938], Loss: 0.6065\n",
      "Epoch [3/10], Batch [161/938], Loss: 0.5917\n",
      "Epoch [3/10], Batch [162/938], Loss: 0.5884\n",
      "Epoch [3/10], Batch [163/938], Loss: 0.6076\n",
      "Epoch [3/10], Batch [164/938], Loss: 0.5967\n",
      "Epoch [3/10], Batch [165/938], Loss: 0.5880\n",
      "Epoch [3/10], Batch [166/938], Loss: 0.6234\n",
      "Epoch [3/10], Batch [167/938], Loss: 0.5749\n",
      "Epoch [3/10], Batch [168/938], Loss: 0.5997\n",
      "Epoch [3/10], Batch [169/938], Loss: 0.5902\n",
      "Epoch [3/10], Batch [170/938], Loss: 0.5836\n",
      "Epoch [3/10], Batch [171/938], Loss: 0.5965\n",
      "Epoch [3/10], Batch [172/938], Loss: 0.5855\n",
      "Epoch [3/10], Batch [173/938], Loss: 0.5970\n",
      "Epoch [3/10], Batch [174/938], Loss: 0.5921\n",
      "Epoch [3/10], Batch [175/938], Loss: 0.6261\n",
      "Epoch [3/10], Batch [176/938], Loss: 0.6194\n",
      "Epoch [3/10], Batch [177/938], Loss: 0.5799\n",
      "Epoch [3/10], Batch [178/938], Loss: 0.6141\n",
      "Epoch [3/10], Batch [179/938], Loss: 0.5737\n",
      "Epoch [3/10], Batch [180/938], Loss: 0.5817\n",
      "Epoch [3/10], Batch [181/938], Loss: 0.5936\n",
      "Epoch [3/10], Batch [182/938], Loss: 0.6033\n",
      "Epoch [3/10], Batch [183/938], Loss: 0.6331\n",
      "Epoch [3/10], Batch [184/938], Loss: 0.5708\n",
      "Epoch [3/10], Batch [185/938], Loss: 0.5871\n",
      "Epoch [3/10], Batch [186/938], Loss: 0.5998\n",
      "Epoch [3/10], Batch [187/938], Loss: 0.6036\n",
      "Epoch [3/10], Batch [188/938], Loss: 0.5807\n",
      "Epoch [3/10], Batch [189/938], Loss: 0.6055\n",
      "Epoch [3/10], Batch [190/938], Loss: 0.6153\n",
      "Epoch [3/10], Batch [191/938], Loss: 0.5904\n",
      "Epoch [3/10], Batch [192/938], Loss: 0.5664\n",
      "Epoch [3/10], Batch [193/938], Loss: 0.5896\n",
      "Epoch [3/10], Batch [194/938], Loss: 0.5843\n",
      "Epoch [3/10], Batch [195/938], Loss: 0.6073\n",
      "Epoch [3/10], Batch [196/938], Loss: 0.5929\n",
      "Epoch [3/10], Batch [197/938], Loss: 0.5962\n",
      "Epoch [3/10], Batch [198/938], Loss: 0.6207\n",
      "Epoch [3/10], Batch [199/938], Loss: 0.5747\n",
      "Epoch [3/10], Batch [200/938], Loss: 0.5771\n",
      "Epoch [3/10], Batch [201/938], Loss: 0.5663\n",
      "Epoch [3/10], Batch [202/938], Loss: 0.6256\n",
      "Epoch [3/10], Batch [203/938], Loss: 0.5975\n",
      "Epoch [3/10], Batch [204/938], Loss: 0.5956\n",
      "Epoch [3/10], Batch [205/938], Loss: 0.5983\n",
      "Epoch [3/10], Batch [206/938], Loss: 0.6039\n",
      "Epoch [3/10], Batch [207/938], Loss: 0.5928\n",
      "Epoch [3/10], Batch [208/938], Loss: 0.5665\n",
      "Epoch [3/10], Batch [209/938], Loss: 0.5733\n",
      "Epoch [3/10], Batch [210/938], Loss: 0.6124\n",
      "Epoch [3/10], Batch [211/938], Loss: 0.5877\n",
      "Epoch [3/10], Batch [212/938], Loss: 0.5946\n",
      "Epoch [3/10], Batch [213/938], Loss: 0.5939\n",
      "Epoch [3/10], Batch [214/938], Loss: 0.6097\n",
      "Epoch [3/10], Batch [215/938], Loss: 0.5841\n",
      "Epoch [3/10], Batch [216/938], Loss: 0.5783\n",
      "Epoch [3/10], Batch [217/938], Loss: 0.5913\n",
      "Epoch [3/10], Batch [218/938], Loss: 0.6180\n",
      "Epoch [3/10], Batch [219/938], Loss: 0.5981\n",
      "Epoch [3/10], Batch [220/938], Loss: 0.6126\n",
      "Epoch [3/10], Batch [221/938], Loss: 0.5796\n",
      "Epoch [3/10], Batch [222/938], Loss: 0.6077\n",
      "Epoch [3/10], Batch [223/938], Loss: 0.6251\n",
      "Epoch [3/10], Batch [224/938], Loss: 0.5923\n",
      "Epoch [3/10], Batch [225/938], Loss: 0.6350\n",
      "Epoch [3/10], Batch [226/938], Loss: 0.5953\n",
      "Epoch [3/10], Batch [227/938], Loss: 0.6209\n",
      "Epoch [3/10], Batch [228/938], Loss: 0.6051\n",
      "Epoch [3/10], Batch [229/938], Loss: 0.5817\n",
      "Epoch [3/10], Batch [230/938], Loss: 0.6363\n",
      "Epoch [3/10], Batch [231/938], Loss: 0.6109\n",
      "Epoch [3/10], Batch [232/938], Loss: 0.5924\n",
      "Epoch [3/10], Batch [233/938], Loss: 0.5859\n",
      "Epoch [3/10], Batch [234/938], Loss: 0.6215\n",
      "Epoch [3/10], Batch [235/938], Loss: 0.6211\n",
      "Epoch [3/10], Batch [236/938], Loss: 0.5972\n",
      "Epoch [3/10], Batch [237/938], Loss: 0.5827\n",
      "Epoch [3/10], Batch [238/938], Loss: 0.5918\n",
      "Epoch [3/10], Batch [239/938], Loss: 0.5911\n",
      "Epoch [3/10], Batch [240/938], Loss: 0.6233\n",
      "Epoch [3/10], Batch [241/938], Loss: 0.6046\n",
      "Epoch [3/10], Batch [242/938], Loss: 0.5784\n",
      "Epoch [3/10], Batch [243/938], Loss: 0.6072\n",
      "Epoch [3/10], Batch [244/938], Loss: 0.6261\n",
      "Epoch [3/10], Batch [245/938], Loss: 0.6428\n",
      "Epoch [3/10], Batch [246/938], Loss: 0.6020\n",
      "Epoch [3/10], Batch [247/938], Loss: 0.6091\n",
      "Epoch [3/10], Batch [248/938], Loss: 0.5843\n",
      "Epoch [3/10], Batch [249/938], Loss: 0.5823\n",
      "Epoch [3/10], Batch [250/938], Loss: 0.6042\n",
      "Epoch [3/10], Batch [251/938], Loss: 0.5911\n",
      "Epoch [3/10], Batch [252/938], Loss: 0.6092\n",
      "Epoch [3/10], Batch [253/938], Loss: 0.6026\n",
      "Epoch [3/10], Batch [254/938], Loss: 0.6026\n",
      "Epoch [3/10], Batch [255/938], Loss: 0.5974\n",
      "Epoch [3/10], Batch [256/938], Loss: 0.5798\n",
      "Epoch [3/10], Batch [257/938], Loss: 0.5936\n",
      "Epoch [3/10], Batch [258/938], Loss: 0.6030\n",
      "Epoch [3/10], Batch [259/938], Loss: 0.5929\n",
      "Epoch [3/10], Batch [260/938], Loss: 0.6065\n",
      "Epoch [3/10], Batch [261/938], Loss: 0.6351\n",
      "Epoch [3/10], Batch [262/938], Loss: 0.6039\n",
      "Epoch [3/10], Batch [263/938], Loss: 0.6111\n",
      "Epoch [3/10], Batch [264/938], Loss: 0.6221\n",
      "Epoch [3/10], Batch [265/938], Loss: 0.5833\n",
      "Epoch [3/10], Batch [266/938], Loss: 0.5758\n",
      "Epoch [3/10], Batch [267/938], Loss: 0.5705\n",
      "Epoch [3/10], Batch [268/938], Loss: 0.5701\n",
      "Epoch [3/10], Batch [269/938], Loss: 0.6252\n",
      "Epoch [3/10], Batch [270/938], Loss: 0.5921\n",
      "Epoch [3/10], Batch [271/938], Loss: 0.5832\n",
      "Epoch [3/10], Batch [272/938], Loss: 0.5722\n",
      "Epoch [3/10], Batch [273/938], Loss: 0.5956\n",
      "Epoch [3/10], Batch [274/938], Loss: 0.5944\n",
      "Epoch [3/10], Batch [275/938], Loss: 0.5910\n",
      "Epoch [3/10], Batch [276/938], Loss: 0.5785\n",
      "Epoch [3/10], Batch [277/938], Loss: 0.5918\n",
      "Epoch [3/10], Batch [278/938], Loss: 0.6080\n",
      "Epoch [3/10], Batch [279/938], Loss: 0.5979\n",
      "Epoch [3/10], Batch [280/938], Loss: 0.5880\n",
      "Epoch [3/10], Batch [281/938], Loss: 0.5980\n",
      "Epoch [3/10], Batch [282/938], Loss: 0.5652\n",
      "Epoch [3/10], Batch [283/938], Loss: 0.5892\n",
      "Epoch [3/10], Batch [284/938], Loss: 0.5896\n",
      "Epoch [3/10], Batch [285/938], Loss: 0.5894\n",
      "Epoch [3/10], Batch [286/938], Loss: 0.6046\n",
      "Epoch [3/10], Batch [287/938], Loss: 0.6093\n",
      "Epoch [3/10], Batch [288/938], Loss: 0.6509\n",
      "Epoch [3/10], Batch [289/938], Loss: 0.6071\n",
      "Epoch [3/10], Batch [290/938], Loss: 0.5826\n",
      "Epoch [3/10], Batch [291/938], Loss: 0.6095\n",
      "Epoch [3/10], Batch [292/938], Loss: 0.6080\n",
      "Epoch [3/10], Batch [293/938], Loss: 0.6162\n",
      "Epoch [3/10], Batch [294/938], Loss: 0.5952\n",
      "Epoch [3/10], Batch [295/938], Loss: 0.6207\n",
      "Epoch [3/10], Batch [296/938], Loss: 0.6163\n",
      "Epoch [3/10], Batch [297/938], Loss: 0.6134\n",
      "Epoch [3/10], Batch [298/938], Loss: 0.6197\n",
      "Epoch [3/10], Batch [299/938], Loss: 0.5597\n",
      "Epoch [3/10], Batch [300/938], Loss: 0.5805\n",
      "Epoch [3/10], Batch [301/938], Loss: 0.5968\n",
      "Epoch [3/10], Batch [302/938], Loss: 0.6328\n",
      "Epoch [3/10], Batch [303/938], Loss: 0.5701\n",
      "Epoch [3/10], Batch [304/938], Loss: 0.6008\n",
      "Epoch [3/10], Batch [305/938], Loss: 0.5681\n",
      "Epoch [3/10], Batch [306/938], Loss: 0.6029\n",
      "Epoch [3/10], Batch [307/938], Loss: 0.5799\n",
      "Epoch [3/10], Batch [308/938], Loss: 0.6079\n",
      "Epoch [3/10], Batch [309/938], Loss: 0.6099\n",
      "Epoch [3/10], Batch [310/938], Loss: 0.6098\n",
      "Epoch [3/10], Batch [311/938], Loss: 0.5767\n",
      "Epoch [3/10], Batch [312/938], Loss: 0.5973\n",
      "Epoch [3/10], Batch [313/938], Loss: 0.5923\n",
      "Epoch [3/10], Batch [314/938], Loss: 0.5565\n",
      "Epoch [3/10], Batch [315/938], Loss: 0.5853\n",
      "Epoch [3/10], Batch [316/938], Loss: 0.6083\n",
      "Epoch [3/10], Batch [317/938], Loss: 0.5947\n",
      "Epoch [3/10], Batch [318/938], Loss: 0.5622\n",
      "Epoch [3/10], Batch [319/938], Loss: 0.6286\n",
      "Epoch [3/10], Batch [320/938], Loss: 0.6167\n",
      "Epoch [3/10], Batch [321/938], Loss: 0.5940\n",
      "Epoch [3/10], Batch [322/938], Loss: 0.5818\n",
      "Epoch [3/10], Batch [323/938], Loss: 0.5982\n",
      "Epoch [3/10], Batch [324/938], Loss: 0.5847\n",
      "Epoch [3/10], Batch [325/938], Loss: 0.5950\n",
      "Epoch [3/10], Batch [326/938], Loss: 0.5884\n",
      "Epoch [3/10], Batch [327/938], Loss: 0.5826\n",
      "Epoch [3/10], Batch [328/938], Loss: 0.5915\n",
      "Epoch [3/10], Batch [329/938], Loss: 0.6007\n",
      "Epoch [3/10], Batch [330/938], Loss: 0.5984\n",
      "Epoch [3/10], Batch [331/938], Loss: 0.6234\n",
      "Epoch [3/10], Batch [332/938], Loss: 0.6068\n",
      "Epoch [3/10], Batch [333/938], Loss: 0.5807\n",
      "Epoch [3/10], Batch [334/938], Loss: 0.5702\n",
      "Epoch [3/10], Batch [335/938], Loss: 0.6113\n",
      "Epoch [3/10], Batch [336/938], Loss: 0.5869\n",
      "Epoch [3/10], Batch [337/938], Loss: 0.5709\n",
      "Epoch [3/10], Batch [338/938], Loss: 0.6081\n",
      "Epoch [3/10], Batch [339/938], Loss: 0.5998\n",
      "Epoch [3/10], Batch [340/938], Loss: 0.5996\n",
      "Epoch [3/10], Batch [341/938], Loss: 0.5946\n",
      "Epoch [3/10], Batch [342/938], Loss: 0.5768\n",
      "Epoch [3/10], Batch [343/938], Loss: 0.5809\n",
      "Epoch [3/10], Batch [344/938], Loss: 0.5952\n",
      "Epoch [3/10], Batch [345/938], Loss: 0.6118\n",
      "Epoch [3/10], Batch [346/938], Loss: 0.6193\n",
      "Epoch [3/10], Batch [347/938], Loss: 0.6136\n",
      "Epoch [3/10], Batch [348/938], Loss: 0.5965\n",
      "Epoch [3/10], Batch [349/938], Loss: 0.6183\n",
      "Epoch [3/10], Batch [350/938], Loss: 0.6145\n",
      "Epoch [3/10], Batch [351/938], Loss: 0.5609\n",
      "Epoch [3/10], Batch [352/938], Loss: 0.5951\n",
      "Epoch [3/10], Batch [353/938], Loss: 0.6136\n",
      "Epoch [3/10], Batch [354/938], Loss: 0.5880\n",
      "Epoch [3/10], Batch [355/938], Loss: 0.5996\n",
      "Epoch [3/10], Batch [356/938], Loss: 0.5934\n",
      "Epoch [3/10], Batch [357/938], Loss: 0.5690\n",
      "Epoch [3/10], Batch [358/938], Loss: 0.5962\n",
      "Epoch [3/10], Batch [359/938], Loss: 0.5846\n",
      "Epoch [3/10], Batch [360/938], Loss: 0.6213\n",
      "Epoch [3/10], Batch [361/938], Loss: 0.5704\n",
      "Epoch [3/10], Batch [362/938], Loss: 0.5981\n",
      "Epoch [3/10], Batch [363/938], Loss: 0.5585\n",
      "Epoch [3/10], Batch [364/938], Loss: 0.6009\n",
      "Epoch [3/10], Batch [365/938], Loss: 0.6276\n",
      "Epoch [3/10], Batch [366/938], Loss: 0.5983\n",
      "Epoch [3/10], Batch [367/938], Loss: 0.5804\n",
      "Epoch [3/10], Batch [368/938], Loss: 0.6155\n",
      "Epoch [3/10], Batch [369/938], Loss: 0.5824\n",
      "Epoch [3/10], Batch [370/938], Loss: 0.5927\n",
      "Epoch [3/10], Batch [371/938], Loss: 0.5914\n",
      "Epoch [3/10], Batch [372/938], Loss: 0.5832\n",
      "Epoch [3/10], Batch [373/938], Loss: 0.6093\n",
      "Epoch [3/10], Batch [374/938], Loss: 0.5754\n",
      "Epoch [3/10], Batch [375/938], Loss: 0.5933\n",
      "Epoch [3/10], Batch [376/938], Loss: 0.5787\n",
      "Epoch [3/10], Batch [377/938], Loss: 0.5874\n",
      "Epoch [3/10], Batch [378/938], Loss: 0.6033\n",
      "Epoch [3/10], Batch [379/938], Loss: 0.5924\n",
      "Epoch [3/10], Batch [380/938], Loss: 0.5795\n",
      "Epoch [3/10], Batch [381/938], Loss: 0.6143\n",
      "Epoch [3/10], Batch [382/938], Loss: 0.5883\n",
      "Epoch [3/10], Batch [383/938], Loss: 0.6121\n",
      "Epoch [3/10], Batch [384/938], Loss: 0.6273\n",
      "Epoch [3/10], Batch [385/938], Loss: 0.6214\n",
      "Epoch [3/10], Batch [386/938], Loss: 0.6096\n",
      "Epoch [3/10], Batch [387/938], Loss: 0.6173\n",
      "Epoch [3/10], Batch [388/938], Loss: 0.5999\n",
      "Epoch [3/10], Batch [389/938], Loss: 0.5633\n",
      "Epoch [3/10], Batch [390/938], Loss: 0.6373\n",
      "Epoch [3/10], Batch [391/938], Loss: 0.5851\n",
      "Epoch [3/10], Batch [392/938], Loss: 0.5778\n",
      "Epoch [3/10], Batch [393/938], Loss: 0.5986\n",
      "Epoch [3/10], Batch [394/938], Loss: 0.5734\n",
      "Epoch [3/10], Batch [395/938], Loss: 0.6135\n",
      "Epoch [3/10], Batch [396/938], Loss: 0.6153\n",
      "Epoch [3/10], Batch [397/938], Loss: 0.6336\n",
      "Epoch [3/10], Batch [398/938], Loss: 0.5907\n",
      "Epoch [3/10], Batch [399/938], Loss: 0.5931\n",
      "Epoch [3/10], Batch [400/938], Loss: 0.5833\n",
      "Epoch [3/10], Batch [401/938], Loss: 0.6104\n",
      "Epoch [3/10], Batch [402/938], Loss: 0.6062\n",
      "Epoch [3/10], Batch [403/938], Loss: 0.5896\n",
      "Epoch [3/10], Batch [404/938], Loss: 0.6197\n",
      "Epoch [3/10], Batch [405/938], Loss: 0.6100\n",
      "Epoch [3/10], Batch [406/938], Loss: 0.5731\n",
      "Epoch [3/10], Batch [407/938], Loss: 0.6007\n",
      "Epoch [3/10], Batch [408/938], Loss: 0.5814\n",
      "Epoch [3/10], Batch [409/938], Loss: 0.5793\n",
      "Epoch [3/10], Batch [410/938], Loss: 0.6250\n",
      "Epoch [3/10], Batch [411/938], Loss: 0.5905\n",
      "Epoch [3/10], Batch [412/938], Loss: 0.5973\n",
      "Epoch [3/10], Batch [413/938], Loss: 0.5852\n",
      "Epoch [3/10], Batch [414/938], Loss: 0.5713\n",
      "Epoch [3/10], Batch [415/938], Loss: 0.5543\n",
      "Epoch [3/10], Batch [416/938], Loss: 0.6152\n",
      "Epoch [3/10], Batch [417/938], Loss: 0.6203\n",
      "Epoch [3/10], Batch [418/938], Loss: 0.5911\n",
      "Epoch [3/10], Batch [419/938], Loss: 0.6092\n",
      "Epoch [3/10], Batch [420/938], Loss: 0.6023\n",
      "Epoch [3/10], Batch [421/938], Loss: 0.5855\n",
      "Epoch [3/10], Batch [422/938], Loss: 0.5855\n",
      "Epoch [3/10], Batch [423/938], Loss: 0.5940\n",
      "Epoch [3/10], Batch [424/938], Loss: 0.5837\n",
      "Epoch [3/10], Batch [425/938], Loss: 0.5855\n",
      "Epoch [3/10], Batch [426/938], Loss: 0.5908\n",
      "Epoch [3/10], Batch [427/938], Loss: 0.5916\n",
      "Epoch [3/10], Batch [428/938], Loss: 0.5854\n",
      "Epoch [3/10], Batch [429/938], Loss: 0.5701\n",
      "Epoch [3/10], Batch [430/938], Loss: 0.6040\n",
      "Epoch [3/10], Batch [431/938], Loss: 0.5836\n",
      "Epoch [3/10], Batch [432/938], Loss: 0.5978\n",
      "Epoch [3/10], Batch [433/938], Loss: 0.6250\n",
      "Epoch [3/10], Batch [434/938], Loss: 0.5908\n",
      "Epoch [3/10], Batch [435/938], Loss: 0.5898\n",
      "Epoch [3/10], Batch [436/938], Loss: 0.6232\n",
      "Epoch [3/10], Batch [437/938], Loss: 0.5851\n",
      "Epoch [3/10], Batch [438/938], Loss: 0.5993\n",
      "Epoch [3/10], Batch [439/938], Loss: 0.5922\n",
      "Epoch [3/10], Batch [440/938], Loss: 0.6193\n",
      "Epoch [3/10], Batch [441/938], Loss: 0.5995\n",
      "Epoch [3/10], Batch [442/938], Loss: 0.5886\n",
      "Epoch [3/10], Batch [443/938], Loss: 0.6102\n",
      "Epoch [3/10], Batch [444/938], Loss: 0.6048\n",
      "Epoch [3/10], Batch [445/938], Loss: 0.5917\n",
      "Epoch [3/10], Batch [446/938], Loss: 0.5875\n",
      "Epoch [3/10], Batch [447/938], Loss: 0.5821\n",
      "Epoch [3/10], Batch [448/938], Loss: 0.5902\n",
      "Epoch [3/10], Batch [449/938], Loss: 0.5869\n",
      "Epoch [3/10], Batch [450/938], Loss: 0.5876\n",
      "Epoch [3/10], Batch [451/938], Loss: 0.6125\n",
      "Epoch [3/10], Batch [452/938], Loss: 0.6009\n",
      "Epoch [3/10], Batch [453/938], Loss: 0.6293\n",
      "Epoch [3/10], Batch [454/938], Loss: 0.6241\n",
      "Epoch [3/10], Batch [455/938], Loss: 0.5915\n",
      "Epoch [3/10], Batch [456/938], Loss: 0.6131\n",
      "Epoch [3/10], Batch [457/938], Loss: 0.5897\n",
      "Epoch [3/10], Batch [458/938], Loss: 0.5947\n",
      "Epoch [3/10], Batch [459/938], Loss: 0.6006\n",
      "Epoch [3/10], Batch [460/938], Loss: 0.5968\n",
      "Epoch [3/10], Batch [461/938], Loss: 0.5886\n",
      "Epoch [3/10], Batch [462/938], Loss: 0.5959\n",
      "Epoch [3/10], Batch [463/938], Loss: 0.5795\n",
      "Epoch [3/10], Batch [464/938], Loss: 0.5882\n",
      "Epoch [3/10], Batch [465/938], Loss: 0.6102\n",
      "Epoch [3/10], Batch [466/938], Loss: 0.6050\n",
      "Epoch [3/10], Batch [467/938], Loss: 0.6267\n",
      "Epoch [3/10], Batch [468/938], Loss: 0.5960\n",
      "Epoch [3/10], Batch [469/938], Loss: 0.5956\n",
      "Epoch [3/10], Batch [470/938], Loss: 0.6128\n",
      "Epoch [3/10], Batch [471/938], Loss: 0.6157\n",
      "Epoch [3/10], Batch [472/938], Loss: 0.6018\n",
      "Epoch [3/10], Batch [473/938], Loss: 0.6148\n",
      "Epoch [3/10], Batch [474/938], Loss: 0.6031\n",
      "Epoch [3/10], Batch [475/938], Loss: 0.5939\n",
      "Epoch [3/10], Batch [476/938], Loss: 0.5990\n",
      "Epoch [3/10], Batch [477/938], Loss: 0.5647\n",
      "Epoch [3/10], Batch [478/938], Loss: 0.6166\n",
      "Epoch [3/10], Batch [479/938], Loss: 0.5680\n",
      "Epoch [3/10], Batch [480/938], Loss: 0.5969\n",
      "Epoch [3/10], Batch [481/938], Loss: 0.6229\n",
      "Epoch [3/10], Batch [482/938], Loss: 0.5945\n",
      "Epoch [3/10], Batch [483/938], Loss: 0.6061\n",
      "Epoch [3/10], Batch [484/938], Loss: 0.5851\n",
      "Epoch [3/10], Batch [485/938], Loss: 0.6206\n",
      "Epoch [3/10], Batch [486/938], Loss: 0.6040\n",
      "Epoch [3/10], Batch [487/938], Loss: 0.6250\n",
      "Epoch [3/10], Batch [488/938], Loss: 0.5867\n",
      "Epoch [3/10], Batch [489/938], Loss: 0.5981\n",
      "Epoch [3/10], Batch [490/938], Loss: 0.5904\n",
      "Epoch [3/10], Batch [491/938], Loss: 0.6126\n",
      "Epoch [3/10], Batch [492/938], Loss: 0.5812\n",
      "Epoch [3/10], Batch [493/938], Loss: 0.6239\n",
      "Epoch [3/10], Batch [494/938], Loss: 0.5960\n",
      "Epoch [3/10], Batch [495/938], Loss: 0.6291\n",
      "Epoch [3/10], Batch [496/938], Loss: 0.5577\n",
      "Epoch [3/10], Batch [497/938], Loss: 0.5763\n",
      "Epoch [3/10], Batch [498/938], Loss: 0.5666\n",
      "Epoch [3/10], Batch [499/938], Loss: 0.6183\n",
      "Epoch [3/10], Batch [500/938], Loss: 0.6436\n",
      "Epoch [3/10], Batch [501/938], Loss: 0.5682\n",
      "Epoch [3/10], Batch [502/938], Loss: 0.5891\n",
      "Epoch [3/10], Batch [503/938], Loss: 0.6032\n",
      "Epoch [3/10], Batch [504/938], Loss: 0.5964\n",
      "Epoch [3/10], Batch [505/938], Loss: 0.5997\n",
      "Epoch [3/10], Batch [506/938], Loss: 0.5691\n",
      "Epoch [3/10], Batch [507/938], Loss: 0.6326\n",
      "Epoch [3/10], Batch [508/938], Loss: 0.5858\n",
      "Epoch [3/10], Batch [509/938], Loss: 0.6035\n",
      "Epoch [3/10], Batch [510/938], Loss: 0.6002\n",
      "Epoch [3/10], Batch [511/938], Loss: 0.5819\n",
      "Epoch [3/10], Batch [512/938], Loss: 0.5897\n",
      "Epoch [3/10], Batch [513/938], Loss: 0.5985\n",
      "Epoch [3/10], Batch [514/938], Loss: 0.6096\n",
      "Epoch [3/10], Batch [515/938], Loss: 0.6108\n",
      "Epoch [3/10], Batch [516/938], Loss: 0.5906\n",
      "Epoch [3/10], Batch [517/938], Loss: 0.5901\n",
      "Epoch [3/10], Batch [518/938], Loss: 0.5980\n",
      "Epoch [3/10], Batch [519/938], Loss: 0.5683\n",
      "Epoch [3/10], Batch [520/938], Loss: 0.6252\n",
      "Epoch [3/10], Batch [521/938], Loss: 0.6052\n",
      "Epoch [3/10], Batch [522/938], Loss: 0.5835\n",
      "Epoch [3/10], Batch [523/938], Loss: 0.6121\n",
      "Epoch [3/10], Batch [524/938], Loss: 0.6009\n",
      "Epoch [3/10], Batch [525/938], Loss: 0.6055\n",
      "Epoch [3/10], Batch [526/938], Loss: 0.5974\n",
      "Epoch [3/10], Batch [527/938], Loss: 0.5980\n",
      "Epoch [3/10], Batch [528/938], Loss: 0.6132\n",
      "Epoch [3/10], Batch [529/938], Loss: 0.6288\n",
      "Epoch [3/10], Batch [530/938], Loss: 0.6211\n",
      "Epoch [3/10], Batch [531/938], Loss: 0.6093\n",
      "Epoch [3/10], Batch [532/938], Loss: 0.5852\n",
      "Epoch [3/10], Batch [533/938], Loss: 0.5972\n",
      "Epoch [3/10], Batch [534/938], Loss: 0.6134\n",
      "Epoch [3/10], Batch [535/938], Loss: 0.6056\n",
      "Epoch [3/10], Batch [536/938], Loss: 0.6137\n",
      "Epoch [3/10], Batch [537/938], Loss: 0.5704\n",
      "Epoch [3/10], Batch [538/938], Loss: 0.6153\n",
      "Epoch [3/10], Batch [539/938], Loss: 0.6170\n",
      "Epoch [3/10], Batch [540/938], Loss: 0.5935\n",
      "Epoch [3/10], Batch [541/938], Loss: 0.5683\n",
      "Epoch [3/10], Batch [542/938], Loss: 0.6127\n",
      "Epoch [3/10], Batch [543/938], Loss: 0.6074\n",
      "Epoch [3/10], Batch [544/938], Loss: 0.5914\n",
      "Epoch [3/10], Batch [545/938], Loss: 0.5890\n",
      "Epoch [3/10], Batch [546/938], Loss: 0.6292\n",
      "Epoch [3/10], Batch [547/938], Loss: 0.5888\n",
      "Epoch [3/10], Batch [548/938], Loss: 0.5951\n",
      "Epoch [3/10], Batch [549/938], Loss: 0.5745\n",
      "Epoch [3/10], Batch [550/938], Loss: 0.5780\n",
      "Epoch [3/10], Batch [551/938], Loss: 0.5963\n",
      "Epoch [3/10], Batch [552/938], Loss: 0.5939\n",
      "Epoch [3/10], Batch [553/938], Loss: 0.6043\n",
      "Epoch [3/10], Batch [554/938], Loss: 0.5529\n",
      "Epoch [3/10], Batch [555/938], Loss: 0.6160\n",
      "Epoch [3/10], Batch [556/938], Loss: 0.6024\n",
      "Epoch [3/10], Batch [557/938], Loss: 0.5874\n",
      "Epoch [3/10], Batch [558/938], Loss: 0.6287\n",
      "Epoch [3/10], Batch [559/938], Loss: 0.5936\n",
      "Epoch [3/10], Batch [560/938], Loss: 0.6362\n",
      "Epoch [3/10], Batch [561/938], Loss: 0.6056\n",
      "Epoch [3/10], Batch [562/938], Loss: 0.6190\n",
      "Epoch [3/10], Batch [563/938], Loss: 0.5865\n",
      "Epoch [3/10], Batch [564/938], Loss: 0.5719\n",
      "Epoch [3/10], Batch [565/938], Loss: 0.6054\n",
      "Epoch [3/10], Batch [566/938], Loss: 0.5760\n",
      "Epoch [3/10], Batch [567/938], Loss: 0.5842\n",
      "Epoch [3/10], Batch [568/938], Loss: 0.5978\n",
      "Epoch [3/10], Batch [569/938], Loss: 0.5939\n",
      "Epoch [3/10], Batch [570/938], Loss: 0.6011\n",
      "Epoch [3/10], Batch [571/938], Loss: 0.6085\n",
      "Epoch [3/10], Batch [572/938], Loss: 0.6159\n",
      "Epoch [3/10], Batch [573/938], Loss: 0.5761\n",
      "Epoch [3/10], Batch [574/938], Loss: 0.5957\n",
      "Epoch [3/10], Batch [575/938], Loss: 0.6135\n",
      "Epoch [3/10], Batch [576/938], Loss: 0.5949\n",
      "Epoch [3/10], Batch [577/938], Loss: 0.5789\n",
      "Epoch [3/10], Batch [578/938], Loss: 0.5794\n",
      "Epoch [3/10], Batch [579/938], Loss: 0.5967\n",
      "Epoch [3/10], Batch [580/938], Loss: 0.6053\n",
      "Epoch [3/10], Batch [581/938], Loss: 0.6155\n",
      "Epoch [3/10], Batch [582/938], Loss: 0.5755\n",
      "Epoch [3/10], Batch [583/938], Loss: 0.5921\n",
      "Epoch [3/10], Batch [584/938], Loss: 0.5814\n",
      "Epoch [3/10], Batch [585/938], Loss: 0.5981\n",
      "Epoch [3/10], Batch [586/938], Loss: 0.5807\n",
      "Epoch [3/10], Batch [587/938], Loss: 0.6248\n",
      "Epoch [3/10], Batch [588/938], Loss: 0.6031\n",
      "Epoch [3/10], Batch [589/938], Loss: 0.5732\n",
      "Epoch [3/10], Batch [590/938], Loss: 0.6040\n",
      "Epoch [3/10], Batch [591/938], Loss: 0.6255\n",
      "Epoch [3/10], Batch [592/938], Loss: 0.6297\n",
      "Epoch [3/10], Batch [593/938], Loss: 0.5969\n",
      "Epoch [3/10], Batch [594/938], Loss: 0.6149\n",
      "Epoch [3/10], Batch [595/938], Loss: 0.5643\n",
      "Epoch [3/10], Batch [596/938], Loss: 0.6044\n",
      "Epoch [3/10], Batch [597/938], Loss: 0.5826\n",
      "Epoch [3/10], Batch [598/938], Loss: 0.5992\n",
      "Epoch [3/10], Batch [599/938], Loss: 0.6026\n",
      "Epoch [3/10], Batch [600/938], Loss: 0.6068\n",
      "Epoch [3/10], Batch [601/938], Loss: 0.5911\n",
      "Epoch [3/10], Batch [602/938], Loss: 0.5937\n",
      "Epoch [3/10], Batch [603/938], Loss: 0.6229\n",
      "Epoch [3/10], Batch [604/938], Loss: 0.5998\n",
      "Epoch [3/10], Batch [605/938], Loss: 0.5906\n",
      "Epoch [3/10], Batch [606/938], Loss: 0.6032\n",
      "Epoch [3/10], Batch [607/938], Loss: 0.5990\n",
      "Epoch [3/10], Batch [608/938], Loss: 0.6025\n",
      "Epoch [3/10], Batch [609/938], Loss: 0.6101\n",
      "Epoch [3/10], Batch [610/938], Loss: 0.6208\n",
      "Epoch [3/10], Batch [611/938], Loss: 0.5936\n",
      "Epoch [3/10], Batch [612/938], Loss: 0.5955\n",
      "Epoch [3/10], Batch [613/938], Loss: 0.5872\n",
      "Epoch [3/10], Batch [614/938], Loss: 0.5768\n",
      "Epoch [3/10], Batch [615/938], Loss: 0.6032\n",
      "Epoch [3/10], Batch [616/938], Loss: 0.6064\n",
      "Epoch [3/10], Batch [617/938], Loss: 0.5940\n",
      "Epoch [3/10], Batch [618/938], Loss: 0.5906\n",
      "Epoch [3/10], Batch [619/938], Loss: 0.5799\n",
      "Epoch [3/10], Batch [620/938], Loss: 0.6035\n",
      "Epoch [3/10], Batch [621/938], Loss: 0.6143\n",
      "Epoch [3/10], Batch [622/938], Loss: 0.6024\n",
      "Epoch [3/10], Batch [623/938], Loss: 0.6222\n",
      "Epoch [3/10], Batch [624/938], Loss: 0.6133\n",
      "Epoch [3/10], Batch [625/938], Loss: 0.5802\n",
      "Epoch [3/10], Batch [626/938], Loss: 0.6060\n",
      "Epoch [3/10], Batch [627/938], Loss: 0.6014\n",
      "Epoch [3/10], Batch [628/938], Loss: 0.5953\n",
      "Epoch [3/10], Batch [629/938], Loss: 0.6073\n",
      "Epoch [3/10], Batch [630/938], Loss: 0.6166\n",
      "Epoch [3/10], Batch [631/938], Loss: 0.5632\n",
      "Epoch [3/10], Batch [632/938], Loss: 0.5931\n",
      "Epoch [3/10], Batch [633/938], Loss: 0.6051\n",
      "Epoch [3/10], Batch [634/938], Loss: 0.5928\n",
      "Epoch [3/10], Batch [635/938], Loss: 0.5716\n",
      "Epoch [3/10], Batch [636/938], Loss: 0.5880\n",
      "Epoch [3/10], Batch [637/938], Loss: 0.5925\n",
      "Epoch [3/10], Batch [638/938], Loss: 0.5908\n",
      "Epoch [3/10], Batch [639/938], Loss: 0.5786\n",
      "Epoch [3/10], Batch [640/938], Loss: 0.5856\n",
      "Epoch [3/10], Batch [641/938], Loss: 0.5939\n",
      "Epoch [3/10], Batch [642/938], Loss: 0.5973\n",
      "Epoch [3/10], Batch [643/938], Loss: 0.6120\n",
      "Epoch [3/10], Batch [644/938], Loss: 0.5818\n",
      "Epoch [3/10], Batch [645/938], Loss: 0.6127\n",
      "Epoch [3/10], Batch [646/938], Loss: 0.6073\n",
      "Epoch [3/10], Batch [647/938], Loss: 0.6126\n",
      "Epoch [3/10], Batch [648/938], Loss: 0.6279\n",
      "Epoch [3/10], Batch [649/938], Loss: 0.5914\n",
      "Epoch [3/10], Batch [650/938], Loss: 0.5820\n",
      "Epoch [3/10], Batch [651/938], Loss: 0.6094\n",
      "Epoch [3/10], Batch [652/938], Loss: 0.5979\n",
      "Epoch [3/10], Batch [653/938], Loss: 0.5925\n",
      "Epoch [3/10], Batch [654/938], Loss: 0.6164\n",
      "Epoch [3/10], Batch [655/938], Loss: 0.5862\n",
      "Epoch [3/10], Batch [656/938], Loss: 0.6255\n",
      "Epoch [3/10], Batch [657/938], Loss: 0.5782\n",
      "Epoch [3/10], Batch [658/938], Loss: 0.5833\n",
      "Epoch [3/10], Batch [659/938], Loss: 0.5708\n",
      "Epoch [3/10], Batch [660/938], Loss: 0.6016\n",
      "Epoch [3/10], Batch [661/938], Loss: 0.5904\n",
      "Epoch [3/10], Batch [662/938], Loss: 0.6529\n",
      "Epoch [3/10], Batch [663/938], Loss: 0.5745\n",
      "Epoch [3/10], Batch [664/938], Loss: 0.5933\n",
      "Epoch [3/10], Batch [665/938], Loss: 0.5838\n",
      "Epoch [3/10], Batch [666/938], Loss: 0.6009\n",
      "Epoch [3/10], Batch [667/938], Loss: 0.6093\n",
      "Epoch [3/10], Batch [668/938], Loss: 0.5929\n",
      "Epoch [3/10], Batch [669/938], Loss: 0.6003\n",
      "Epoch [3/10], Batch [670/938], Loss: 0.5879\n",
      "Epoch [3/10], Batch [671/938], Loss: 0.5712\n",
      "Epoch [3/10], Batch [672/938], Loss: 0.5949\n",
      "Epoch [3/10], Batch [673/938], Loss: 0.5977\n",
      "Epoch [3/10], Batch [674/938], Loss: 0.5944\n",
      "Epoch [3/10], Batch [675/938], Loss: 0.6006\n",
      "Epoch [3/10], Batch [676/938], Loss: 0.5766\n",
      "Epoch [3/10], Batch [677/938], Loss: 0.5775\n",
      "Epoch [3/10], Batch [678/938], Loss: 0.5879\n",
      "Epoch [3/10], Batch [679/938], Loss: 0.6095\n",
      "Epoch [3/10], Batch [680/938], Loss: 0.5970\n",
      "Epoch [3/10], Batch [681/938], Loss: 0.5628\n",
      "Epoch [3/10], Batch [682/938], Loss: 0.5977\n",
      "Epoch [3/10], Batch [683/938], Loss: 0.6157\n",
      "Epoch [3/10], Batch [684/938], Loss: 0.5909\n",
      "Epoch [3/10], Batch [685/938], Loss: 0.5941\n",
      "Epoch [3/10], Batch [686/938], Loss: 0.6066\n",
      "Epoch [3/10], Batch [687/938], Loss: 0.6116\n",
      "Epoch [3/10], Batch [688/938], Loss: 0.6000\n",
      "Epoch [3/10], Batch [689/938], Loss: 0.5760\n",
      "Epoch [3/10], Batch [690/938], Loss: 0.6009\n",
      "Epoch [3/10], Batch [691/938], Loss: 0.5957\n",
      "Epoch [3/10], Batch [692/938], Loss: 0.5941\n",
      "Epoch [3/10], Batch [693/938], Loss: 0.6003\n",
      "Epoch [3/10], Batch [694/938], Loss: 0.6002\n",
      "Epoch [3/10], Batch [695/938], Loss: 0.5949\n",
      "Epoch [3/10], Batch [696/938], Loss: 0.6010\n",
      "Epoch [3/10], Batch [697/938], Loss: 0.5985\n",
      "Epoch [3/10], Batch [698/938], Loss: 0.6040\n",
      "Epoch [3/10], Batch [699/938], Loss: 0.6210\n",
      "Epoch [3/10], Batch [700/938], Loss: 0.5973\n",
      "Epoch [3/10], Batch [701/938], Loss: 0.6060\n",
      "Epoch [3/10], Batch [702/938], Loss: 0.5770\n",
      "Epoch [3/10], Batch [703/938], Loss: 0.5899\n",
      "Epoch [3/10], Batch [704/938], Loss: 0.6142\n",
      "Epoch [3/10], Batch [705/938], Loss: 0.5806\n",
      "Epoch [3/10], Batch [706/938], Loss: 0.5922\n",
      "Epoch [3/10], Batch [707/938], Loss: 0.6095\n",
      "Epoch [3/10], Batch [708/938], Loss: 0.5714\n",
      "Epoch [3/10], Batch [709/938], Loss: 0.5924\n",
      "Epoch [3/10], Batch [710/938], Loss: 0.5858\n",
      "Epoch [3/10], Batch [711/938], Loss: 0.6037\n",
      "Epoch [3/10], Batch [712/938], Loss: 0.5799\n",
      "Epoch [3/10], Batch [713/938], Loss: 0.6148\n",
      "Epoch [3/10], Batch [714/938], Loss: 0.5714\n",
      "Epoch [3/10], Batch [715/938], Loss: 0.5966\n",
      "Epoch [3/10], Batch [716/938], Loss: 0.5910\n",
      "Epoch [3/10], Batch [717/938], Loss: 0.5744\n",
      "Epoch [3/10], Batch [718/938], Loss: 0.5881\n",
      "Epoch [3/10], Batch [719/938], Loss: 0.6157\n",
      "Epoch [3/10], Batch [720/938], Loss: 0.5952\n",
      "Epoch [3/10], Batch [721/938], Loss: 0.6199\n",
      "Epoch [3/10], Batch [722/938], Loss: 0.6187\n",
      "Epoch [3/10], Batch [723/938], Loss: 0.5879\n",
      "Epoch [3/10], Batch [724/938], Loss: 0.5876\n",
      "Epoch [3/10], Batch [725/938], Loss: 0.5593\n",
      "Epoch [3/10], Batch [726/938], Loss: 0.6055\n",
      "Epoch [3/10], Batch [727/938], Loss: 0.5545\n",
      "Epoch [3/10], Batch [728/938], Loss: 0.6121\n",
      "Epoch [3/10], Batch [729/938], Loss: 0.5957\n",
      "Epoch [3/10], Batch [730/938], Loss: 0.6172\n",
      "Epoch [3/10], Batch [731/938], Loss: 0.5707\n",
      "Epoch [3/10], Batch [732/938], Loss: 0.5962\n",
      "Epoch [3/10], Batch [733/938], Loss: 0.5989\n",
      "Epoch [3/10], Batch [734/938], Loss: 0.6091\n",
      "Epoch [3/10], Batch [735/938], Loss: 0.5805\n",
      "Epoch [3/10], Batch [736/938], Loss: 0.6117\n",
      "Epoch [3/10], Batch [737/938], Loss: 0.5762\n",
      "Epoch [3/10], Batch [738/938], Loss: 0.6050\n",
      "Epoch [3/10], Batch [739/938], Loss: 0.5920\n",
      "Epoch [3/10], Batch [740/938], Loss: 0.6103\n",
      "Epoch [3/10], Batch [741/938], Loss: 0.5954\n",
      "Epoch [3/10], Batch [742/938], Loss: 0.5860\n",
      "Epoch [3/10], Batch [743/938], Loss: 0.5761\n",
      "Epoch [3/10], Batch [744/938], Loss: 0.5970\n",
      "Epoch [3/10], Batch [745/938], Loss: 0.5795\n",
      "Epoch [3/10], Batch [746/938], Loss: 0.6312\n",
      "Epoch [3/10], Batch [747/938], Loss: 0.5650\n",
      "Epoch [3/10], Batch [748/938], Loss: 0.5819\n",
      "Epoch [3/10], Batch [749/938], Loss: 0.6078\n",
      "Epoch [3/10], Batch [750/938], Loss: 0.5924\n",
      "Epoch [3/10], Batch [751/938], Loss: 0.6053\n",
      "Epoch [3/10], Batch [752/938], Loss: 0.5578\n",
      "Epoch [3/10], Batch [753/938], Loss: 0.5995\n",
      "Epoch [3/10], Batch [754/938], Loss: 0.5938\n",
      "Epoch [3/10], Batch [755/938], Loss: 0.6168\n",
      "Epoch [3/10], Batch [756/938], Loss: 0.5919\n",
      "Epoch [3/10], Batch [757/938], Loss: 0.6118\n",
      "Epoch [3/10], Batch [758/938], Loss: 0.5931\n",
      "Epoch [3/10], Batch [759/938], Loss: 0.5695\n",
      "Epoch [3/10], Batch [760/938], Loss: 0.5579\n",
      "Epoch [3/10], Batch [761/938], Loss: 0.5886\n",
      "Epoch [3/10], Batch [762/938], Loss: 0.6097\n",
      "Epoch [3/10], Batch [763/938], Loss: 0.6180\n",
      "Epoch [3/10], Batch [764/938], Loss: 0.5721\n",
      "Epoch [3/10], Batch [765/938], Loss: 0.5795\n",
      "Epoch [3/10], Batch [766/938], Loss: 0.5980\n",
      "Epoch [3/10], Batch [767/938], Loss: 0.6216\n",
      "Epoch [3/10], Batch [768/938], Loss: 0.5854\n",
      "Epoch [3/10], Batch [769/938], Loss: 0.5795\n",
      "Epoch [3/10], Batch [770/938], Loss: 0.5654\n",
      "Epoch [3/10], Batch [771/938], Loss: 0.5869\n",
      "Epoch [3/10], Batch [772/938], Loss: 0.6087\n",
      "Epoch [3/10], Batch [773/938], Loss: 0.5883\n",
      "Epoch [3/10], Batch [774/938], Loss: 0.5849\n",
      "Epoch [3/10], Batch [775/938], Loss: 0.6205\n",
      "Epoch [3/10], Batch [776/938], Loss: 0.6126\n",
      "Epoch [3/10], Batch [777/938], Loss: 0.5836\n",
      "Epoch [3/10], Batch [778/938], Loss: 0.5994\n",
      "Epoch [3/10], Batch [779/938], Loss: 0.5810\n",
      "Epoch [3/10], Batch [780/938], Loss: 0.5875\n",
      "Epoch [3/10], Batch [781/938], Loss: 0.5885\n",
      "Epoch [3/10], Batch [782/938], Loss: 0.5878\n",
      "Epoch [3/10], Batch [783/938], Loss: 0.5814\n",
      "Epoch [3/10], Batch [784/938], Loss: 0.5765\n",
      "Epoch [3/10], Batch [785/938], Loss: 0.6346\n",
      "Epoch [3/10], Batch [786/938], Loss: 0.5888\n",
      "Epoch [3/10], Batch [787/938], Loss: 0.6288\n",
      "Epoch [3/10], Batch [788/938], Loss: 0.6044\n",
      "Epoch [3/10], Batch [789/938], Loss: 0.5751\n",
      "Epoch [3/10], Batch [790/938], Loss: 0.6091\n",
      "Epoch [3/10], Batch [791/938], Loss: 0.5928\n",
      "Epoch [3/10], Batch [792/938], Loss: 0.5941\n",
      "Epoch [3/10], Batch [793/938], Loss: 0.5834\n",
      "Epoch [3/10], Batch [794/938], Loss: 0.5735\n",
      "Epoch [3/10], Batch [795/938], Loss: 0.6060\n",
      "Epoch [3/10], Batch [796/938], Loss: 0.6164\n",
      "Epoch [3/10], Batch [797/938], Loss: 0.5777\n",
      "Epoch [3/10], Batch [798/938], Loss: 0.6072\n",
      "Epoch [3/10], Batch [799/938], Loss: 0.6240\n",
      "Epoch [3/10], Batch [800/938], Loss: 0.5978\n",
      "Epoch [3/10], Batch [801/938], Loss: 0.5902\n",
      "Epoch [3/10], Batch [802/938], Loss: 0.5809\n",
      "Epoch [3/10], Batch [803/938], Loss: 0.6009\n",
      "Epoch [3/10], Batch [804/938], Loss: 0.5955\n",
      "Epoch [3/10], Batch [805/938], Loss: 0.6077\n",
      "Epoch [3/10], Batch [806/938], Loss: 0.5890\n",
      "Epoch [3/10], Batch [807/938], Loss: 0.5909\n",
      "Epoch [3/10], Batch [808/938], Loss: 0.5953\n",
      "Epoch [3/10], Batch [809/938], Loss: 0.5924\n",
      "Epoch [3/10], Batch [810/938], Loss: 0.5812\n",
      "Epoch [3/10], Batch [811/938], Loss: 0.6030\n",
      "Epoch [3/10], Batch [812/938], Loss: 0.6194\n",
      "Epoch [3/10], Batch [813/938], Loss: 0.6103\n",
      "Epoch [3/10], Batch [814/938], Loss: 0.6010\n",
      "Epoch [3/10], Batch [815/938], Loss: 0.6183\n",
      "Epoch [3/10], Batch [816/938], Loss: 0.6232\n",
      "Epoch [3/10], Batch [817/938], Loss: 0.5971\n",
      "Epoch [3/10], Batch [818/938], Loss: 0.5792\n",
      "Epoch [3/10], Batch [819/938], Loss: 0.5667\n",
      "Epoch [3/10], Batch [820/938], Loss: 0.6039\n",
      "Epoch [3/10], Batch [821/938], Loss: 0.6296\n",
      "Epoch [3/10], Batch [822/938], Loss: 0.5704\n",
      "Epoch [3/10], Batch [823/938], Loss: 0.5997\n",
      "Epoch [3/10], Batch [824/938], Loss: 0.5985\n",
      "Epoch [3/10], Batch [825/938], Loss: 0.5780\n",
      "Epoch [3/10], Batch [826/938], Loss: 0.5945\n",
      "Epoch [3/10], Batch [827/938], Loss: 0.5932\n",
      "Epoch [3/10], Batch [828/938], Loss: 0.5875\n",
      "Epoch [3/10], Batch [829/938], Loss: 0.5971\n",
      "Epoch [3/10], Batch [830/938], Loss: 0.6128\n",
      "Epoch [3/10], Batch [831/938], Loss: 0.5703\n",
      "Epoch [3/10], Batch [832/938], Loss: 0.5867\n",
      "Epoch [3/10], Batch [833/938], Loss: 0.5817\n",
      "Epoch [3/10], Batch [834/938], Loss: 0.5981\n",
      "Epoch [3/10], Batch [835/938], Loss: 0.5761\n",
      "Epoch [3/10], Batch [836/938], Loss: 0.6122\n",
      "Epoch [3/10], Batch [837/938], Loss: 0.5799\n",
      "Epoch [3/10], Batch [838/938], Loss: 0.6129\n",
      "Epoch [3/10], Batch [839/938], Loss: 0.5780\n",
      "Epoch [3/10], Batch [840/938], Loss: 0.5823\n",
      "Epoch [3/10], Batch [841/938], Loss: 0.6085\n",
      "Epoch [3/10], Batch [842/938], Loss: 0.6182\n",
      "Epoch [3/10], Batch [843/938], Loss: 0.6262\n",
      "Epoch [3/10], Batch [844/938], Loss: 0.6063\n",
      "Epoch [3/10], Batch [845/938], Loss: 0.5904\n",
      "Epoch [3/10], Batch [846/938], Loss: 0.5952\n",
      "Epoch [3/10], Batch [847/938], Loss: 0.5931\n",
      "Epoch [3/10], Batch [848/938], Loss: 0.6041\n",
      "Epoch [3/10], Batch [849/938], Loss: 0.6090\n",
      "Epoch [3/10], Batch [850/938], Loss: 0.6042\n",
      "Epoch [3/10], Batch [851/938], Loss: 0.6044\n",
      "Epoch [3/10], Batch [852/938], Loss: 0.5954\n",
      "Epoch [3/10], Batch [853/938], Loss: 0.5914\n",
      "Epoch [3/10], Batch [854/938], Loss: 0.5742\n",
      "Epoch [3/10], Batch [855/938], Loss: 0.5839\n",
      "Epoch [3/10], Batch [856/938], Loss: 0.5911\n",
      "Epoch [3/10], Batch [857/938], Loss: 0.6264\n",
      "Epoch [3/10], Batch [858/938], Loss: 0.5985\n",
      "Epoch [3/10], Batch [859/938], Loss: 0.6116\n",
      "Epoch [3/10], Batch [860/938], Loss: 0.5790\n",
      "Epoch [3/10], Batch [861/938], Loss: 0.5986\n",
      "Epoch [3/10], Batch [862/938], Loss: 0.6170\n",
      "Epoch [3/10], Batch [863/938], Loss: 0.5878\n",
      "Epoch [3/10], Batch [864/938], Loss: 0.5887\n",
      "Epoch [3/10], Batch [865/938], Loss: 0.5963\n",
      "Epoch [3/10], Batch [866/938], Loss: 0.5995\n",
      "Epoch [3/10], Batch [867/938], Loss: 0.6039\n",
      "Epoch [3/10], Batch [868/938], Loss: 0.6475\n",
      "Epoch [3/10], Batch [869/938], Loss: 0.5878\n",
      "Epoch [3/10], Batch [870/938], Loss: 0.6126\n",
      "Epoch [3/10], Batch [871/938], Loss: 0.5945\n",
      "Epoch [3/10], Batch [872/938], Loss: 0.5680\n",
      "Epoch [3/10], Batch [873/938], Loss: 0.5804\n",
      "Epoch [3/10], Batch [874/938], Loss: 0.5970\n",
      "Epoch [3/10], Batch [875/938], Loss: 0.5933\n",
      "Epoch [3/10], Batch [876/938], Loss: 0.6317\n",
      "Epoch [3/10], Batch [877/938], Loss: 0.5965\n",
      "Epoch [3/10], Batch [878/938], Loss: 0.6195\n",
      "Epoch [3/10], Batch [879/938], Loss: 0.5659\n",
      "Epoch [3/10], Batch [880/938], Loss: 0.6017\n",
      "Epoch [3/10], Batch [881/938], Loss: 0.5818\n",
      "Epoch [3/10], Batch [882/938], Loss: 0.6045\n",
      "Epoch [3/10], Batch [883/938], Loss: 0.6147\n",
      "Epoch [3/10], Batch [884/938], Loss: 0.5865\n",
      "Epoch [3/10], Batch [885/938], Loss: 0.5838\n",
      "Epoch [3/10], Batch [886/938], Loss: 0.5834\n",
      "Epoch [3/10], Batch [887/938], Loss: 0.6158\n",
      "Epoch [3/10], Batch [888/938], Loss: 0.5976\n",
      "Epoch [3/10], Batch [889/938], Loss: 0.5993\n",
      "Epoch [3/10], Batch [890/938], Loss: 0.5896\n",
      "Epoch [3/10], Batch [891/938], Loss: 0.6066\n",
      "Epoch [3/10], Batch [892/938], Loss: 0.6138\n",
      "Epoch [3/10], Batch [893/938], Loss: 0.5873\n",
      "Epoch [3/10], Batch [894/938], Loss: 0.5977\n",
      "Epoch [3/10], Batch [895/938], Loss: 0.5979\n",
      "Epoch [3/10], Batch [896/938], Loss: 0.5911\n",
      "Epoch [3/10], Batch [897/938], Loss: 0.5913\n",
      "Epoch [3/10], Batch [898/938], Loss: 0.6013\n",
      "Epoch [3/10], Batch [899/938], Loss: 0.6042\n",
      "Epoch [3/10], Batch [900/938], Loss: 0.5782\n",
      "Epoch [3/10], Batch [901/938], Loss: 0.5968\n",
      "Epoch [3/10], Batch [902/938], Loss: 0.6196\n",
      "Epoch [3/10], Batch [903/938], Loss: 0.5924\n",
      "Epoch [3/10], Batch [904/938], Loss: 0.6013\n",
      "Epoch [3/10], Batch [905/938], Loss: 0.5975\n",
      "Epoch [3/10], Batch [906/938], Loss: 0.5954\n",
      "Epoch [3/10], Batch [907/938], Loss: 0.5854\n",
      "Epoch [3/10], Batch [908/938], Loss: 0.6000\n",
      "Epoch [3/10], Batch [909/938], Loss: 0.5801\n",
      "Epoch [3/10], Batch [910/938], Loss: 0.5767\n",
      "Epoch [3/10], Batch [911/938], Loss: 0.5712\n",
      "Epoch [3/10], Batch [912/938], Loss: 0.6083\n",
      "Epoch [3/10], Batch [913/938], Loss: 0.5830\n",
      "Epoch [3/10], Batch [914/938], Loss: 0.5860\n",
      "Epoch [3/10], Batch [915/938], Loss: 0.5974\n",
      "Epoch [3/10], Batch [916/938], Loss: 0.5866\n",
      "Epoch [3/10], Batch [917/938], Loss: 0.5530\n",
      "Epoch [3/10], Batch [918/938], Loss: 0.6074\n",
      "Epoch [3/10], Batch [919/938], Loss: 0.5832\n",
      "Epoch [3/10], Batch [920/938], Loss: 0.5893\n",
      "Epoch [3/10], Batch [921/938], Loss: 0.5911\n",
      "Epoch [3/10], Batch [922/938], Loss: 0.5845\n",
      "Epoch [3/10], Batch [923/938], Loss: 0.5975\n",
      "Epoch [3/10], Batch [924/938], Loss: 0.6041\n",
      "Epoch [3/10], Batch [925/938], Loss: 0.6112\n",
      "Epoch [3/10], Batch [926/938], Loss: 0.5979\n",
      "Epoch [3/10], Batch [927/938], Loss: 0.5667\n",
      "Epoch [3/10], Batch [928/938], Loss: 0.5971\n",
      "Epoch [3/10], Batch [929/938], Loss: 0.6083\n",
      "Epoch [3/10], Batch [930/938], Loss: 0.5915\n",
      "Epoch [3/10], Batch [931/938], Loss: 0.5593\n",
      "Epoch [3/10], Batch [932/938], Loss: 0.6236\n",
      "Epoch [3/10], Batch [933/938], Loss: 0.6027\n",
      "Epoch [3/10], Batch [934/938], Loss: 0.6163\n",
      "Epoch [3/10], Batch [935/938], Loss: 0.5785\n",
      "Epoch [3/10], Batch [936/938], Loss: 0.6092\n",
      "Epoch [3/10], Batch [937/938], Loss: 0.5805\n",
      "Epoch [3/10], Batch [938/938], Loss: 0.5454\n",
      "Epoch [3/10], Loss: 0.5454\n",
      "Epoch [4/10], Batch [1/938], Loss: 0.5890\n",
      "Epoch [4/10], Batch [2/938], Loss: 0.5809\n",
      "Epoch [4/10], Batch [3/938], Loss: 0.6191\n",
      "Epoch [4/10], Batch [4/938], Loss: 0.5681\n",
      "Epoch [4/10], Batch [5/938], Loss: 0.5955\n",
      "Epoch [4/10], Batch [6/938], Loss: 0.5939\n",
      "Epoch [4/10], Batch [7/938], Loss: 0.5932\n",
      "Epoch [4/10], Batch [8/938], Loss: 0.5932\n",
      "Epoch [4/10], Batch [9/938], Loss: 0.5899\n",
      "Epoch [4/10], Batch [10/938], Loss: 0.6135\n",
      "Epoch [4/10], Batch [11/938], Loss: 0.6107\n",
      "Epoch [4/10], Batch [12/938], Loss: 0.5932\n",
      "Epoch [4/10], Batch [13/938], Loss: 0.6027\n",
      "Epoch [4/10], Batch [14/938], Loss: 0.5808\n",
      "Epoch [4/10], Batch [15/938], Loss: 0.5786\n",
      "Epoch [4/10], Batch [16/938], Loss: 0.5759\n",
      "Epoch [4/10], Batch [17/938], Loss: 0.5893\n",
      "Epoch [4/10], Batch [18/938], Loss: 0.5904\n",
      "Epoch [4/10], Batch [19/938], Loss: 0.6238\n",
      "Epoch [4/10], Batch [20/938], Loss: 0.5879\n",
      "Epoch [4/10], Batch [21/938], Loss: 0.6280\n",
      "Epoch [4/10], Batch [22/938], Loss: 0.5888\n",
      "Epoch [4/10], Batch [23/938], Loss: 0.6301\n",
      "Epoch [4/10], Batch [24/938], Loss: 0.5873\n",
      "Epoch [4/10], Batch [25/938], Loss: 0.5765\n",
      "Epoch [4/10], Batch [26/938], Loss: 0.5824\n",
      "Epoch [4/10], Batch [27/938], Loss: 0.6155\n",
      "Epoch [4/10], Batch [28/938], Loss: 0.5923\n",
      "Epoch [4/10], Batch [29/938], Loss: 0.6058\n",
      "Epoch [4/10], Batch [30/938], Loss: 0.5692\n",
      "Epoch [4/10], Batch [31/938], Loss: 0.6053\n",
      "Epoch [4/10], Batch [32/938], Loss: 0.6123\n",
      "Epoch [4/10], Batch [33/938], Loss: 0.5792\n",
      "Epoch [4/10], Batch [34/938], Loss: 0.6249\n",
      "Epoch [4/10], Batch [35/938], Loss: 0.6087\n",
      "Epoch [4/10], Batch [36/938], Loss: 0.5979\n",
      "Epoch [4/10], Batch [37/938], Loss: 0.5789\n",
      "Epoch [4/10], Batch [38/938], Loss: 0.5617\n",
      "Epoch [4/10], Batch [39/938], Loss: 0.5855\n",
      "Epoch [4/10], Batch [40/938], Loss: 0.5742\n",
      "Epoch [4/10], Batch [41/938], Loss: 0.5993\n",
      "Epoch [4/10], Batch [42/938], Loss: 0.6182\n",
      "Epoch [4/10], Batch [43/938], Loss: 0.5849\n",
      "Epoch [4/10], Batch [44/938], Loss: 0.6077\n",
      "Epoch [4/10], Batch [45/938], Loss: 0.6089\n",
      "Epoch [4/10], Batch [46/938], Loss: 0.5837\n",
      "Epoch [4/10], Batch [47/938], Loss: 0.6069\n",
      "Epoch [4/10], Batch [48/938], Loss: 0.6187\n",
      "Epoch [4/10], Batch [49/938], Loss: 0.5861\n",
      "Epoch [4/10], Batch [50/938], Loss: 0.6150\n",
      "Epoch [4/10], Batch [51/938], Loss: 0.6162\n",
      "Epoch [4/10], Batch [52/938], Loss: 0.6206\n",
      "Epoch [4/10], Batch [53/938], Loss: 0.5935\n",
      "Epoch [4/10], Batch [54/938], Loss: 0.5743\n",
      "Epoch [4/10], Batch [55/938], Loss: 0.6220\n",
      "Epoch [4/10], Batch [56/938], Loss: 0.6270\n",
      "Epoch [4/10], Batch [57/938], Loss: 0.6158\n",
      "Epoch [4/10], Batch [58/938], Loss: 0.5774\n",
      "Epoch [4/10], Batch [59/938], Loss: 0.5963\n",
      "Epoch [4/10], Batch [60/938], Loss: 0.6035\n",
      "Epoch [4/10], Batch [61/938], Loss: 0.6075\n",
      "Epoch [4/10], Batch [62/938], Loss: 0.6103\n",
      "Epoch [4/10], Batch [63/938], Loss: 0.5929\n",
      "Epoch [4/10], Batch [64/938], Loss: 0.5960\n",
      "Epoch [4/10], Batch [65/938], Loss: 0.6336\n",
      "Epoch [4/10], Batch [66/938], Loss: 0.5965\n",
      "Epoch [4/10], Batch [67/938], Loss: 0.5942\n",
      "Epoch [4/10], Batch [68/938], Loss: 0.5774\n",
      "Epoch [4/10], Batch [69/938], Loss: 0.5894\n",
      "Epoch [4/10], Batch [70/938], Loss: 0.6067\n",
      "Epoch [4/10], Batch [71/938], Loss: 0.5927\n",
      "Epoch [4/10], Batch [72/938], Loss: 0.6080\n",
      "Epoch [4/10], Batch [73/938], Loss: 0.5722\n",
      "Epoch [4/10], Batch [74/938], Loss: 0.5941\n",
      "Epoch [4/10], Batch [75/938], Loss: 0.6013\n",
      "Epoch [4/10], Batch [76/938], Loss: 0.5794\n",
      "Epoch [4/10], Batch [77/938], Loss: 0.6085\n",
      "Epoch [4/10], Batch [78/938], Loss: 0.6174\n",
      "Epoch [4/10], Batch [79/938], Loss: 0.5596\n",
      "Epoch [4/10], Batch [80/938], Loss: 0.6079\n",
      "Epoch [4/10], Batch [81/938], Loss: 0.5711\n",
      "Epoch [4/10], Batch [82/938], Loss: 0.6021\n",
      "Epoch [4/10], Batch [83/938], Loss: 0.6031\n",
      "Epoch [4/10], Batch [84/938], Loss: 0.6246\n",
      "Epoch [4/10], Batch [85/938], Loss: 0.6007\n",
      "Epoch [4/10], Batch [86/938], Loss: 0.6053\n",
      "Epoch [4/10], Batch [87/938], Loss: 0.5726\n",
      "Epoch [4/10], Batch [88/938], Loss: 0.5507\n",
      "Epoch [4/10], Batch [89/938], Loss: 0.6071\n",
      "Epoch [4/10], Batch [90/938], Loss: 0.6331\n",
      "Epoch [4/10], Batch [91/938], Loss: 0.5681\n",
      "Epoch [4/10], Batch [92/938], Loss: 0.5695\n",
      "Epoch [4/10], Batch [93/938], Loss: 0.5879\n",
      "Epoch [4/10], Batch [94/938], Loss: 0.5563\n",
      "Epoch [4/10], Batch [95/938], Loss: 0.5852\n",
      "Epoch [4/10], Batch [96/938], Loss: 0.6326\n",
      "Epoch [4/10], Batch [97/938], Loss: 0.6231\n",
      "Epoch [4/10], Batch [98/938], Loss: 0.5908\n",
      "Epoch [4/10], Batch [99/938], Loss: 0.6389\n",
      "Epoch [4/10], Batch [100/938], Loss: 0.5839\n",
      "Epoch [4/10], Batch [101/938], Loss: 0.6057\n",
      "Epoch [4/10], Batch [102/938], Loss: 0.6018\n",
      "Epoch [4/10], Batch [103/938], Loss: 0.5685\n",
      "Epoch [4/10], Batch [104/938], Loss: 0.5817\n",
      "Epoch [4/10], Batch [105/938], Loss: 0.6017\n",
      "Epoch [4/10], Batch [106/938], Loss: 0.5914\n",
      "Epoch [4/10], Batch [107/938], Loss: 0.6107\n",
      "Epoch [4/10], Batch [108/938], Loss: 0.5710\n",
      "Epoch [4/10], Batch [109/938], Loss: 0.6021\n",
      "Epoch [4/10], Batch [110/938], Loss: 0.6083\n",
      "Epoch [4/10], Batch [111/938], Loss: 0.5947\n",
      "Epoch [4/10], Batch [112/938], Loss: 0.6138\n",
      "Epoch [4/10], Batch [113/938], Loss: 0.5985\n",
      "Epoch [4/10], Batch [114/938], Loss: 0.5911\n",
      "Epoch [4/10], Batch [115/938], Loss: 0.6151\n",
      "Epoch [4/10], Batch [116/938], Loss: 0.5742\n",
      "Epoch [4/10], Batch [117/938], Loss: 0.5829\n",
      "Epoch [4/10], Batch [118/938], Loss: 0.5728\n",
      "Epoch [4/10], Batch [119/938], Loss: 0.5930\n",
      "Epoch [4/10], Batch [120/938], Loss: 0.6006\n",
      "Epoch [4/10], Batch [121/938], Loss: 0.5892\n",
      "Epoch [4/10], Batch [122/938], Loss: 0.6008\n",
      "Epoch [4/10], Batch [123/938], Loss: 0.5813\n",
      "Epoch [4/10], Batch [124/938], Loss: 0.5797\n",
      "Epoch [4/10], Batch [125/938], Loss: 0.5878\n",
      "Epoch [4/10], Batch [126/938], Loss: 0.5795\n",
      "Epoch [4/10], Batch [127/938], Loss: 0.6169\n",
      "Epoch [4/10], Batch [128/938], Loss: 0.5595\n",
      "Epoch [4/10], Batch [129/938], Loss: 0.6034\n",
      "Epoch [4/10], Batch [130/938], Loss: 0.5763\n",
      "Epoch [4/10], Batch [131/938], Loss: 0.5942\n",
      "Epoch [4/10], Batch [132/938], Loss: 0.6270\n",
      "Epoch [4/10], Batch [133/938], Loss: 0.5858\n",
      "Epoch [4/10], Batch [134/938], Loss: 0.5765\n",
      "Epoch [4/10], Batch [135/938], Loss: 0.5737\n",
      "Epoch [4/10], Batch [136/938], Loss: 0.5894\n",
      "Epoch [4/10], Batch [137/938], Loss: 0.5606\n",
      "Epoch [4/10], Batch [138/938], Loss: 0.5812\n",
      "Epoch [4/10], Batch [139/938], Loss: 0.5674\n",
      "Epoch [4/10], Batch [140/938], Loss: 0.6075\n",
      "Epoch [4/10], Batch [141/938], Loss: 0.6229\n",
      "Epoch [4/10], Batch [142/938], Loss: 0.6132\n",
      "Epoch [4/10], Batch [143/938], Loss: 0.5731\n",
      "Epoch [4/10], Batch [144/938], Loss: 0.5837\n",
      "Epoch [4/10], Batch [145/938], Loss: 0.5889\n",
      "Epoch [4/10], Batch [146/938], Loss: 0.5958\n",
      "Epoch [4/10], Batch [147/938], Loss: 0.5978\n",
      "Epoch [4/10], Batch [148/938], Loss: 0.6192\n",
      "Epoch [4/10], Batch [149/938], Loss: 0.5981\n",
      "Epoch [4/10], Batch [150/938], Loss: 0.6045\n",
      "Epoch [4/10], Batch [151/938], Loss: 0.5996\n",
      "Epoch [4/10], Batch [152/938], Loss: 0.6013\n",
      "Epoch [4/10], Batch [153/938], Loss: 0.6072\n",
      "Epoch [4/10], Batch [154/938], Loss: 0.5696\n",
      "Epoch [4/10], Batch [155/938], Loss: 0.5818\n",
      "Epoch [4/10], Batch [156/938], Loss: 0.5916\n",
      "Epoch [4/10], Batch [157/938], Loss: 0.5755\n",
      "Epoch [4/10], Batch [158/938], Loss: 0.5839\n",
      "Epoch [4/10], Batch [159/938], Loss: 0.6018\n",
      "Epoch [4/10], Batch [160/938], Loss: 0.5968\n",
      "Epoch [4/10], Batch [161/938], Loss: 0.6319\n",
      "Epoch [4/10], Batch [162/938], Loss: 0.5785\n",
      "Epoch [4/10], Batch [163/938], Loss: 0.5883\n",
      "Epoch [4/10], Batch [164/938], Loss: 0.5707\n",
      "Epoch [4/10], Batch [165/938], Loss: 0.5902\n",
      "Epoch [4/10], Batch [166/938], Loss: 0.5975\n",
      "Epoch [4/10], Batch [167/938], Loss: 0.6015\n",
      "Epoch [4/10], Batch [168/938], Loss: 0.5749\n",
      "Epoch [4/10], Batch [169/938], Loss: 0.6331\n",
      "Epoch [4/10], Batch [170/938], Loss: 0.5834\n",
      "Epoch [4/10], Batch [171/938], Loss: 0.5917\n",
      "Epoch [4/10], Batch [172/938], Loss: 0.6330\n",
      "Epoch [4/10], Batch [173/938], Loss: 0.5930\n",
      "Epoch [4/10], Batch [174/938], Loss: 0.6057\n",
      "Epoch [4/10], Batch [175/938], Loss: 0.5765\n",
      "Epoch [4/10], Batch [176/938], Loss: 0.5929\n",
      "Epoch [4/10], Batch [177/938], Loss: 0.5731\n",
      "Epoch [4/10], Batch [178/938], Loss: 0.5792\n",
      "Epoch [4/10], Batch [179/938], Loss: 0.5936\n",
      "Epoch [4/10], Batch [180/938], Loss: 0.5789\n",
      "Epoch [4/10], Batch [181/938], Loss: 0.6029\n",
      "Epoch [4/10], Batch [182/938], Loss: 0.5882\n",
      "Epoch [4/10], Batch [183/938], Loss: 0.6074\n",
      "Epoch [4/10], Batch [184/938], Loss: 0.6203\n",
      "Epoch [4/10], Batch [185/938], Loss: 0.5977\n",
      "Epoch [4/10], Batch [186/938], Loss: 0.6232\n",
      "Epoch [4/10], Batch [187/938], Loss: 0.5724\n",
      "Epoch [4/10], Batch [188/938], Loss: 0.5839\n",
      "Epoch [4/10], Batch [189/938], Loss: 0.5708\n",
      "Epoch [4/10], Batch [190/938], Loss: 0.5822\n",
      "Epoch [4/10], Batch [191/938], Loss: 0.5621\n",
      "Epoch [4/10], Batch [192/938], Loss: 0.6196\n",
      "Epoch [4/10], Batch [193/938], Loss: 0.5770\n",
      "Epoch [4/10], Batch [194/938], Loss: 0.5653\n",
      "Epoch [4/10], Batch [195/938], Loss: 0.5996\n",
      "Epoch [4/10], Batch [196/938], Loss: 0.5963\n",
      "Epoch [4/10], Batch [197/938], Loss: 0.5680\n",
      "Epoch [4/10], Batch [198/938], Loss: 0.6016\n",
      "Epoch [4/10], Batch [199/938], Loss: 0.5930\n",
      "Epoch [4/10], Batch [200/938], Loss: 0.5794\n",
      "Epoch [4/10], Batch [201/938], Loss: 0.6114\n",
      "Epoch [4/10], Batch [202/938], Loss: 0.6173\n",
      "Epoch [4/10], Batch [203/938], Loss: 0.5956\n",
      "Epoch [4/10], Batch [204/938], Loss: 0.5711\n",
      "Epoch [4/10], Batch [205/938], Loss: 0.5863\n",
      "Epoch [4/10], Batch [206/938], Loss: 0.6063\n",
      "Epoch [4/10], Batch [207/938], Loss: 0.6480\n",
      "Epoch [4/10], Batch [208/938], Loss: 0.6111\n",
      "Epoch [4/10], Batch [209/938], Loss: 0.5755\n",
      "Epoch [4/10], Batch [210/938], Loss: 0.5835\n",
      "Epoch [4/10], Batch [211/938], Loss: 0.6016\n",
      "Epoch [4/10], Batch [212/938], Loss: 0.6257\n",
      "Epoch [4/10], Batch [213/938], Loss: 0.6090\n",
      "Epoch [4/10], Batch [214/938], Loss: 0.5908\n",
      "Epoch [4/10], Batch [215/938], Loss: 0.5937\n",
      "Epoch [4/10], Batch [216/938], Loss: 0.5591\n",
      "Epoch [4/10], Batch [217/938], Loss: 0.5902\n",
      "Epoch [4/10], Batch [218/938], Loss: 0.5862\n",
      "Epoch [4/10], Batch [219/938], Loss: 0.6135\n",
      "Epoch [4/10], Batch [220/938], Loss: 0.6133\n",
      "Epoch [4/10], Batch [221/938], Loss: 0.6139\n",
      "Epoch [4/10], Batch [222/938], Loss: 0.5727\n",
      "Epoch [4/10], Batch [223/938], Loss: 0.6145\n",
      "Epoch [4/10], Batch [224/938], Loss: 0.5757\n",
      "Epoch [4/10], Batch [225/938], Loss: 0.5847\n",
      "Epoch [4/10], Batch [226/938], Loss: 0.6035\n",
      "Epoch [4/10], Batch [227/938], Loss: 0.5707\n",
      "Epoch [4/10], Batch [228/938], Loss: 0.5982\n",
      "Epoch [4/10], Batch [229/938], Loss: 0.5865\n",
      "Epoch [4/10], Batch [230/938], Loss: 0.5656\n",
      "Epoch [4/10], Batch [231/938], Loss: 0.6192\n",
      "Epoch [4/10], Batch [232/938], Loss: 0.6013\n",
      "Epoch [4/10], Batch [233/938], Loss: 0.6199\n",
      "Epoch [4/10], Batch [234/938], Loss: 0.6018\n",
      "Epoch [4/10], Batch [235/938], Loss: 0.5676\n",
      "Epoch [4/10], Batch [236/938], Loss: 0.5996\n",
      "Epoch [4/10], Batch [237/938], Loss: 0.6105\n",
      "Epoch [4/10], Batch [238/938], Loss: 0.6187\n",
      "Epoch [4/10], Batch [239/938], Loss: 0.5785\n",
      "Epoch [4/10], Batch [240/938], Loss: 0.6095\n",
      "Epoch [4/10], Batch [241/938], Loss: 0.5872\n",
      "Epoch [4/10], Batch [242/938], Loss: 0.6201\n",
      "Epoch [4/10], Batch [243/938], Loss: 0.5927\n",
      "Epoch [4/10], Batch [244/938], Loss: 0.6163\n",
      "Epoch [4/10], Batch [245/938], Loss: 0.6116\n",
      "Epoch [4/10], Batch [246/938], Loss: 0.5864\n",
      "Epoch [4/10], Batch [247/938], Loss: 0.5829\n",
      "Epoch [4/10], Batch [248/938], Loss: 0.5901\n",
      "Epoch [4/10], Batch [249/938], Loss: 0.6096\n",
      "Epoch [4/10], Batch [250/938], Loss: 0.6077\n",
      "Epoch [4/10], Batch [251/938], Loss: 0.5874\n",
      "Epoch [4/10], Batch [252/938], Loss: 0.5812\n",
      "Epoch [4/10], Batch [253/938], Loss: 0.5975\n",
      "Epoch [4/10], Batch [254/938], Loss: 0.5847\n",
      "Epoch [4/10], Batch [255/938], Loss: 0.5716\n",
      "Epoch [4/10], Batch [256/938], Loss: 0.6009\n",
      "Epoch [4/10], Batch [257/938], Loss: 0.5791\n",
      "Epoch [4/10], Batch [258/938], Loss: 0.5993\n",
      "Epoch [4/10], Batch [259/938], Loss: 0.5930\n",
      "Epoch [4/10], Batch [260/938], Loss: 0.5999\n",
      "Epoch [4/10], Batch [261/938], Loss: 0.5715\n",
      "Epoch [4/10], Batch [262/938], Loss: 0.6005\n",
      "Epoch [4/10], Batch [263/938], Loss: 0.5762\n",
      "Epoch [4/10], Batch [264/938], Loss: 0.6075\n",
      "Epoch [4/10], Batch [265/938], Loss: 0.5888\n",
      "Epoch [4/10], Batch [266/938], Loss: 0.5882\n",
      "Epoch [4/10], Batch [267/938], Loss: 0.5941\n",
      "Epoch [4/10], Batch [268/938], Loss: 0.5831\n",
      "Epoch [4/10], Batch [269/938], Loss: 0.5841\n",
      "Epoch [4/10], Batch [270/938], Loss: 0.5927\n",
      "Epoch [4/10], Batch [271/938], Loss: 0.5705\n",
      "Epoch [4/10], Batch [272/938], Loss: 0.6182\n",
      "Epoch [4/10], Batch [273/938], Loss: 0.5984\n",
      "Epoch [4/10], Batch [274/938], Loss: 0.6048\n",
      "Epoch [4/10], Batch [275/938], Loss: 0.6110\n",
      "Epoch [4/10], Batch [276/938], Loss: 0.5898\n",
      "Epoch [4/10], Batch [277/938], Loss: 0.5885\n",
      "Epoch [4/10], Batch [278/938], Loss: 0.5812\n",
      "Epoch [4/10], Batch [279/938], Loss: 0.5838\n",
      "Epoch [4/10], Batch [280/938], Loss: 0.6382\n",
      "Epoch [4/10], Batch [281/938], Loss: 0.6068\n",
      "Epoch [4/10], Batch [282/938], Loss: 0.5675\n",
      "Epoch [4/10], Batch [283/938], Loss: 0.5782\n",
      "Epoch [4/10], Batch [284/938], Loss: 0.6009\n",
      "Epoch [4/10], Batch [285/938], Loss: 0.6156\n",
      "Epoch [4/10], Batch [286/938], Loss: 0.5863\n",
      "Epoch [4/10], Batch [287/938], Loss: 0.5931\n",
      "Epoch [4/10], Batch [288/938], Loss: 0.5683\n",
      "Epoch [4/10], Batch [289/938], Loss: 0.5663\n",
      "Epoch [4/10], Batch [290/938], Loss: 0.5920\n",
      "Epoch [4/10], Batch [291/938], Loss: 0.6202\n",
      "Epoch [4/10], Batch [292/938], Loss: 0.5695\n",
      "Epoch [4/10], Batch [293/938], Loss: 0.5827\n",
      "Epoch [4/10], Batch [294/938], Loss: 0.6183\n",
      "Epoch [4/10], Batch [295/938], Loss: 0.5723\n",
      "Epoch [4/10], Batch [296/938], Loss: 0.6038\n",
      "Epoch [4/10], Batch [297/938], Loss: 0.6398\n",
      "Epoch [4/10], Batch [298/938], Loss: 0.5954\n",
      "Epoch [4/10], Batch [299/938], Loss: 0.5898\n",
      "Epoch [4/10], Batch [300/938], Loss: 0.5834\n",
      "Epoch [4/10], Batch [301/938], Loss: 0.6031\n",
      "Epoch [4/10], Batch [302/938], Loss: 0.6198\n",
      "Epoch [4/10], Batch [303/938], Loss: 0.6039\n",
      "Epoch [4/10], Batch [304/938], Loss: 0.5933\n",
      "Epoch [4/10], Batch [305/938], Loss: 0.5937\n",
      "Epoch [4/10], Batch [306/938], Loss: 0.5672\n",
      "Epoch [4/10], Batch [307/938], Loss: 0.6237\n",
      "Epoch [4/10], Batch [308/938], Loss: 0.6002\n",
      "Epoch [4/10], Batch [309/938], Loss: 0.6123\n",
      "Epoch [4/10], Batch [310/938], Loss: 0.5844\n",
      "Epoch [4/10], Batch [311/938], Loss: 0.5995\n",
      "Epoch [4/10], Batch [312/938], Loss: 0.5853\n",
      "Epoch [4/10], Batch [313/938], Loss: 0.5807\n",
      "Epoch [4/10], Batch [314/938], Loss: 0.6137\n",
      "Epoch [4/10], Batch [315/938], Loss: 0.5888\n",
      "Epoch [4/10], Batch [316/938], Loss: 0.6288\n",
      "Epoch [4/10], Batch [317/938], Loss: 0.6000\n",
      "Epoch [4/10], Batch [318/938], Loss: 0.5471\n",
      "Epoch [4/10], Batch [319/938], Loss: 0.6056\n",
      "Epoch [4/10], Batch [320/938], Loss: 0.6235\n",
      "Epoch [4/10], Batch [321/938], Loss: 0.6144\n",
      "Epoch [4/10], Batch [322/938], Loss: 0.6004\n",
      "Epoch [4/10], Batch [323/938], Loss: 0.6019\n",
      "Epoch [4/10], Batch [324/938], Loss: 0.6115\n",
      "Epoch [4/10], Batch [325/938], Loss: 0.6025\n",
      "Epoch [4/10], Batch [326/938], Loss: 0.5911\n",
      "Epoch [4/10], Batch [327/938], Loss: 0.5890\n",
      "Epoch [4/10], Batch [328/938], Loss: 0.5798\n",
      "Epoch [4/10], Batch [329/938], Loss: 0.5908\n",
      "Epoch [4/10], Batch [330/938], Loss: 0.5763\n",
      "Epoch [4/10], Batch [331/938], Loss: 0.5854\n",
      "Epoch [4/10], Batch [332/938], Loss: 0.5860\n",
      "Epoch [4/10], Batch [333/938], Loss: 0.6285\n",
      "Epoch [4/10], Batch [334/938], Loss: 0.5923\n",
      "Epoch [4/10], Batch [335/938], Loss: 0.5819\n",
      "Epoch [4/10], Batch [336/938], Loss: 0.5816\n",
      "Epoch [4/10], Batch [337/938], Loss: 0.5966\n",
      "Epoch [4/10], Batch [338/938], Loss: 0.6006\n",
      "Epoch [4/10], Batch [339/938], Loss: 0.5811\n",
      "Epoch [4/10], Batch [340/938], Loss: 0.6252\n",
      "Epoch [4/10], Batch [341/938], Loss: 0.5765\n",
      "Epoch [4/10], Batch [342/938], Loss: 0.5910\n",
      "Epoch [4/10], Batch [343/938], Loss: 0.5844\n",
      "Epoch [4/10], Batch [344/938], Loss: 0.5709\n",
      "Epoch [4/10], Batch [345/938], Loss: 0.6372\n",
      "Epoch [4/10], Batch [346/938], Loss: 0.6021\n",
      "Epoch [4/10], Batch [347/938], Loss: 0.6033\n",
      "Epoch [4/10], Batch [348/938], Loss: 0.6133\n",
      "Epoch [4/10], Batch [349/938], Loss: 0.6023\n",
      "Epoch [4/10], Batch [350/938], Loss: 0.6064\n",
      "Epoch [4/10], Batch [351/938], Loss: 0.6212\n",
      "Epoch [4/10], Batch [352/938], Loss: 0.6084\n",
      "Epoch [4/10], Batch [353/938], Loss: 0.5631\n",
      "Epoch [4/10], Batch [354/938], Loss: 0.5867\n",
      "Epoch [4/10], Batch [355/938], Loss: 0.6199\n",
      "Epoch [4/10], Batch [356/938], Loss: 0.5877\n",
      "Epoch [4/10], Batch [357/938], Loss: 0.5891\n",
      "Epoch [4/10], Batch [358/938], Loss: 0.5951\n",
      "Epoch [4/10], Batch [359/938], Loss: 0.5854\n",
      "Epoch [4/10], Batch [360/938], Loss: 0.5839\n",
      "Epoch [4/10], Batch [361/938], Loss: 0.6046\n",
      "Epoch [4/10], Batch [362/938], Loss: 0.5792\n",
      "Epoch [4/10], Batch [363/938], Loss: 0.5800\n",
      "Epoch [4/10], Batch [364/938], Loss: 0.6047\n",
      "Epoch [4/10], Batch [365/938], Loss: 0.5876\n",
      "Epoch [4/10], Batch [366/938], Loss: 0.5891\n",
      "Epoch [4/10], Batch [367/938], Loss: 0.6078\n",
      "Epoch [4/10], Batch [368/938], Loss: 0.5737\n",
      "Epoch [4/10], Batch [369/938], Loss: 0.5852\n",
      "Epoch [4/10], Batch [370/938], Loss: 0.5860\n",
      "Epoch [4/10], Batch [371/938], Loss: 0.5992\n",
      "Epoch [4/10], Batch [372/938], Loss: 0.6026\n",
      "Epoch [4/10], Batch [373/938], Loss: 0.5897\n",
      "Epoch [4/10], Batch [374/938], Loss: 0.5933\n",
      "Epoch [4/10], Batch [375/938], Loss: 0.5973\n",
      "Epoch [4/10], Batch [376/938], Loss: 0.5995\n",
      "Epoch [4/10], Batch [377/938], Loss: 0.5861\n",
      "Epoch [4/10], Batch [378/938], Loss: 0.5895\n",
      "Epoch [4/10], Batch [379/938], Loss: 0.6246\n",
      "Epoch [4/10], Batch [380/938], Loss: 0.5850\n",
      "Epoch [4/10], Batch [381/938], Loss: 0.6372\n",
      "Epoch [4/10], Batch [382/938], Loss: 0.5980\n",
      "Epoch [4/10], Batch [383/938], Loss: 0.5757\n",
      "Epoch [4/10], Batch [384/938], Loss: 0.5749\n",
      "Epoch [4/10], Batch [385/938], Loss: 0.5950\n",
      "Epoch [4/10], Batch [386/938], Loss: 0.6044\n",
      "Epoch [4/10], Batch [387/938], Loss: 0.5934\n",
      "Epoch [4/10], Batch [388/938], Loss: 0.5932\n",
      "Epoch [4/10], Batch [389/938], Loss: 0.5764\n",
      "Epoch [4/10], Batch [390/938], Loss: 0.5989\n",
      "Epoch [4/10], Batch [391/938], Loss: 0.5996\n",
      "Epoch [4/10], Batch [392/938], Loss: 0.6127\n",
      "Epoch [4/10], Batch [393/938], Loss: 0.6043\n",
      "Epoch [4/10], Batch [394/938], Loss: 0.5970\n",
      "Epoch [4/10], Batch [395/938], Loss: 0.5856\n",
      "Epoch [4/10], Batch [396/938], Loss: 0.6051\n",
      "Epoch [4/10], Batch [397/938], Loss: 0.5897\n",
      "Epoch [4/10], Batch [398/938], Loss: 0.6239\n",
      "Epoch [4/10], Batch [399/938], Loss: 0.5756\n",
      "Epoch [4/10], Batch [400/938], Loss: 0.6009\n",
      "Epoch [4/10], Batch [401/938], Loss: 0.6028\n",
      "Epoch [4/10], Batch [402/938], Loss: 0.5961\n",
      "Epoch [4/10], Batch [403/938], Loss: 0.5809\n",
      "Epoch [4/10], Batch [404/938], Loss: 0.6032\n",
      "Epoch [4/10], Batch [405/938], Loss: 0.5889\n",
      "Epoch [4/10], Batch [406/938], Loss: 0.5977\n",
      "Epoch [4/10], Batch [407/938], Loss: 0.5926\n",
      "Epoch [4/10], Batch [408/938], Loss: 0.6125\n",
      "Epoch [4/10], Batch [409/938], Loss: 0.5908\n",
      "Epoch [4/10], Batch [410/938], Loss: 0.5964\n",
      "Epoch [4/10], Batch [411/938], Loss: 0.5660\n",
      "Epoch [4/10], Batch [412/938], Loss: 0.5701\n",
      "Epoch [4/10], Batch [413/938], Loss: 0.6064\n",
      "Epoch [4/10], Batch [414/938], Loss: 0.5806\n",
      "Epoch [4/10], Batch [415/938], Loss: 0.5826\n",
      "Epoch [4/10], Batch [416/938], Loss: 0.5784\n",
      "Epoch [4/10], Batch [417/938], Loss: 0.6240\n",
      "Epoch [4/10], Batch [418/938], Loss: 0.5951\n",
      "Epoch [4/10], Batch [419/938], Loss: 0.6029\n",
      "Epoch [4/10], Batch [420/938], Loss: 0.5920\n",
      "Epoch [4/10], Batch [421/938], Loss: 0.5962\n",
      "Epoch [4/10], Batch [422/938], Loss: 0.5754\n",
      "Epoch [4/10], Batch [423/938], Loss: 0.6348\n",
      "Epoch [4/10], Batch [424/938], Loss: 0.6190\n",
      "Epoch [4/10], Batch [425/938], Loss: 0.5979\n",
      "Epoch [4/10], Batch [426/938], Loss: 0.6152\n",
      "Epoch [4/10], Batch [427/938], Loss: 0.5865\n",
      "Epoch [4/10], Batch [428/938], Loss: 0.5895\n",
      "Epoch [4/10], Batch [429/938], Loss: 0.6085\n",
      "Epoch [4/10], Batch [430/938], Loss: 0.5692\n",
      "Epoch [4/10], Batch [431/938], Loss: 0.6060\n",
      "Epoch [4/10], Batch [432/938], Loss: 0.5716\n",
      "Epoch [4/10], Batch [433/938], Loss: 0.5770\n",
      "Epoch [4/10], Batch [434/938], Loss: 0.6163\n",
      "Epoch [4/10], Batch [435/938], Loss: 0.6138\n",
      "Epoch [4/10], Batch [436/938], Loss: 0.5737\n",
      "Epoch [4/10], Batch [437/938], Loss: 0.5842\n",
      "Epoch [4/10], Batch [438/938], Loss: 0.5803\n",
      "Epoch [4/10], Batch [439/938], Loss: 0.5832\n",
      "Epoch [4/10], Batch [440/938], Loss: 0.6238\n",
      "Epoch [4/10], Batch [441/938], Loss: 0.5772\n",
      "Epoch [4/10], Batch [442/938], Loss: 0.5999\n",
      "Epoch [4/10], Batch [443/938], Loss: 0.5734\n",
      "Epoch [4/10], Batch [444/938], Loss: 0.5849\n",
      "Epoch [4/10], Batch [445/938], Loss: 0.6001\n",
      "Epoch [4/10], Batch [446/938], Loss: 0.5728\n",
      "Epoch [4/10], Batch [447/938], Loss: 0.6065\n",
      "Epoch [4/10], Batch [448/938], Loss: 0.6259\n",
      "Epoch [4/10], Batch [449/938], Loss: 0.6083\n",
      "Epoch [4/10], Batch [450/938], Loss: 0.5981\n",
      "Epoch [4/10], Batch [451/938], Loss: 0.5790\n",
      "Epoch [4/10], Batch [452/938], Loss: 0.6183\n",
      "Epoch [4/10], Batch [453/938], Loss: 0.6105\n",
      "Epoch [4/10], Batch [454/938], Loss: 0.5932\n",
      "Epoch [4/10], Batch [455/938], Loss: 0.5904\n",
      "Epoch [4/10], Batch [456/938], Loss: 0.5828\n",
      "Epoch [4/10], Batch [457/938], Loss: 0.5582\n",
      "Epoch [4/10], Batch [458/938], Loss: 0.6022\n",
      "Epoch [4/10], Batch [459/938], Loss: 0.5817\n",
      "Epoch [4/10], Batch [460/938], Loss: 0.6173\n",
      "Epoch [4/10], Batch [461/938], Loss: 0.5932\n",
      "Epoch [4/10], Batch [462/938], Loss: 0.5840\n",
      "Epoch [4/10], Batch [463/938], Loss: 0.6436\n",
      "Epoch [4/10], Batch [464/938], Loss: 0.5924\n",
      "Epoch [4/10], Batch [465/938], Loss: 0.5802\n",
      "Epoch [4/10], Batch [466/938], Loss: 0.5763\n",
      "Epoch [4/10], Batch [467/938], Loss: 0.6066\n",
      "Epoch [4/10], Batch [468/938], Loss: 0.5960\n",
      "Epoch [4/10], Batch [469/938], Loss: 0.6111\n",
      "Epoch [4/10], Batch [470/938], Loss: 0.5980\n",
      "Epoch [4/10], Batch [471/938], Loss: 0.6057\n",
      "Epoch [4/10], Batch [472/938], Loss: 0.5933\n",
      "Epoch [4/10], Batch [473/938], Loss: 0.5687\n",
      "Epoch [4/10], Batch [474/938], Loss: 0.5849\n",
      "Epoch [4/10], Batch [475/938], Loss: 0.6190\n",
      "Epoch [4/10], Batch [476/938], Loss: 0.6084\n",
      "Epoch [4/10], Batch [477/938], Loss: 0.5954\n",
      "Epoch [4/10], Batch [478/938], Loss: 0.6121\n",
      "Epoch [4/10], Batch [479/938], Loss: 0.5809\n",
      "Epoch [4/10], Batch [480/938], Loss: 0.5884\n",
      "Epoch [4/10], Batch [481/938], Loss: 0.5730\n",
      "Epoch [4/10], Batch [482/938], Loss: 0.5937\n",
      "Epoch [4/10], Batch [483/938], Loss: 0.5858\n",
      "Epoch [4/10], Batch [484/938], Loss: 0.6105\n",
      "Epoch [4/10], Batch [485/938], Loss: 0.5937\n",
      "Epoch [4/10], Batch [486/938], Loss: 0.6029\n",
      "Epoch [4/10], Batch [487/938], Loss: 0.5658\n",
      "Epoch [4/10], Batch [488/938], Loss: 0.5625\n",
      "Epoch [4/10], Batch [489/938], Loss: 0.5777\n",
      "Epoch [4/10], Batch [490/938], Loss: 0.5807\n",
      "Epoch [4/10], Batch [491/938], Loss: 0.5788\n",
      "Epoch [4/10], Batch [492/938], Loss: 0.6013\n",
      "Epoch [4/10], Batch [493/938], Loss: 0.5766\n",
      "Epoch [4/10], Batch [494/938], Loss: 0.6242\n",
      "Epoch [4/10], Batch [495/938], Loss: 0.6333\n",
      "Epoch [4/10], Batch [496/938], Loss: 0.5867\n",
      "Epoch [4/10], Batch [497/938], Loss: 0.5975\n",
      "Epoch [4/10], Batch [498/938], Loss: 0.6007\n",
      "Epoch [4/10], Batch [499/938], Loss: 0.5951\n",
      "Epoch [4/10], Batch [500/938], Loss: 0.5915\n",
      "Epoch [4/10], Batch [501/938], Loss: 0.5806\n",
      "Epoch [4/10], Batch [502/938], Loss: 0.5941\n",
      "Epoch [4/10], Batch [503/938], Loss: 0.5939\n",
      "Epoch [4/10], Batch [504/938], Loss: 0.5585\n",
      "Epoch [4/10], Batch [505/938], Loss: 0.5884\n",
      "Epoch [4/10], Batch [506/938], Loss: 0.5674\n",
      "Epoch [4/10], Batch [507/938], Loss: 0.6123\n",
      "Epoch [4/10], Batch [508/938], Loss: 0.5845\n",
      "Epoch [4/10], Batch [509/938], Loss: 0.5795\n",
      "Epoch [4/10], Batch [510/938], Loss: 0.6138\n",
      "Epoch [4/10], Batch [511/938], Loss: 0.5872\n",
      "Epoch [4/10], Batch [512/938], Loss: 0.5990\n",
      "Epoch [4/10], Batch [513/938], Loss: 0.6140\n",
      "Epoch [4/10], Batch [514/938], Loss: 0.6091\n",
      "Epoch [4/10], Batch [515/938], Loss: 0.5960\n",
      "Epoch [4/10], Batch [516/938], Loss: 0.6105\n",
      "Epoch [4/10], Batch [517/938], Loss: 0.6001\n",
      "Epoch [4/10], Batch [518/938], Loss: 0.5935\n",
      "Epoch [4/10], Batch [519/938], Loss: 0.5974\n",
      "Epoch [4/10], Batch [520/938], Loss: 0.5954\n",
      "Epoch [4/10], Batch [521/938], Loss: 0.5963\n",
      "Epoch [4/10], Batch [522/938], Loss: 0.6048\n",
      "Epoch [4/10], Batch [523/938], Loss: 0.5706\n",
      "Epoch [4/10], Batch [524/938], Loss: 0.6087\n",
      "Epoch [4/10], Batch [525/938], Loss: 0.5987\n",
      "Epoch [4/10], Batch [526/938], Loss: 0.6056\n",
      "Epoch [4/10], Batch [527/938], Loss: 0.5961\n",
      "Epoch [4/10], Batch [528/938], Loss: 0.6105\n",
      "Epoch [4/10], Batch [529/938], Loss: 0.5751\n",
      "Epoch [4/10], Batch [530/938], Loss: 0.5764\n",
      "Epoch [4/10], Batch [531/938], Loss: 0.6195\n",
      "Epoch [4/10], Batch [532/938], Loss: 0.6150\n",
      "Epoch [4/10], Batch [533/938], Loss: 0.5973\n",
      "Epoch [4/10], Batch [534/938], Loss: 0.5698\n",
      "Epoch [4/10], Batch [535/938], Loss: 0.6283\n",
      "Epoch [4/10], Batch [536/938], Loss: 0.6253\n",
      "Epoch [4/10], Batch [537/938], Loss: 0.5837\n",
      "Epoch [4/10], Batch [538/938], Loss: 0.5929\n",
      "Epoch [4/10], Batch [539/938], Loss: 0.5954\n",
      "Epoch [4/10], Batch [540/938], Loss: 0.5951\n",
      "Epoch [4/10], Batch [541/938], Loss: 0.5677\n",
      "Epoch [4/10], Batch [542/938], Loss: 0.6079\n",
      "Epoch [4/10], Batch [543/938], Loss: 0.6172\n",
      "Epoch [4/10], Batch [544/938], Loss: 0.5814\n",
      "Epoch [4/10], Batch [545/938], Loss: 0.6306\n",
      "Epoch [4/10], Batch [546/938], Loss: 0.5891\n",
      "Epoch [4/10], Batch [547/938], Loss: 0.6153\n",
      "Epoch [4/10], Batch [548/938], Loss: 0.6003\n",
      "Epoch [4/10], Batch [549/938], Loss: 0.5799\n",
      "Epoch [4/10], Batch [550/938], Loss: 0.5939\n",
      "Epoch [4/10], Batch [551/938], Loss: 0.6174\n",
      "Epoch [4/10], Batch [552/938], Loss: 0.5889\n",
      "Epoch [4/10], Batch [553/938], Loss: 0.6181\n",
      "Epoch [4/10], Batch [554/938], Loss: 0.5933\n",
      "Epoch [4/10], Batch [555/938], Loss: 0.5920\n",
      "Epoch [4/10], Batch [556/938], Loss: 0.5959\n",
      "Epoch [4/10], Batch [557/938], Loss: 0.5976\n",
      "Epoch [4/10], Batch [558/938], Loss: 0.5931\n",
      "Epoch [4/10], Batch [559/938], Loss: 0.5837\n",
      "Epoch [4/10], Batch [560/938], Loss: 0.6083\n",
      "Epoch [4/10], Batch [561/938], Loss: 0.6031\n",
      "Epoch [4/10], Batch [562/938], Loss: 0.6008\n",
      "Epoch [4/10], Batch [563/938], Loss: 0.5852\n",
      "Epoch [4/10], Batch [564/938], Loss: 0.6100\n",
      "Epoch [4/10], Batch [565/938], Loss: 0.5910\n",
      "Epoch [4/10], Batch [566/938], Loss: 0.6248\n",
      "Epoch [4/10], Batch [567/938], Loss: 0.5614\n",
      "Epoch [4/10], Batch [568/938], Loss: 0.6272\n",
      "Epoch [4/10], Batch [569/938], Loss: 0.6096\n",
      "Epoch [4/10], Batch [570/938], Loss: 0.6403\n",
      "Epoch [4/10], Batch [571/938], Loss: 0.6061\n",
      "Epoch [4/10], Batch [572/938], Loss: 0.6071\n",
      "Epoch [4/10], Batch [573/938], Loss: 0.6049\n",
      "Epoch [4/10], Batch [574/938], Loss: 0.5946\n",
      "Epoch [4/10], Batch [575/938], Loss: 0.5655\n",
      "Epoch [4/10], Batch [576/938], Loss: 0.5615\n",
      "Epoch [4/10], Batch [577/938], Loss: 0.5938\n",
      "Epoch [4/10], Batch [578/938], Loss: 0.6047\n",
      "Epoch [4/10], Batch [579/938], Loss: 0.5680\n",
      "Epoch [4/10], Batch [580/938], Loss: 0.5956\n",
      "Epoch [4/10], Batch [581/938], Loss: 0.5905\n",
      "Epoch [4/10], Batch [582/938], Loss: 0.5618\n",
      "Epoch [4/10], Batch [583/938], Loss: 0.6109\n",
      "Epoch [4/10], Batch [584/938], Loss: 0.6141\n",
      "Epoch [4/10], Batch [585/938], Loss: 0.5943\n",
      "Epoch [4/10], Batch [586/938], Loss: 0.5655\n",
      "Epoch [4/10], Batch [587/938], Loss: 0.5986\n",
      "Epoch [4/10], Batch [588/938], Loss: 0.5905\n",
      "Epoch [4/10], Batch [589/938], Loss: 0.5854\n",
      "Epoch [4/10], Batch [590/938], Loss: 0.6107\n",
      "Epoch [4/10], Batch [591/938], Loss: 0.6228\n",
      "Epoch [4/10], Batch [592/938], Loss: 0.5758\n",
      "Epoch [4/10], Batch [593/938], Loss: 0.6047\n",
      "Epoch [4/10], Batch [594/938], Loss: 0.5911\n",
      "Epoch [4/10], Batch [595/938], Loss: 0.5781\n",
      "Epoch [4/10], Batch [596/938], Loss: 0.5730\n",
      "Epoch [4/10], Batch [597/938], Loss: 0.6088\n",
      "Epoch [4/10], Batch [598/938], Loss: 0.5897\n",
      "Epoch [4/10], Batch [599/938], Loss: 0.6229\n",
      "Epoch [4/10], Batch [600/938], Loss: 0.6022\n",
      "Epoch [4/10], Batch [601/938], Loss: 0.5787\n",
      "Epoch [4/10], Batch [602/938], Loss: 0.5855\n",
      "Epoch [4/10], Batch [603/938], Loss: 0.5987\n",
      "Epoch [4/10], Batch [604/938], Loss: 0.5713\n",
      "Epoch [4/10], Batch [605/938], Loss: 0.5922\n",
      "Epoch [4/10], Batch [606/938], Loss: 0.5958\n",
      "Epoch [4/10], Batch [607/938], Loss: 0.5985\n",
      "Epoch [4/10], Batch [608/938], Loss: 0.6185\n",
      "Epoch [4/10], Batch [609/938], Loss: 0.5759\n",
      "Epoch [4/10], Batch [610/938], Loss: 0.5993\n",
      "Epoch [4/10], Batch [611/938], Loss: 0.6244\n",
      "Epoch [4/10], Batch [612/938], Loss: 0.5822\n",
      "Epoch [4/10], Batch [613/938], Loss: 0.5823\n",
      "Epoch [4/10], Batch [614/938], Loss: 0.6002\n",
      "Epoch [4/10], Batch [615/938], Loss: 0.5920\n",
      "Epoch [4/10], Batch [616/938], Loss: 0.5885\n",
      "Epoch [4/10], Batch [617/938], Loss: 0.6040\n",
      "Epoch [4/10], Batch [618/938], Loss: 0.5831\n",
      "Epoch [4/10], Batch [619/938], Loss: 0.5822\n",
      "Epoch [4/10], Batch [620/938], Loss: 0.6074\n",
      "Epoch [4/10], Batch [621/938], Loss: 0.5783\n",
      "Epoch [4/10], Batch [622/938], Loss: 0.5897\n",
      "Epoch [4/10], Batch [623/938], Loss: 0.5620\n",
      "Epoch [4/10], Batch [624/938], Loss: 0.6309\n",
      "Epoch [4/10], Batch [625/938], Loss: 0.5897\n",
      "Epoch [4/10], Batch [626/938], Loss: 0.5846\n",
      "Epoch [4/10], Batch [627/938], Loss: 0.5812\n",
      "Epoch [4/10], Batch [628/938], Loss: 0.6265\n",
      "Epoch [4/10], Batch [629/938], Loss: 0.5701\n",
      "Epoch [4/10], Batch [630/938], Loss: 0.6075\n",
      "Epoch [4/10], Batch [631/938], Loss: 0.5939\n",
      "Epoch [4/10], Batch [632/938], Loss: 0.5775\n",
      "Epoch [4/10], Batch [633/938], Loss: 0.6134\n",
      "Epoch [4/10], Batch [634/938], Loss: 0.6281\n",
      "Epoch [4/10], Batch [635/938], Loss: 0.6179\n",
      "Epoch [4/10], Batch [636/938], Loss: 0.5923\n",
      "Epoch [4/10], Batch [637/938], Loss: 0.6143\n",
      "Epoch [4/10], Batch [638/938], Loss: 0.5799\n",
      "Epoch [4/10], Batch [639/938], Loss: 0.5564\n",
      "Epoch [4/10], Batch [640/938], Loss: 0.6115\n",
      "Epoch [4/10], Batch [641/938], Loss: 0.5931\n",
      "Epoch [4/10], Batch [642/938], Loss: 0.5755\n",
      "Epoch [4/10], Batch [643/938], Loss: 0.5871\n",
      "Epoch [4/10], Batch [644/938], Loss: 0.5876\n",
      "Epoch [4/10], Batch [645/938], Loss: 0.6084\n",
      "Epoch [4/10], Batch [646/938], Loss: 0.6018\n",
      "Epoch [4/10], Batch [647/938], Loss: 0.6054\n",
      "Epoch [4/10], Batch [648/938], Loss: 0.6358\n",
      "Epoch [4/10], Batch [649/938], Loss: 0.5915\n",
      "Epoch [4/10], Batch [650/938], Loss: 0.5972\n",
      "Epoch [4/10], Batch [651/938], Loss: 0.5994\n",
      "Epoch [4/10], Batch [652/938], Loss: 0.5766\n",
      "Epoch [4/10], Batch [653/938], Loss: 0.6029\n",
      "Epoch [4/10], Batch [654/938], Loss: 0.5881\n",
      "Epoch [4/10], Batch [655/938], Loss: 0.5939\n",
      "Epoch [4/10], Batch [656/938], Loss: 0.5959\n",
      "Epoch [4/10], Batch [657/938], Loss: 0.6122\n",
      "Epoch [4/10], Batch [658/938], Loss: 0.5980\n",
      "Epoch [4/10], Batch [659/938], Loss: 0.5868\n",
      "Epoch [4/10], Batch [660/938], Loss: 0.5904\n",
      "Epoch [4/10], Batch [661/938], Loss: 0.5895\n",
      "Epoch [4/10], Batch [662/938], Loss: 0.6129\n",
      "Epoch [4/10], Batch [663/938], Loss: 0.5770\n",
      "Epoch [4/10], Batch [664/938], Loss: 0.6231\n",
      "Epoch [4/10], Batch [665/938], Loss: 0.5948\n",
      "Epoch [4/10], Batch [666/938], Loss: 0.6020\n",
      "Epoch [4/10], Batch [667/938], Loss: 0.5615\n",
      "Epoch [4/10], Batch [668/938], Loss: 0.5947\n",
      "Epoch [4/10], Batch [669/938], Loss: 0.5670\n",
      "Epoch [4/10], Batch [670/938], Loss: 0.5789\n",
      "Epoch [4/10], Batch [671/938], Loss: 0.5910\n",
      "Epoch [4/10], Batch [672/938], Loss: 0.5853\n",
      "Epoch [4/10], Batch [673/938], Loss: 0.5960\n",
      "Epoch [4/10], Batch [674/938], Loss: 0.6070\n",
      "Epoch [4/10], Batch [675/938], Loss: 0.6029\n",
      "Epoch [4/10], Batch [676/938], Loss: 0.5775\n",
      "Epoch [4/10], Batch [677/938], Loss: 0.6000\n",
      "Epoch [4/10], Batch [678/938], Loss: 0.5992\n",
      "Epoch [4/10], Batch [679/938], Loss: 0.5801\n",
      "Epoch [4/10], Batch [680/938], Loss: 0.5542\n",
      "Epoch [4/10], Batch [681/938], Loss: 0.6011\n",
      "Epoch [4/10], Batch [682/938], Loss: 0.5903\n",
      "Epoch [4/10], Batch [683/938], Loss: 0.6101\n",
      "Epoch [4/10], Batch [684/938], Loss: 0.5878\n",
      "Epoch [4/10], Batch [685/938], Loss: 0.5596\n",
      "Epoch [4/10], Batch [686/938], Loss: 0.6195\n",
      "Epoch [4/10], Batch [687/938], Loss: 0.6128\n",
      "Epoch [4/10], Batch [688/938], Loss: 0.6358\n",
      "Epoch [4/10], Batch [689/938], Loss: 0.5876\n",
      "Epoch [4/10], Batch [690/938], Loss: 0.5893\n",
      "Epoch [4/10], Batch [691/938], Loss: 0.5988\n",
      "Epoch [4/10], Batch [692/938], Loss: 0.6141\n",
      "Epoch [4/10], Batch [693/938], Loss: 0.5777\n",
      "Epoch [4/10], Batch [694/938], Loss: 0.5946\n",
      "Epoch [4/10], Batch [695/938], Loss: 0.6148\n",
      "Epoch [4/10], Batch [696/938], Loss: 0.6050\n",
      "Epoch [4/10], Batch [697/938], Loss: 0.6010\n",
      "Epoch [4/10], Batch [698/938], Loss: 0.5875\n",
      "Epoch [4/10], Batch [699/938], Loss: 0.5894\n",
      "Epoch [4/10], Batch [700/938], Loss: 0.5957\n",
      "Epoch [4/10], Batch [701/938], Loss: 0.5928\n",
      "Epoch [4/10], Batch [702/938], Loss: 0.5991\n",
      "Epoch [4/10], Batch [703/938], Loss: 0.5898\n",
      "Epoch [4/10], Batch [704/938], Loss: 0.5883\n",
      "Epoch [4/10], Batch [705/938], Loss: 0.6063\n",
      "Epoch [4/10], Batch [706/938], Loss: 0.6119\n",
      "Epoch [4/10], Batch [707/938], Loss: 0.5882\n",
      "Epoch [4/10], Batch [708/938], Loss: 0.5786\n",
      "Epoch [4/10], Batch [709/938], Loss: 0.6281\n",
      "Epoch [4/10], Batch [710/938], Loss: 0.5964\n",
      "Epoch [4/10], Batch [711/938], Loss: 0.5967\n",
      "Epoch [4/10], Batch [712/938], Loss: 0.6037\n",
      "Epoch [4/10], Batch [713/938], Loss: 0.5730\n",
      "Epoch [4/10], Batch [714/938], Loss: 0.6024\n",
      "Epoch [4/10], Batch [715/938], Loss: 0.5808\n",
      "Epoch [4/10], Batch [716/938], Loss: 0.5810\n",
      "Epoch [4/10], Batch [717/938], Loss: 0.6090\n",
      "Epoch [4/10], Batch [718/938], Loss: 0.6046\n",
      "Epoch [4/10], Batch [719/938], Loss: 0.5728\n",
      "Epoch [4/10], Batch [720/938], Loss: 0.5904\n",
      "Epoch [4/10], Batch [721/938], Loss: 0.6182\n",
      "Epoch [4/10], Batch [722/938], Loss: 0.6053\n",
      "Epoch [4/10], Batch [723/938], Loss: 0.5794\n",
      "Epoch [4/10], Batch [724/938], Loss: 0.5855\n",
      "Epoch [4/10], Batch [725/938], Loss: 0.6227\n",
      "Epoch [4/10], Batch [726/938], Loss: 0.5917\n",
      "Epoch [4/10], Batch [727/938], Loss: 0.5648\n",
      "Epoch [4/10], Batch [728/938], Loss: 0.5637\n",
      "Epoch [4/10], Batch [729/938], Loss: 0.5996\n",
      "Epoch [4/10], Batch [730/938], Loss: 0.6121\n",
      "Epoch [4/10], Batch [731/938], Loss: 0.5964\n",
      "Epoch [4/10], Batch [732/938], Loss: 0.5820\n",
      "Epoch [4/10], Batch [733/938], Loss: 0.5723\n",
      "Epoch [4/10], Batch [734/938], Loss: 0.5597\n",
      "Epoch [4/10], Batch [735/938], Loss: 0.5923\n",
      "Epoch [4/10], Batch [736/938], Loss: 0.6350\n",
      "Epoch [4/10], Batch [737/938], Loss: 0.6147\n",
      "Epoch [4/10], Batch [738/938], Loss: 0.6196\n",
      "Epoch [4/10], Batch [739/938], Loss: 0.5996\n",
      "Epoch [4/10], Batch [740/938], Loss: 0.6102\n",
      "Epoch [4/10], Batch [741/938], Loss: 0.5865\n",
      "Epoch [4/10], Batch [742/938], Loss: 0.5885\n",
      "Epoch [4/10], Batch [743/938], Loss: 0.5902\n",
      "Epoch [4/10], Batch [744/938], Loss: 0.5957\n",
      "Epoch [4/10], Batch [745/938], Loss: 0.6538\n",
      "Epoch [4/10], Batch [746/938], Loss: 0.6251\n",
      "Epoch [4/10], Batch [747/938], Loss: 0.5824\n",
      "Epoch [4/10], Batch [748/938], Loss: 0.5949\n",
      "Epoch [4/10], Batch [749/938], Loss: 0.5870\n",
      "Epoch [4/10], Batch [750/938], Loss: 0.5852\n",
      "Epoch [4/10], Batch [751/938], Loss: 0.5978\n",
      "Epoch [4/10], Batch [752/938], Loss: 0.5994\n",
      "Epoch [4/10], Batch [753/938], Loss: 0.5864\n",
      "Epoch [4/10], Batch [754/938], Loss: 0.5773\n",
      "Epoch [4/10], Batch [755/938], Loss: 0.5888\n",
      "Epoch [4/10], Batch [756/938], Loss: 0.5537\n",
      "Epoch [4/10], Batch [757/938], Loss: 0.6175\n",
      "Epoch [4/10], Batch [758/938], Loss: 0.5962\n",
      "Epoch [4/10], Batch [759/938], Loss: 0.6002\n",
      "Epoch [4/10], Batch [760/938], Loss: 0.6597\n",
      "Epoch [4/10], Batch [761/938], Loss: 0.6093\n",
      "Epoch [4/10], Batch [762/938], Loss: 0.6039\n",
      "Epoch [4/10], Batch [763/938], Loss: 0.5522\n",
      "Epoch [4/10], Batch [764/938], Loss: 0.5924\n",
      "Epoch [4/10], Batch [765/938], Loss: 0.5717\n",
      "Epoch [4/10], Batch [766/938], Loss: 0.6126\n",
      "Epoch [4/10], Batch [767/938], Loss: 0.6199\n",
      "Epoch [4/10], Batch [768/938], Loss: 0.5772\n",
      "Epoch [4/10], Batch [769/938], Loss: 0.5677\n",
      "Epoch [4/10], Batch [770/938], Loss: 0.5948\n",
      "Epoch [4/10], Batch [771/938], Loss: 0.6141\n",
      "Epoch [4/10], Batch [772/938], Loss: 0.5912\n",
      "Epoch [4/10], Batch [773/938], Loss: 0.5857\n",
      "Epoch [4/10], Batch [774/938], Loss: 0.5928\n",
      "Epoch [4/10], Batch [775/938], Loss: 0.6200\n",
      "Epoch [4/10], Batch [776/938], Loss: 0.5747\n",
      "Epoch [4/10], Batch [777/938], Loss: 0.6076\n",
      "Epoch [4/10], Batch [778/938], Loss: 0.5893\n",
      "Epoch [4/10], Batch [779/938], Loss: 0.5936\n",
      "Epoch [4/10], Batch [780/938], Loss: 0.6099\n",
      "Epoch [4/10], Batch [781/938], Loss: 0.5774\n",
      "Epoch [4/10], Batch [782/938], Loss: 0.5704\n",
      "Epoch [4/10], Batch [783/938], Loss: 0.5894\n",
      "Epoch [4/10], Batch [784/938], Loss: 0.5656\n",
      "Epoch [4/10], Batch [785/938], Loss: 0.5722\n",
      "Epoch [4/10], Batch [786/938], Loss: 0.6022\n",
      "Epoch [4/10], Batch [787/938], Loss: 0.6056\n",
      "Epoch [4/10], Batch [788/938], Loss: 0.5716\n",
      "Epoch [4/10], Batch [789/938], Loss: 0.6040\n",
      "Epoch [4/10], Batch [790/938], Loss: 0.6011\n",
      "Epoch [4/10], Batch [791/938], Loss: 0.5930\n",
      "Epoch [4/10], Batch [792/938], Loss: 0.6313\n",
      "Epoch [4/10], Batch [793/938], Loss: 0.6000\n",
      "Epoch [4/10], Batch [794/938], Loss: 0.5783\n",
      "Epoch [4/10], Batch [795/938], Loss: 0.6021\n",
      "Epoch [4/10], Batch [796/938], Loss: 0.5704\n",
      "Epoch [4/10], Batch [797/938], Loss: 0.5981\n",
      "Epoch [4/10], Batch [798/938], Loss: 0.5859\n",
      "Epoch [4/10], Batch [799/938], Loss: 0.6078\n",
      "Epoch [4/10], Batch [800/938], Loss: 0.5928\n",
      "Epoch [4/10], Batch [801/938], Loss: 0.6229\n",
      "Epoch [4/10], Batch [802/938], Loss: 0.5704\n",
      "Epoch [4/10], Batch [803/938], Loss: 0.5876\n",
      "Epoch [4/10], Batch [804/938], Loss: 0.5952\n",
      "Epoch [4/10], Batch [805/938], Loss: 0.5742\n",
      "Epoch [4/10], Batch [806/938], Loss: 0.5768\n",
      "Epoch [4/10], Batch [807/938], Loss: 0.5803\n",
      "Epoch [4/10], Batch [808/938], Loss: 0.5719\n",
      "Epoch [4/10], Batch [809/938], Loss: 0.5863\n",
      "Epoch [4/10], Batch [810/938], Loss: 0.5985\n",
      "Epoch [4/10], Batch [811/938], Loss: 0.5869\n",
      "Epoch [4/10], Batch [812/938], Loss: 0.6097\n",
      "Epoch [4/10], Batch [813/938], Loss: 0.5723\n",
      "Epoch [4/10], Batch [814/938], Loss: 0.5928\n",
      "Epoch [4/10], Batch [815/938], Loss: 0.6073\n",
      "Epoch [4/10], Batch [816/938], Loss: 0.5801\n",
      "Epoch [4/10], Batch [817/938], Loss: 0.5753\n",
      "Epoch [4/10], Batch [818/938], Loss: 0.5812\n",
      "Epoch [4/10], Batch [819/938], Loss: 0.5750\n",
      "Epoch [4/10], Batch [820/938], Loss: 0.6075\n",
      "Epoch [4/10], Batch [821/938], Loss: 0.5848\n",
      "Epoch [4/10], Batch [822/938], Loss: 0.6084\n",
      "Epoch [4/10], Batch [823/938], Loss: 0.5962\n",
      "Epoch [4/10], Batch [824/938], Loss: 0.6258\n",
      "Epoch [4/10], Batch [825/938], Loss: 0.5956\n",
      "Epoch [4/10], Batch [826/938], Loss: 0.5933\n",
      "Epoch [4/10], Batch [827/938], Loss: 0.5990\n",
      "Epoch [4/10], Batch [828/938], Loss: 0.5959\n",
      "Epoch [4/10], Batch [829/938], Loss: 0.5916\n",
      "Epoch [4/10], Batch [830/938], Loss: 0.5874\n",
      "Epoch [4/10], Batch [831/938], Loss: 0.6016\n",
      "Epoch [4/10], Batch [832/938], Loss: 0.6000\n",
      "Epoch [4/10], Batch [833/938], Loss: 0.5897\n",
      "Epoch [4/10], Batch [834/938], Loss: 0.5599\n",
      "Epoch [4/10], Batch [835/938], Loss: 0.5883\n",
      "Epoch [4/10], Batch [836/938], Loss: 0.6221\n",
      "Epoch [4/10], Batch [837/938], Loss: 0.5997\n",
      "Epoch [4/10], Batch [838/938], Loss: 0.6077\n",
      "Epoch [4/10], Batch [839/938], Loss: 0.5734\n",
      "Epoch [4/10], Batch [840/938], Loss: 0.5890\n",
      "Epoch [4/10], Batch [841/938], Loss: 0.5850\n",
      "Epoch [4/10], Batch [842/938], Loss: 0.5680\n",
      "Epoch [4/10], Batch [843/938], Loss: 0.6044\n",
      "Epoch [4/10], Batch [844/938], Loss: 0.6011\n",
      "Epoch [4/10], Batch [845/938], Loss: 0.5787\n",
      "Epoch [4/10], Batch [846/938], Loss: 0.5916\n",
      "Epoch [4/10], Batch [847/938], Loss: 0.5978\n",
      "Epoch [4/10], Batch [848/938], Loss: 0.5759\n",
      "Epoch [4/10], Batch [849/938], Loss: 0.6177\n",
      "Epoch [4/10], Batch [850/938], Loss: 0.5940\n",
      "Epoch [4/10], Batch [851/938], Loss: 0.5702\n",
      "Epoch [4/10], Batch [852/938], Loss: 0.5847\n",
      "Epoch [4/10], Batch [853/938], Loss: 0.6230\n",
      "Epoch [4/10], Batch [854/938], Loss: 0.5944\n",
      "Epoch [4/10], Batch [855/938], Loss: 0.5929\n",
      "Epoch [4/10], Batch [856/938], Loss: 0.5712\n",
      "Epoch [4/10], Batch [857/938], Loss: 0.5740\n",
      "Epoch [4/10], Batch [858/938], Loss: 0.5874\n",
      "Epoch [4/10], Batch [859/938], Loss: 0.6057\n",
      "Epoch [4/10], Batch [860/938], Loss: 0.5853\n",
      "Epoch [4/10], Batch [861/938], Loss: 0.6058\n",
      "Epoch [4/10], Batch [862/938], Loss: 0.5811\n",
      "Epoch [4/10], Batch [863/938], Loss: 0.6005\n",
      "Epoch [4/10], Batch [864/938], Loss: 0.6227\n",
      "Epoch [4/10], Batch [865/938], Loss: 0.6045\n",
      "Epoch [4/10], Batch [866/938], Loss: 0.5943\n",
      "Epoch [4/10], Batch [867/938], Loss: 0.5918\n",
      "Epoch [4/10], Batch [868/938], Loss: 0.5715\n",
      "Epoch [4/10], Batch [869/938], Loss: 0.5836\n",
      "Epoch [4/10], Batch [870/938], Loss: 0.6164\n",
      "Epoch [4/10], Batch [871/938], Loss: 0.6046\n",
      "Epoch [4/10], Batch [872/938], Loss: 0.6275\n",
      "Epoch [4/10], Batch [873/938], Loss: 0.5808\n",
      "Epoch [4/10], Batch [874/938], Loss: 0.5877\n",
      "Epoch [4/10], Batch [875/938], Loss: 0.5883\n",
      "Epoch [4/10], Batch [876/938], Loss: 0.5693\n",
      "Epoch [4/10], Batch [877/938], Loss: 0.5977\n",
      "Epoch [4/10], Batch [878/938], Loss: 0.5972\n",
      "Epoch [4/10], Batch [879/938], Loss: 0.5870\n",
      "Epoch [4/10], Batch [880/938], Loss: 0.5948\n",
      "Epoch [4/10], Batch [881/938], Loss: 0.5949\n",
      "Epoch [4/10], Batch [882/938], Loss: 0.5809\n",
      "Epoch [4/10], Batch [883/938], Loss: 0.5756\n",
      "Epoch [4/10], Batch [884/938], Loss: 0.6029\n",
      "Epoch [4/10], Batch [885/938], Loss: 0.5911\n",
      "Epoch [4/10], Batch [886/938], Loss: 0.6102\n",
      "Epoch [4/10], Batch [887/938], Loss: 0.6319\n",
      "Epoch [4/10], Batch [888/938], Loss: 0.5914\n",
      "Epoch [4/10], Batch [889/938], Loss: 0.6076\n",
      "Epoch [4/10], Batch [890/938], Loss: 0.5918\n",
      "Epoch [4/10], Batch [891/938], Loss: 0.5802\n",
      "Epoch [4/10], Batch [892/938], Loss: 0.5915\n",
      "Epoch [4/10], Batch [893/938], Loss: 0.6037\n",
      "Epoch [4/10], Batch [894/938], Loss: 0.6348\n",
      "Epoch [4/10], Batch [895/938], Loss: 0.5915\n",
      "Epoch [4/10], Batch [896/938], Loss: 0.5821\n",
      "Epoch [4/10], Batch [897/938], Loss: 0.6303\n",
      "Epoch [4/10], Batch [898/938], Loss: 0.5810\n",
      "Epoch [4/10], Batch [899/938], Loss: 0.5896\n",
      "Epoch [4/10], Batch [900/938], Loss: 0.5934\n",
      "Epoch [4/10], Batch [901/938], Loss: 0.5904\n",
      "Epoch [4/10], Batch [902/938], Loss: 0.5774\n",
      "Epoch [4/10], Batch [903/938], Loss: 0.6136\n",
      "Epoch [4/10], Batch [904/938], Loss: 0.5917\n",
      "Epoch [4/10], Batch [905/938], Loss: 0.6001\n",
      "Epoch [4/10], Batch [906/938], Loss: 0.6127\n",
      "Epoch [4/10], Batch [907/938], Loss: 0.6422\n",
      "Epoch [4/10], Batch [908/938], Loss: 0.5777\n",
      "Epoch [4/10], Batch [909/938], Loss: 0.6077\n",
      "Epoch [4/10], Batch [910/938], Loss: 0.5658\n",
      "Epoch [4/10], Batch [911/938], Loss: 0.6080\n",
      "Epoch [4/10], Batch [912/938], Loss: 0.5669\n",
      "Epoch [4/10], Batch [913/938], Loss: 0.5812\n",
      "Epoch [4/10], Batch [914/938], Loss: 0.5594\n",
      "Epoch [4/10], Batch [915/938], Loss: 0.6098\n",
      "Epoch [4/10], Batch [916/938], Loss: 0.6031\n",
      "Epoch [4/10], Batch [917/938], Loss: 0.6073\n",
      "Epoch [4/10], Batch [918/938], Loss: 0.5869\n",
      "Epoch [4/10], Batch [919/938], Loss: 0.6202\n",
      "Epoch [4/10], Batch [920/938], Loss: 0.5940\n",
      "Epoch [4/10], Batch [921/938], Loss: 0.6264\n",
      "Epoch [4/10], Batch [922/938], Loss: 0.6061\n",
      "Epoch [4/10], Batch [923/938], Loss: 0.6093\n",
      "Epoch [4/10], Batch [924/938], Loss: 0.6023\n",
      "Epoch [4/10], Batch [925/938], Loss: 0.5982\n",
      "Epoch [4/10], Batch [926/938], Loss: 0.5798\n",
      "Epoch [4/10], Batch [927/938], Loss: 0.5972\n",
      "Epoch [4/10], Batch [928/938], Loss: 0.6037\n",
      "Epoch [4/10], Batch [929/938], Loss: 0.5917\n",
      "Epoch [4/10], Batch [930/938], Loss: 0.6143\n",
      "Epoch [4/10], Batch [931/938], Loss: 0.5929\n",
      "Epoch [4/10], Batch [932/938], Loss: 0.5939\n",
      "Epoch [4/10], Batch [933/938], Loss: 0.5984\n",
      "Epoch [4/10], Batch [934/938], Loss: 0.5951\n",
      "Epoch [4/10], Batch [935/938], Loss: 0.5863\n",
      "Epoch [4/10], Batch [936/938], Loss: 0.6411\n",
      "Epoch [4/10], Batch [937/938], Loss: 0.5953\n",
      "Epoch [4/10], Batch [938/938], Loss: 0.5454\n",
      "Epoch [4/10], Loss: 0.5454\n",
      "Epoch [5/10], Batch [1/938], Loss: 0.5850\n",
      "Epoch [5/10], Batch [2/938], Loss: 0.6121\n",
      "Epoch [5/10], Batch [3/938], Loss: 0.5793\n",
      "Epoch [5/10], Batch [4/938], Loss: 0.6015\n",
      "Epoch [5/10], Batch [5/938], Loss: 0.6200\n",
      "Epoch [5/10], Batch [6/938], Loss: 0.5855\n",
      "Epoch [5/10], Batch [7/938], Loss: 0.5883\n",
      "Epoch [5/10], Batch [8/938], Loss: 0.5775\n",
      "Epoch [5/10], Batch [9/938], Loss: 0.5912\n",
      "Epoch [5/10], Batch [10/938], Loss: 0.5995\n",
      "Epoch [5/10], Batch [11/938], Loss: 0.5748\n",
      "Epoch [5/10], Batch [12/938], Loss: 0.5616\n",
      "Epoch [5/10], Batch [13/938], Loss: 0.5625\n",
      "Epoch [5/10], Batch [14/938], Loss: 0.6207\n",
      "Epoch [5/10], Batch [15/938], Loss: 0.5873\n",
      "Epoch [5/10], Batch [16/938], Loss: 0.5689\n",
      "Epoch [5/10], Batch [17/938], Loss: 0.6270\n",
      "Epoch [5/10], Batch [18/938], Loss: 0.5917\n",
      "Epoch [5/10], Batch [19/938], Loss: 0.6068\n",
      "Epoch [5/10], Batch [20/938], Loss: 0.5874\n",
      "Epoch [5/10], Batch [21/938], Loss: 0.5644\n",
      "Epoch [5/10], Batch [22/938], Loss: 0.5926\n",
      "Epoch [5/10], Batch [23/938], Loss: 0.6031\n",
      "Epoch [5/10], Batch [24/938], Loss: 0.5731\n",
      "Epoch [5/10], Batch [25/938], Loss: 0.5950\n",
      "Epoch [5/10], Batch [26/938], Loss: 0.5713\n",
      "Epoch [5/10], Batch [27/938], Loss: 0.5582\n",
      "Epoch [5/10], Batch [28/938], Loss: 0.6452\n",
      "Epoch [5/10], Batch [29/938], Loss: 0.6020\n",
      "Epoch [5/10], Batch [30/938], Loss: 0.5992\n",
      "Epoch [5/10], Batch [31/938], Loss: 0.5739\n",
      "Epoch [5/10], Batch [32/938], Loss: 0.5840\n",
      "Epoch [5/10], Batch [33/938], Loss: 0.5860\n",
      "Epoch [5/10], Batch [34/938], Loss: 0.5880\n",
      "Epoch [5/10], Batch [35/938], Loss: 0.5539\n",
      "Epoch [5/10], Batch [36/938], Loss: 0.5745\n",
      "Epoch [5/10], Batch [37/938], Loss: 0.6021\n",
      "Epoch [5/10], Batch [38/938], Loss: 0.6071\n",
      "Epoch [5/10], Batch [39/938], Loss: 0.6203\n",
      "Epoch [5/10], Batch [40/938], Loss: 0.5766\n",
      "Epoch [5/10], Batch [41/938], Loss: 0.5877\n",
      "Epoch [5/10], Batch [42/938], Loss: 0.6138\n",
      "Epoch [5/10], Batch [43/938], Loss: 0.5667\n",
      "Epoch [5/10], Batch [44/938], Loss: 0.6353\n",
      "Epoch [5/10], Batch [45/938], Loss: 0.5762\n",
      "Epoch [5/10], Batch [46/938], Loss: 0.6392\n",
      "Epoch [5/10], Batch [47/938], Loss: 0.5804\n",
      "Epoch [5/10], Batch [48/938], Loss: 0.5784\n",
      "Epoch [5/10], Batch [49/938], Loss: 0.6071\n",
      "Epoch [5/10], Batch [50/938], Loss: 0.6228\n",
      "Epoch [5/10], Batch [51/938], Loss: 0.5889\n",
      "Epoch [5/10], Batch [52/938], Loss: 0.5793\n",
      "Epoch [5/10], Batch [53/938], Loss: 0.5901\n",
      "Epoch [5/10], Batch [54/938], Loss: 0.6120\n",
      "Epoch [5/10], Batch [55/938], Loss: 0.5956\n",
      "Epoch [5/10], Batch [56/938], Loss: 0.5939\n",
      "Epoch [5/10], Batch [57/938], Loss: 0.5762\n",
      "Epoch [5/10], Batch [58/938], Loss: 0.5967\n",
      "Epoch [5/10], Batch [59/938], Loss: 0.5908\n",
      "Epoch [5/10], Batch [60/938], Loss: 0.5830\n",
      "Epoch [5/10], Batch [61/938], Loss: 0.5812\n",
      "Epoch [5/10], Batch [62/938], Loss: 0.5953\n",
      "Epoch [5/10], Batch [63/938], Loss: 0.5921\n",
      "Epoch [5/10], Batch [64/938], Loss: 0.5824\n",
      "Epoch [5/10], Batch [65/938], Loss: 0.6062\n",
      "Epoch [5/10], Batch [66/938], Loss: 0.5820\n",
      "Epoch [5/10], Batch [67/938], Loss: 0.5882\n",
      "Epoch [5/10], Batch [68/938], Loss: 0.5953\n",
      "Epoch [5/10], Batch [69/938], Loss: 0.5878\n",
      "Epoch [5/10], Batch [70/938], Loss: 0.6049\n",
      "Epoch [5/10], Batch [71/938], Loss: 0.6036\n",
      "Epoch [5/10], Batch [72/938], Loss: 0.5678\n",
      "Epoch [5/10], Batch [73/938], Loss: 0.5585\n",
      "Epoch [5/10], Batch [74/938], Loss: 0.5817\n",
      "Epoch [5/10], Batch [75/938], Loss: 0.6289\n",
      "Epoch [5/10], Batch [76/938], Loss: 0.5828\n",
      "Epoch [5/10], Batch [77/938], Loss: 0.5848\n",
      "Epoch [5/10], Batch [78/938], Loss: 0.6152\n",
      "Epoch [5/10], Batch [79/938], Loss: 0.5725\n",
      "Epoch [5/10], Batch [80/938], Loss: 0.5956\n",
      "Epoch [5/10], Batch [81/938], Loss: 0.6074\n",
      "Epoch [5/10], Batch [82/938], Loss: 0.6294\n",
      "Epoch [5/10], Batch [83/938], Loss: 0.5721\n",
      "Epoch [5/10], Batch [84/938], Loss: 0.5835\n",
      "Epoch [5/10], Batch [85/938], Loss: 0.6071\n",
      "Epoch [5/10], Batch [86/938], Loss: 0.5883\n",
      "Epoch [5/10], Batch [87/938], Loss: 0.5900\n",
      "Epoch [5/10], Batch [88/938], Loss: 0.5857\n",
      "Epoch [5/10], Batch [89/938], Loss: 0.5863\n",
      "Epoch [5/10], Batch [90/938], Loss: 0.6022\n",
      "Epoch [5/10], Batch [91/938], Loss: 0.5920\n",
      "Epoch [5/10], Batch [92/938], Loss: 0.5912\n",
      "Epoch [5/10], Batch [93/938], Loss: 0.5825\n",
      "Epoch [5/10], Batch [94/938], Loss: 0.6007\n",
      "Epoch [5/10], Batch [95/938], Loss: 0.5837\n",
      "Epoch [5/10], Batch [96/938], Loss: 0.5924\n",
      "Epoch [5/10], Batch [97/938], Loss: 0.5761\n",
      "Epoch [5/10], Batch [98/938], Loss: 0.5863\n",
      "Epoch [5/10], Batch [99/938], Loss: 0.5847\n",
      "Epoch [5/10], Batch [100/938], Loss: 0.5888\n",
      "Epoch [5/10], Batch [101/938], Loss: 0.5767\n",
      "Epoch [5/10], Batch [102/938], Loss: 0.5617\n",
      "Epoch [5/10], Batch [103/938], Loss: 0.6105\n",
      "Epoch [5/10], Batch [104/938], Loss: 0.5998\n",
      "Epoch [5/10], Batch [105/938], Loss: 0.5776\n",
      "Epoch [5/10], Batch [106/938], Loss: 0.6009\n",
      "Epoch [5/10], Batch [107/938], Loss: 0.5720\n",
      "Epoch [5/10], Batch [108/938], Loss: 0.5707\n",
      "Epoch [5/10], Batch [109/938], Loss: 0.6072\n",
      "Epoch [5/10], Batch [110/938], Loss: 0.5931\n",
      "Epoch [5/10], Batch [111/938], Loss: 0.6243\n",
      "Epoch [5/10], Batch [112/938], Loss: 0.5840\n",
      "Epoch [5/10], Batch [113/938], Loss: 0.5907\n",
      "Epoch [5/10], Batch [114/938], Loss: 0.5837\n",
      "Epoch [5/10], Batch [115/938], Loss: 0.6159\n",
      "Epoch [5/10], Batch [116/938], Loss: 0.5857\n",
      "Epoch [5/10], Batch [117/938], Loss: 0.6132\n",
      "Epoch [5/10], Batch [118/938], Loss: 0.5786\n",
      "Epoch [5/10], Batch [119/938], Loss: 0.5822\n",
      "Epoch [5/10], Batch [120/938], Loss: 0.5959\n",
      "Epoch [5/10], Batch [121/938], Loss: 0.5851\n",
      "Epoch [5/10], Batch [122/938], Loss: 0.6044\n",
      "Epoch [5/10], Batch [123/938], Loss: 0.6148\n",
      "Epoch [5/10], Batch [124/938], Loss: 0.6200\n",
      "Epoch [5/10], Batch [125/938], Loss: 0.5801\n",
      "Epoch [5/10], Batch [126/938], Loss: 0.5833\n",
      "Epoch [5/10], Batch [127/938], Loss: 0.6115\n",
      "Epoch [5/10], Batch [128/938], Loss: 0.6301\n",
      "Epoch [5/10], Batch [129/938], Loss: 0.5836\n",
      "Epoch [5/10], Batch [130/938], Loss: 0.6001\n",
      "Epoch [5/10], Batch [131/938], Loss: 0.5908\n",
      "Epoch [5/10], Batch [132/938], Loss: 0.5816\n",
      "Epoch [5/10], Batch [133/938], Loss: 0.5936\n",
      "Epoch [5/10], Batch [134/938], Loss: 0.5961\n",
      "Epoch [5/10], Batch [135/938], Loss: 0.6034\n",
      "Epoch [5/10], Batch [136/938], Loss: 0.5862\n",
      "Epoch [5/10], Batch [137/938], Loss: 0.5865\n",
      "Epoch [5/10], Batch [138/938], Loss: 0.5892\n",
      "Epoch [5/10], Batch [139/938], Loss: 0.5802\n",
      "Epoch [5/10], Batch [140/938], Loss: 0.5791\n",
      "Epoch [5/10], Batch [141/938], Loss: 0.5801\n",
      "Epoch [5/10], Batch [142/938], Loss: 0.5799\n",
      "Epoch [5/10], Batch [143/938], Loss: 0.6093\n",
      "Epoch [5/10], Batch [144/938], Loss: 0.5883\n",
      "Epoch [5/10], Batch [145/938], Loss: 0.5808\n",
      "Epoch [5/10], Batch [146/938], Loss: 0.5793\n",
      "Epoch [5/10], Batch [147/938], Loss: 0.6163\n",
      "Epoch [5/10], Batch [148/938], Loss: 0.5666\n",
      "Epoch [5/10], Batch [149/938], Loss: 0.6019\n",
      "Epoch [5/10], Batch [150/938], Loss: 0.5926\n",
      "Epoch [5/10], Batch [151/938], Loss: 0.6048\n",
      "Epoch [5/10], Batch [152/938], Loss: 0.5919\n",
      "Epoch [5/10], Batch [153/938], Loss: 0.5678\n",
      "Epoch [5/10], Batch [154/938], Loss: 0.5718\n",
      "Epoch [5/10], Batch [155/938], Loss: 0.6038\n",
      "Epoch [5/10], Batch [156/938], Loss: 0.5942\n",
      "Epoch [5/10], Batch [157/938], Loss: 0.5751\n",
      "Epoch [5/10], Batch [158/938], Loss: 0.6043\n",
      "Epoch [5/10], Batch [159/938], Loss: 0.5888\n",
      "Epoch [5/10], Batch [160/938], Loss: 0.6334\n",
      "Epoch [5/10], Batch [161/938], Loss: 0.5863\n",
      "Epoch [5/10], Batch [162/938], Loss: 0.6077\n",
      "Epoch [5/10], Batch [163/938], Loss: 0.5740\n",
      "Epoch [5/10], Batch [164/938], Loss: 0.6015\n",
      "Epoch [5/10], Batch [165/938], Loss: 0.5893\n",
      "Epoch [5/10], Batch [166/938], Loss: 0.5853\n",
      "Epoch [5/10], Batch [167/938], Loss: 0.5926\n",
      "Epoch [5/10], Batch [168/938], Loss: 0.5736\n",
      "Epoch [5/10], Batch [169/938], Loss: 0.5785\n",
      "Epoch [5/10], Batch [170/938], Loss: 0.5739\n",
      "Epoch [5/10], Batch [171/938], Loss: 0.6053\n",
      "Epoch [5/10], Batch [172/938], Loss: 0.5930\n",
      "Epoch [5/10], Batch [173/938], Loss: 0.5779\n",
      "Epoch [5/10], Batch [174/938], Loss: 0.5776\n",
      "Epoch [5/10], Batch [175/938], Loss: 0.6229\n",
      "Epoch [5/10], Batch [176/938], Loss: 0.5759\n",
      "Epoch [5/10], Batch [177/938], Loss: 0.5681\n",
      "Epoch [5/10], Batch [178/938], Loss: 0.5920\n",
      "Epoch [5/10], Batch [179/938], Loss: 0.6101\n",
      "Epoch [5/10], Batch [180/938], Loss: 0.5891\n",
      "Epoch [5/10], Batch [181/938], Loss: 0.5881\n",
      "Epoch [5/10], Batch [182/938], Loss: 0.5910\n",
      "Epoch [5/10], Batch [183/938], Loss: 0.5689\n",
      "Epoch [5/10], Batch [184/938], Loss: 0.5946\n",
      "Epoch [5/10], Batch [185/938], Loss: 0.5849\n",
      "Epoch [5/10], Batch [186/938], Loss: 0.5740\n",
      "Epoch [5/10], Batch [187/938], Loss: 0.5751\n",
      "Epoch [5/10], Batch [188/938], Loss: 0.5931\n",
      "Epoch [5/10], Batch [189/938], Loss: 0.5863\n",
      "Epoch [5/10], Batch [190/938], Loss: 0.5701\n",
      "Epoch [5/10], Batch [191/938], Loss: 0.5980\n",
      "Epoch [5/10], Batch [192/938], Loss: 0.5911\n",
      "Epoch [5/10], Batch [193/938], Loss: 0.5747\n",
      "Epoch [5/10], Batch [194/938], Loss: 0.5902\n",
      "Epoch [5/10], Batch [195/938], Loss: 0.6167\n",
      "Epoch [5/10], Batch [196/938], Loss: 0.5753\n",
      "Epoch [5/10], Batch [197/938], Loss: 0.6139\n",
      "Epoch [5/10], Batch [198/938], Loss: 0.5748\n",
      "Epoch [5/10], Batch [199/938], Loss: 0.6174\n",
      "Epoch [5/10], Batch [200/938], Loss: 0.5933\n",
      "Epoch [5/10], Batch [201/938], Loss: 0.6030\n",
      "Epoch [5/10], Batch [202/938], Loss: 0.6140\n",
      "Epoch [5/10], Batch [203/938], Loss: 0.5812\n",
      "Epoch [5/10], Batch [204/938], Loss: 0.5722\n",
      "Epoch [5/10], Batch [205/938], Loss: 0.5776\n",
      "Epoch [5/10], Batch [206/938], Loss: 0.5748\n",
      "Epoch [5/10], Batch [207/938], Loss: 0.6046\n",
      "Epoch [5/10], Batch [208/938], Loss: 0.5656\n",
      "Epoch [5/10], Batch [209/938], Loss: 0.6142\n",
      "Epoch [5/10], Batch [210/938], Loss: 0.6136\n",
      "Epoch [5/10], Batch [211/938], Loss: 0.6100\n",
      "Epoch [5/10], Batch [212/938], Loss: 0.6049\n",
      "Epoch [5/10], Batch [213/938], Loss: 0.5929\n",
      "Epoch [5/10], Batch [214/938], Loss: 0.5925\n",
      "Epoch [5/10], Batch [215/938], Loss: 0.6123\n",
      "Epoch [5/10], Batch [216/938], Loss: 0.5824\n",
      "Epoch [5/10], Batch [217/938], Loss: 0.6022\n",
      "Epoch [5/10], Batch [218/938], Loss: 0.6198\n",
      "Epoch [5/10], Batch [219/938], Loss: 0.5918\n",
      "Epoch [5/10], Batch [220/938], Loss: 0.5888\n",
      "Epoch [5/10], Batch [221/938], Loss: 0.5760\n",
      "Epoch [5/10], Batch [222/938], Loss: 0.5951\n",
      "Epoch [5/10], Batch [223/938], Loss: 0.5998\n",
      "Epoch [5/10], Batch [224/938], Loss: 0.5991\n",
      "Epoch [5/10], Batch [225/938], Loss: 0.5858\n",
      "Epoch [5/10], Batch [226/938], Loss: 0.6132\n",
      "Epoch [5/10], Batch [227/938], Loss: 0.5787\n",
      "Epoch [5/10], Batch [228/938], Loss: 0.5977\n",
      "Epoch [5/10], Batch [229/938], Loss: 0.5682\n",
      "Epoch [5/10], Batch [230/938], Loss: 0.5973\n",
      "Epoch [5/10], Batch [231/938], Loss: 0.5979\n",
      "Epoch [5/10], Batch [232/938], Loss: 0.6310\n",
      "Epoch [5/10], Batch [233/938], Loss: 0.6036\n",
      "Epoch [5/10], Batch [234/938], Loss: 0.6037\n",
      "Epoch [5/10], Batch [235/938], Loss: 0.5731\n",
      "Epoch [5/10], Batch [236/938], Loss: 0.5858\n",
      "Epoch [5/10], Batch [237/938], Loss: 0.5948\n",
      "Epoch [5/10], Batch [238/938], Loss: 0.5939\n",
      "Epoch [5/10], Batch [239/938], Loss: 0.5920\n",
      "Epoch [5/10], Batch [240/938], Loss: 0.5919\n",
      "Epoch [5/10], Batch [241/938], Loss: 0.5869\n",
      "Epoch [5/10], Batch [242/938], Loss: 0.5753\n",
      "Epoch [5/10], Batch [243/938], Loss: 0.5969\n",
      "Epoch [5/10], Batch [244/938], Loss: 0.5875\n",
      "Epoch [5/10], Batch [245/938], Loss: 0.5678\n",
      "Epoch [5/10], Batch [246/938], Loss: 0.5896\n",
      "Epoch [5/10], Batch [247/938], Loss: 0.5914\n",
      "Epoch [5/10], Batch [248/938], Loss: 0.5959\n",
      "Epoch [5/10], Batch [249/938], Loss: 0.6002\n",
      "Epoch [5/10], Batch [250/938], Loss: 0.5814\n",
      "Epoch [5/10], Batch [251/938], Loss: 0.5830\n",
      "Epoch [5/10], Batch [252/938], Loss: 0.5990\n",
      "Epoch [5/10], Batch [253/938], Loss: 0.5801\n",
      "Epoch [5/10], Batch [254/938], Loss: 0.6096\n",
      "Epoch [5/10], Batch [255/938], Loss: 0.5707\n",
      "Epoch [5/10], Batch [256/938], Loss: 0.5918\n",
      "Epoch [5/10], Batch [257/938], Loss: 0.5747\n",
      "Epoch [5/10], Batch [258/938], Loss: 0.6076\n",
      "Epoch [5/10], Batch [259/938], Loss: 0.5919\n",
      "Epoch [5/10], Batch [260/938], Loss: 0.6165\n",
      "Epoch [5/10], Batch [261/938], Loss: 0.5775\n",
      "Epoch [5/10], Batch [262/938], Loss: 0.5919\n",
      "Epoch [5/10], Batch [263/938], Loss: 0.6425\n",
      "Epoch [5/10], Batch [264/938], Loss: 0.6073\n",
      "Epoch [5/10], Batch [265/938], Loss: 0.5727\n",
      "Epoch [5/10], Batch [266/938], Loss: 0.5959\n",
      "Epoch [5/10], Batch [267/938], Loss: 0.5979\n",
      "Epoch [5/10], Batch [268/938], Loss: 0.5605\n",
      "Epoch [5/10], Batch [269/938], Loss: 0.5667\n",
      "Epoch [5/10], Batch [270/938], Loss: 0.6242\n",
      "Epoch [5/10], Batch [271/938], Loss: 0.5904\n",
      "Epoch [5/10], Batch [272/938], Loss: 0.5861\n",
      "Epoch [5/10], Batch [273/938], Loss: 0.5840\n",
      "Epoch [5/10], Batch [274/938], Loss: 0.5965\n",
      "Epoch [5/10], Batch [275/938], Loss: 0.6199\n",
      "Epoch [5/10], Batch [276/938], Loss: 0.6296\n",
      "Epoch [5/10], Batch [277/938], Loss: 0.6040\n",
      "Epoch [5/10], Batch [278/938], Loss: 0.6003\n",
      "Epoch [5/10], Batch [279/938], Loss: 0.5857\n",
      "Epoch [5/10], Batch [280/938], Loss: 0.5694\n",
      "Epoch [5/10], Batch [281/938], Loss: 0.5887\n",
      "Epoch [5/10], Batch [282/938], Loss: 0.5873\n",
      "Epoch [5/10], Batch [283/938], Loss: 0.5841\n",
      "Epoch [5/10], Batch [284/938], Loss: 0.5927\n",
      "Epoch [5/10], Batch [285/938], Loss: 0.5716\n",
      "Epoch [5/10], Batch [286/938], Loss: 0.6231\n",
      "Epoch [5/10], Batch [287/938], Loss: 0.6137\n",
      "Epoch [5/10], Batch [288/938], Loss: 0.6196\n",
      "Epoch [5/10], Batch [289/938], Loss: 0.5993\n",
      "Epoch [5/10], Batch [290/938], Loss: 0.6168\n",
      "Epoch [5/10], Batch [291/938], Loss: 0.5750\n",
      "Epoch [5/10], Batch [292/938], Loss: 0.5716\n",
      "Epoch [5/10], Batch [293/938], Loss: 0.5864\n",
      "Epoch [5/10], Batch [294/938], Loss: 0.5908\n",
      "Epoch [5/10], Batch [295/938], Loss: 0.6154\n",
      "Epoch [5/10], Batch [296/938], Loss: 0.6269\n",
      "Epoch [5/10], Batch [297/938], Loss: 0.5803\n",
      "Epoch [5/10], Batch [298/938], Loss: 0.5836\n",
      "Epoch [5/10], Batch [299/938], Loss: 0.5708\n",
      "Epoch [5/10], Batch [300/938], Loss: 0.5643\n",
      "Epoch [5/10], Batch [301/938], Loss: 0.5958\n",
      "Epoch [5/10], Batch [302/938], Loss: 0.5953\n",
      "Epoch [5/10], Batch [303/938], Loss: 0.5949\n",
      "Epoch [5/10], Batch [304/938], Loss: 0.5850\n",
      "Epoch [5/10], Batch [305/938], Loss: 0.5829\n",
      "Epoch [5/10], Batch [306/938], Loss: 0.5770\n",
      "Epoch [5/10], Batch [307/938], Loss: 0.5826\n",
      "Epoch [5/10], Batch [308/938], Loss: 0.5882\n",
      "Epoch [5/10], Batch [309/938], Loss: 0.6125\n",
      "Epoch [5/10], Batch [310/938], Loss: 0.5974\n",
      "Epoch [5/10], Batch [311/938], Loss: 0.6081\n",
      "Epoch [5/10], Batch [312/938], Loss: 0.5964\n",
      "Epoch [5/10], Batch [313/938], Loss: 0.5897\n",
      "Epoch [5/10], Batch [314/938], Loss: 0.5908\n",
      "Epoch [5/10], Batch [315/938], Loss: 0.6089\n",
      "Epoch [5/10], Batch [316/938], Loss: 0.6109\n",
      "Epoch [5/10], Batch [317/938], Loss: 0.5786\n",
      "Epoch [5/10], Batch [318/938], Loss: 0.5983\n",
      "Epoch [5/10], Batch [319/938], Loss: 0.6046\n",
      "Epoch [5/10], Batch [320/938], Loss: 0.6239\n",
      "Epoch [5/10], Batch [321/938], Loss: 0.6009\n",
      "Epoch [5/10], Batch [322/938], Loss: 0.6036\n",
      "Epoch [5/10], Batch [323/938], Loss: 0.5743\n",
      "Epoch [5/10], Batch [324/938], Loss: 0.6105\n",
      "Epoch [5/10], Batch [325/938], Loss: 0.6101\n",
      "Epoch [5/10], Batch [326/938], Loss: 0.5846\n",
      "Epoch [5/10], Batch [327/938], Loss: 0.6141\n",
      "Epoch [5/10], Batch [328/938], Loss: 0.5851\n",
      "Epoch [5/10], Batch [329/938], Loss: 0.6025\n",
      "Epoch [5/10], Batch [330/938], Loss: 0.6099\n",
      "Epoch [5/10], Batch [331/938], Loss: 0.5826\n",
      "Epoch [5/10], Batch [332/938], Loss: 0.6002\n",
      "Epoch [5/10], Batch [333/938], Loss: 0.6059\n",
      "Epoch [5/10], Batch [334/938], Loss: 0.5459\n",
      "Epoch [5/10], Batch [335/938], Loss: 0.6311\n",
      "Epoch [5/10], Batch [336/938], Loss: 0.6176\n",
      "Epoch [5/10], Batch [337/938], Loss: 0.5878\n",
      "Epoch [5/10], Batch [338/938], Loss: 0.6048\n",
      "Epoch [5/10], Batch [339/938], Loss: 0.5928\n",
      "Epoch [5/10], Batch [340/938], Loss: 0.5796\n",
      "Epoch [5/10], Batch [341/938], Loss: 0.6074\n",
      "Epoch [5/10], Batch [342/938], Loss: 0.6222\n",
      "Epoch [5/10], Batch [343/938], Loss: 0.6084\n",
      "Epoch [5/10], Batch [344/938], Loss: 0.5990\n",
      "Epoch [5/10], Batch [345/938], Loss: 0.6077\n",
      "Epoch [5/10], Batch [346/938], Loss: 0.5844\n",
      "Epoch [5/10], Batch [347/938], Loss: 0.6106\n",
      "Epoch [5/10], Batch [348/938], Loss: 0.6093\n",
      "Epoch [5/10], Batch [349/938], Loss: 0.5910\n",
      "Epoch [5/10], Batch [350/938], Loss: 0.5775\n",
      "Epoch [5/10], Batch [351/938], Loss: 0.5880\n",
      "Epoch [5/10], Batch [352/938], Loss: 0.6222\n",
      "Epoch [5/10], Batch [353/938], Loss: 0.5849\n",
      "Epoch [5/10], Batch [354/938], Loss: 0.5715\n",
      "Epoch [5/10], Batch [355/938], Loss: 0.5799\n",
      "Epoch [5/10], Batch [356/938], Loss: 0.6109\n",
      "Epoch [5/10], Batch [357/938], Loss: 0.5777\n",
      "Epoch [5/10], Batch [358/938], Loss: 0.5774\n",
      "Epoch [5/10], Batch [359/938], Loss: 0.5853\n",
      "Epoch [5/10], Batch [360/938], Loss: 0.5722\n",
      "Epoch [5/10], Batch [361/938], Loss: 0.5892\n",
      "Epoch [5/10], Batch [362/938], Loss: 0.6299\n",
      "Epoch [5/10], Batch [363/938], Loss: 0.5957\n",
      "Epoch [5/10], Batch [364/938], Loss: 0.6012\n",
      "Epoch [5/10], Batch [365/938], Loss: 0.5818\n",
      "Epoch [5/10], Batch [366/938], Loss: 0.6091\n",
      "Epoch [5/10], Batch [367/938], Loss: 0.5826\n",
      "Epoch [5/10], Batch [368/938], Loss: 0.5873\n",
      "Epoch [5/10], Batch [369/938], Loss: 0.5789\n",
      "Epoch [5/10], Batch [370/938], Loss: 0.5876\n",
      "Epoch [5/10], Batch [371/938], Loss: 0.6084\n",
      "Epoch [5/10], Batch [372/938], Loss: 0.5967\n",
      "Epoch [5/10], Batch [373/938], Loss: 0.5833\n",
      "Epoch [5/10], Batch [374/938], Loss: 0.6243\n",
      "Epoch [5/10], Batch [375/938], Loss: 0.6087\n",
      "Epoch [5/10], Batch [376/938], Loss: 0.5853\n",
      "Epoch [5/10], Batch [377/938], Loss: 0.6089\n",
      "Epoch [5/10], Batch [378/938], Loss: 0.6107\n",
      "Epoch [5/10], Batch [379/938], Loss: 0.6147\n",
      "Epoch [5/10], Batch [380/938], Loss: 0.6000\n",
      "Epoch [5/10], Batch [381/938], Loss: 0.6118\n",
      "Epoch [5/10], Batch [382/938], Loss: 0.6066\n",
      "Epoch [5/10], Batch [383/938], Loss: 0.6038\n",
      "Epoch [5/10], Batch [384/938], Loss: 0.5818\n",
      "Epoch [5/10], Batch [385/938], Loss: 0.5851\n",
      "Epoch [5/10], Batch [386/938], Loss: 0.5728\n",
      "Epoch [5/10], Batch [387/938], Loss: 0.6068\n",
      "Epoch [5/10], Batch [388/938], Loss: 0.5767\n",
      "Epoch [5/10], Batch [389/938], Loss: 0.5703\n",
      "Epoch [5/10], Batch [390/938], Loss: 0.5953\n",
      "Epoch [5/10], Batch [391/938], Loss: 0.5968\n",
      "Epoch [5/10], Batch [392/938], Loss: 0.5904\n",
      "Epoch [5/10], Batch [393/938], Loss: 0.5697\n",
      "Epoch [5/10], Batch [394/938], Loss: 0.6152\n",
      "Epoch [5/10], Batch [395/938], Loss: 0.5888\n",
      "Epoch [5/10], Batch [396/938], Loss: 0.5963\n",
      "Epoch [5/10], Batch [397/938], Loss: 0.5934\n",
      "Epoch [5/10], Batch [398/938], Loss: 0.5968\n",
      "Epoch [5/10], Batch [399/938], Loss: 0.5819\n",
      "Epoch [5/10], Batch [400/938], Loss: 0.5947\n",
      "Epoch [5/10], Batch [401/938], Loss: 0.6263\n",
      "Epoch [5/10], Batch [402/938], Loss: 0.5813\n",
      "Epoch [5/10], Batch [403/938], Loss: 0.6142\n",
      "Epoch [5/10], Batch [404/938], Loss: 0.5902\n",
      "Epoch [5/10], Batch [405/938], Loss: 0.5911\n",
      "Epoch [5/10], Batch [406/938], Loss: 0.5774\n",
      "Epoch [5/10], Batch [407/938], Loss: 0.6014\n",
      "Epoch [5/10], Batch [408/938], Loss: 0.5935\n",
      "Epoch [5/10], Batch [409/938], Loss: 0.5761\n",
      "Epoch [5/10], Batch [410/938], Loss: 0.5854\n",
      "Epoch [5/10], Batch [411/938], Loss: 0.5952\n",
      "Epoch [5/10], Batch [412/938], Loss: 0.5851\n",
      "Epoch [5/10], Batch [413/938], Loss: 0.5985\n",
      "Epoch [5/10], Batch [414/938], Loss: 0.5991\n",
      "Epoch [5/10], Batch [415/938], Loss: 0.5887\n",
      "Epoch [5/10], Batch [416/938], Loss: 0.6167\n",
      "Epoch [5/10], Batch [417/938], Loss: 0.5796\n",
      "Epoch [5/10], Batch [418/938], Loss: 0.5742\n",
      "Epoch [5/10], Batch [419/938], Loss: 0.5950\n",
      "Epoch [5/10], Batch [420/938], Loss: 0.5777\n",
      "Epoch [5/10], Batch [421/938], Loss: 0.6076\n",
      "Epoch [5/10], Batch [422/938], Loss: 0.5968\n",
      "Epoch [5/10], Batch [423/938], Loss: 0.6245\n",
      "Epoch [5/10], Batch [424/938], Loss: 0.5961\n",
      "Epoch [5/10], Batch [425/938], Loss: 0.6016\n",
      "Epoch [5/10], Batch [426/938], Loss: 0.6000\n",
      "Epoch [5/10], Batch [427/938], Loss: 0.5900\n",
      "Epoch [5/10], Batch [428/938], Loss: 0.5623\n",
      "Epoch [5/10], Batch [429/938], Loss: 0.6044\n",
      "Epoch [5/10], Batch [430/938], Loss: 0.6334\n",
      "Epoch [5/10], Batch [431/938], Loss: 0.5943\n",
      "Epoch [5/10], Batch [432/938], Loss: 0.6282\n",
      "Epoch [5/10], Batch [433/938], Loss: 0.5889\n",
      "Epoch [5/10], Batch [434/938], Loss: 0.5646\n",
      "Epoch [5/10], Batch [435/938], Loss: 0.5771\n",
      "Epoch [5/10], Batch [436/938], Loss: 0.5909\n",
      "Epoch [5/10], Batch [437/938], Loss: 0.5967\n",
      "Epoch [5/10], Batch [438/938], Loss: 0.5828\n",
      "Epoch [5/10], Batch [439/938], Loss: 0.6083\n",
      "Epoch [5/10], Batch [440/938], Loss: 0.5834\n",
      "Epoch [5/10], Batch [441/938], Loss: 0.6251\n",
      "Epoch [5/10], Batch [442/938], Loss: 0.6432\n",
      "Epoch [5/10], Batch [443/938], Loss: 0.6057\n",
      "Epoch [5/10], Batch [444/938], Loss: 0.5865\n",
      "Epoch [5/10], Batch [445/938], Loss: 0.5804\n",
      "Epoch [5/10], Batch [446/938], Loss: 0.5992\n",
      "Epoch [5/10], Batch [447/938], Loss: 0.5809\n",
      "Epoch [5/10], Batch [448/938], Loss: 0.5863\n",
      "Epoch [5/10], Batch [449/938], Loss: 0.6021\n",
      "Epoch [5/10], Batch [450/938], Loss: 0.6202\n",
      "Epoch [5/10], Batch [451/938], Loss: 0.5862\n",
      "Epoch [5/10], Batch [452/938], Loss: 0.5966\n",
      "Epoch [5/10], Batch [453/938], Loss: 0.5766\n",
      "Epoch [5/10], Batch [454/938], Loss: 0.5877\n",
      "Epoch [5/10], Batch [455/938], Loss: 0.6314\n",
      "Epoch [5/10], Batch [456/938], Loss: 0.5942\n",
      "Epoch [5/10], Batch [457/938], Loss: 0.5861\n",
      "Epoch [5/10], Batch [458/938], Loss: 0.5786\n",
      "Epoch [5/10], Batch [459/938], Loss: 0.5821\n",
      "Epoch [5/10], Batch [460/938], Loss: 0.5944\n",
      "Epoch [5/10], Batch [461/938], Loss: 0.5742\n",
      "Epoch [5/10], Batch [462/938], Loss: 0.5708\n",
      "Epoch [5/10], Batch [463/938], Loss: 0.6189\n",
      "Epoch [5/10], Batch [464/938], Loss: 0.5990\n",
      "Epoch [5/10], Batch [465/938], Loss: 0.5774\n",
      "Epoch [5/10], Batch [466/938], Loss: 0.5858\n",
      "Epoch [5/10], Batch [467/938], Loss: 0.5977\n",
      "Epoch [5/10], Batch [468/938], Loss: 0.5750\n",
      "Epoch [5/10], Batch [469/938], Loss: 0.5804\n",
      "Epoch [5/10], Batch [470/938], Loss: 0.5822\n",
      "Epoch [5/10], Batch [471/938], Loss: 0.5924\n",
      "Epoch [5/10], Batch [472/938], Loss: 0.6250\n",
      "Epoch [5/10], Batch [473/938], Loss: 0.6074\n",
      "Epoch [5/10], Batch [474/938], Loss: 0.5734\n",
      "Epoch [5/10], Batch [475/938], Loss: 0.6156\n",
      "Epoch [5/10], Batch [476/938], Loss: 0.5842\n",
      "Epoch [5/10], Batch [477/938], Loss: 0.5852\n",
      "Epoch [5/10], Batch [478/938], Loss: 0.5788\n",
      "Epoch [5/10], Batch [479/938], Loss: 0.6122\n",
      "Epoch [5/10], Batch [480/938], Loss: 0.5833\n",
      "Epoch [5/10], Batch [481/938], Loss: 0.5833\n",
      "Epoch [5/10], Batch [482/938], Loss: 0.5948\n",
      "Epoch [5/10], Batch [483/938], Loss: 0.6020\n",
      "Epoch [5/10], Batch [484/938], Loss: 0.6135\n",
      "Epoch [5/10], Batch [485/938], Loss: 0.5881\n",
      "Epoch [5/10], Batch [486/938], Loss: 0.6387\n",
      "Epoch [5/10], Batch [487/938], Loss: 0.5795\n",
      "Epoch [5/10], Batch [488/938], Loss: 0.5646\n",
      "Epoch [5/10], Batch [489/938], Loss: 0.6074\n",
      "Epoch [5/10], Batch [490/938], Loss: 0.5811\n",
      "Epoch [5/10], Batch [491/938], Loss: 0.6039\n",
      "Epoch [5/10], Batch [492/938], Loss: 0.6150\n",
      "Epoch [5/10], Batch [493/938], Loss: 0.5849\n",
      "Epoch [5/10], Batch [494/938], Loss: 0.5844\n",
      "Epoch [5/10], Batch [495/938], Loss: 0.5871\n",
      "Epoch [5/10], Batch [496/938], Loss: 0.6012\n",
      "Epoch [5/10], Batch [497/938], Loss: 0.5652\n",
      "Epoch [5/10], Batch [498/938], Loss: 0.6030\n",
      "Epoch [5/10], Batch [499/938], Loss: 0.5945\n",
      "Epoch [5/10], Batch [500/938], Loss: 0.5673\n",
      "Epoch [5/10], Batch [501/938], Loss: 0.5721\n",
      "Epoch [5/10], Batch [502/938], Loss: 0.6165\n",
      "Epoch [5/10], Batch [503/938], Loss: 0.5696\n",
      "Epoch [5/10], Batch [504/938], Loss: 0.5896\n",
      "Epoch [5/10], Batch [505/938], Loss: 0.6234\n",
      "Epoch [5/10], Batch [506/938], Loss: 0.6017\n",
      "Epoch [5/10], Batch [507/938], Loss: 0.6251\n",
      "Epoch [5/10], Batch [508/938], Loss: 0.5966\n",
      "Epoch [5/10], Batch [509/938], Loss: 0.5596\n",
      "Epoch [5/10], Batch [510/938], Loss: 0.5872\n",
      "Epoch [5/10], Batch [511/938], Loss: 0.6342\n",
      "Epoch [5/10], Batch [512/938], Loss: 0.6136\n",
      "Epoch [5/10], Batch [513/938], Loss: 0.6007\n",
      "Epoch [5/10], Batch [514/938], Loss: 0.6040\n",
      "Epoch [5/10], Batch [515/938], Loss: 0.5757\n",
      "Epoch [5/10], Batch [516/938], Loss: 0.5954\n",
      "Epoch [5/10], Batch [517/938], Loss: 0.6051\n",
      "Epoch [5/10], Batch [518/938], Loss: 0.5782\n",
      "Epoch [5/10], Batch [519/938], Loss: 0.5797\n",
      "Epoch [5/10], Batch [520/938], Loss: 0.6191\n",
      "Epoch [5/10], Batch [521/938], Loss: 0.6122\n",
      "Epoch [5/10], Batch [522/938], Loss: 0.5995\n",
      "Epoch [5/10], Batch [523/938], Loss: 0.5817\n",
      "Epoch [5/10], Batch [524/938], Loss: 0.6142\n",
      "Epoch [5/10], Batch [525/938], Loss: 0.5769\n",
      "Epoch [5/10], Batch [526/938], Loss: 0.6126\n",
      "Epoch [5/10], Batch [527/938], Loss: 0.5935\n",
      "Epoch [5/10], Batch [528/938], Loss: 0.5778\n",
      "Epoch [5/10], Batch [529/938], Loss: 0.5925\n",
      "Epoch [5/10], Batch [530/938], Loss: 0.6049\n",
      "Epoch [5/10], Batch [531/938], Loss: 0.6006\n",
      "Epoch [5/10], Batch [532/938], Loss: 0.5796\n",
      "Epoch [5/10], Batch [533/938], Loss: 0.5981\n",
      "Epoch [5/10], Batch [534/938], Loss: 0.6190\n",
      "Epoch [5/10], Batch [535/938], Loss: 0.5911\n",
      "Epoch [5/10], Batch [536/938], Loss: 0.5729\n",
      "Epoch [5/10], Batch [537/938], Loss: 0.5776\n",
      "Epoch [5/10], Batch [538/938], Loss: 0.6038\n",
      "Epoch [5/10], Batch [539/938], Loss: 0.5887\n",
      "Epoch [5/10], Batch [540/938], Loss: 0.5913\n",
      "Epoch [5/10], Batch [541/938], Loss: 0.5816\n",
      "Epoch [5/10], Batch [542/938], Loss: 0.5840\n",
      "Epoch [5/10], Batch [543/938], Loss: 0.5867\n",
      "Epoch [5/10], Batch [544/938], Loss: 0.5853\n",
      "Epoch [5/10], Batch [545/938], Loss: 0.5778\n",
      "Epoch [5/10], Batch [546/938], Loss: 0.5682\n",
      "Epoch [5/10], Batch [547/938], Loss: 0.6329\n",
      "Epoch [5/10], Batch [548/938], Loss: 0.6024\n",
      "Epoch [5/10], Batch [549/938], Loss: 0.5956\n",
      "Epoch [5/10], Batch [550/938], Loss: 0.6114\n",
      "Epoch [5/10], Batch [551/938], Loss: 0.6125\n",
      "Epoch [5/10], Batch [552/938], Loss: 0.5758\n",
      "Epoch [5/10], Batch [553/938], Loss: 0.5944\n",
      "Epoch [5/10], Batch [554/938], Loss: 0.5987\n",
      "Epoch [5/10], Batch [555/938], Loss: 0.5822\n",
      "Epoch [5/10], Batch [556/938], Loss: 0.5962\n",
      "Epoch [5/10], Batch [557/938], Loss: 0.6044\n",
      "Epoch [5/10], Batch [558/938], Loss: 0.6166\n",
      "Epoch [5/10], Batch [559/938], Loss: 0.5930\n",
      "Epoch [5/10], Batch [560/938], Loss: 0.5994\n",
      "Epoch [5/10], Batch [561/938], Loss: 0.6041\n",
      "Epoch [5/10], Batch [562/938], Loss: 0.6143\n",
      "Epoch [5/10], Batch [563/938], Loss: 0.5754\n",
      "Epoch [5/10], Batch [564/938], Loss: 0.6238\n",
      "Epoch [5/10], Batch [565/938], Loss: 0.6218\n",
      "Epoch [5/10], Batch [566/938], Loss: 0.5893\n",
      "Epoch [5/10], Batch [567/938], Loss: 0.5959\n",
      "Epoch [5/10], Batch [568/938], Loss: 0.6221\n",
      "Epoch [5/10], Batch [569/938], Loss: 0.5870\n",
      "Epoch [5/10], Batch [570/938], Loss: 0.6025\n",
      "Epoch [5/10], Batch [571/938], Loss: 0.5827\n",
      "Epoch [5/10], Batch [572/938], Loss: 0.5829\n",
      "Epoch [5/10], Batch [573/938], Loss: 0.6029\n",
      "Epoch [5/10], Batch [574/938], Loss: 0.6132\n",
      "Epoch [5/10], Batch [575/938], Loss: 0.5664\n",
      "Epoch [5/10], Batch [576/938], Loss: 0.6045\n",
      "Epoch [5/10], Batch [577/938], Loss: 0.5630\n",
      "Epoch [5/10], Batch [578/938], Loss: 0.5902\n",
      "Epoch [5/10], Batch [579/938], Loss: 0.5750\n",
      "Epoch [5/10], Batch [580/938], Loss: 0.6017\n",
      "Epoch [5/10], Batch [581/938], Loss: 0.5962\n",
      "Epoch [5/10], Batch [582/938], Loss: 0.5812\n",
      "Epoch [5/10], Batch [583/938], Loss: 0.5871\n",
      "Epoch [5/10], Batch [584/938], Loss: 0.5838\n",
      "Epoch [5/10], Batch [585/938], Loss: 0.5964\n",
      "Epoch [5/10], Batch [586/938], Loss: 0.5951\n",
      "Epoch [5/10], Batch [587/938], Loss: 0.6182\n",
      "Epoch [5/10], Batch [588/938], Loss: 0.6107\n",
      "Epoch [5/10], Batch [589/938], Loss: 0.5757\n",
      "Epoch [5/10], Batch [590/938], Loss: 0.5914\n",
      "Epoch [5/10], Batch [591/938], Loss: 0.5882\n",
      "Epoch [5/10], Batch [592/938], Loss: 0.5965\n",
      "Epoch [5/10], Batch [593/938], Loss: 0.5788\n",
      "Epoch [5/10], Batch [594/938], Loss: 0.5875\n",
      "Epoch [5/10], Batch [595/938], Loss: 0.6217\n",
      "Epoch [5/10], Batch [596/938], Loss: 0.5743\n",
      "Epoch [5/10], Batch [597/938], Loss: 0.5832\n",
      "Epoch [5/10], Batch [598/938], Loss: 0.5938\n",
      "Epoch [5/10], Batch [599/938], Loss: 0.5997\n",
      "Epoch [5/10], Batch [600/938], Loss: 0.5876\n",
      "Epoch [5/10], Batch [601/938], Loss: 0.5694\n",
      "Epoch [5/10], Batch [602/938], Loss: 0.6045\n",
      "Epoch [5/10], Batch [603/938], Loss: 0.5757\n",
      "Epoch [5/10], Batch [604/938], Loss: 0.6046\n",
      "Epoch [5/10], Batch [605/938], Loss: 0.6012\n",
      "Epoch [5/10], Batch [606/938], Loss: 0.5934\n",
      "Epoch [5/10], Batch [607/938], Loss: 0.5605\n",
      "Epoch [5/10], Batch [608/938], Loss: 0.6000\n",
      "Epoch [5/10], Batch [609/938], Loss: 0.5770\n",
      "Epoch [5/10], Batch [610/938], Loss: 0.6067\n",
      "Epoch [5/10], Batch [611/938], Loss: 0.6016\n",
      "Epoch [5/10], Batch [612/938], Loss: 0.5790\n",
      "Epoch [5/10], Batch [613/938], Loss: 0.5737\n",
      "Epoch [5/10], Batch [614/938], Loss: 0.5976\n",
      "Epoch [5/10], Batch [615/938], Loss: 0.5617\n",
      "Epoch [5/10], Batch [616/938], Loss: 0.6012\n",
      "Epoch [5/10], Batch [617/938], Loss: 0.5871\n",
      "Epoch [5/10], Batch [618/938], Loss: 0.5836\n",
      "Epoch [5/10], Batch [619/938], Loss: 0.5992\n",
      "Epoch [5/10], Batch [620/938], Loss: 0.5899\n",
      "Epoch [5/10], Batch [621/938], Loss: 0.6011\n",
      "Epoch [5/10], Batch [622/938], Loss: 0.6036\n",
      "Epoch [5/10], Batch [623/938], Loss: 0.5801\n",
      "Epoch [5/10], Batch [624/938], Loss: 0.5943\n",
      "Epoch [5/10], Batch [625/938], Loss: 0.6042\n",
      "Epoch [5/10], Batch [626/938], Loss: 0.5842\n",
      "Epoch [5/10], Batch [627/938], Loss: 0.6337\n",
      "Epoch [5/10], Batch [628/938], Loss: 0.5738\n",
      "Epoch [5/10], Batch [629/938], Loss: 0.6070\n",
      "Epoch [5/10], Batch [630/938], Loss: 0.5903\n",
      "Epoch [5/10], Batch [631/938], Loss: 0.5927\n",
      "Epoch [5/10], Batch [632/938], Loss: 0.5938\n",
      "Epoch [5/10], Batch [633/938], Loss: 0.6188\n",
      "Epoch [5/10], Batch [634/938], Loss: 0.6101\n",
      "Epoch [5/10], Batch [635/938], Loss: 0.5929\n",
      "Epoch [5/10], Batch [636/938], Loss: 0.5921\n",
      "Epoch [5/10], Batch [637/938], Loss: 0.5776\n",
      "Epoch [5/10], Batch [638/938], Loss: 0.5813\n",
      "Epoch [5/10], Batch [639/938], Loss: 0.5979\n",
      "Epoch [5/10], Batch [640/938], Loss: 0.5902\n",
      "Epoch [5/10], Batch [641/938], Loss: 0.5740\n",
      "Epoch [5/10], Batch [642/938], Loss: 0.5890\n",
      "Epoch [5/10], Batch [643/938], Loss: 0.5881\n",
      "Epoch [5/10], Batch [644/938], Loss: 0.5713\n",
      "Epoch [5/10], Batch [645/938], Loss: 0.6011\n",
      "Epoch [5/10], Batch [646/938], Loss: 0.6107\n",
      "Epoch [5/10], Batch [647/938], Loss: 0.5893\n",
      "Epoch [5/10], Batch [648/938], Loss: 0.5658\n",
      "Epoch [5/10], Batch [649/938], Loss: 0.5806\n",
      "Epoch [5/10], Batch [650/938], Loss: 0.5893\n",
      "Epoch [5/10], Batch [651/938], Loss: 0.5709\n",
      "Epoch [5/10], Batch [652/938], Loss: 0.6058\n",
      "Epoch [5/10], Batch [653/938], Loss: 0.5997\n",
      "Epoch [5/10], Batch [654/938], Loss: 0.6046\n",
      "Epoch [5/10], Batch [655/938], Loss: 0.5803\n",
      "Epoch [5/10], Batch [656/938], Loss: 0.5835\n",
      "Epoch [5/10], Batch [657/938], Loss: 0.6146\n",
      "Epoch [5/10], Batch [658/938], Loss: 0.5692\n",
      "Epoch [5/10], Batch [659/938], Loss: 0.5775\n",
      "Epoch [5/10], Batch [660/938], Loss: 0.5833\n",
      "Epoch [5/10], Batch [661/938], Loss: 0.6297\n",
      "Epoch [5/10], Batch [662/938], Loss: 0.5877\n",
      "Epoch [5/10], Batch [663/938], Loss: 0.6115\n",
      "Epoch [5/10], Batch [664/938], Loss: 0.5710\n",
      "Epoch [5/10], Batch [665/938], Loss: 0.5959\n",
      "Epoch [5/10], Batch [666/938], Loss: 0.6095\n",
      "Epoch [5/10], Batch [667/938], Loss: 0.6078\n",
      "Epoch [5/10], Batch [668/938], Loss: 0.6067\n",
      "Epoch [5/10], Batch [669/938], Loss: 0.5832\n",
      "Epoch [5/10], Batch [670/938], Loss: 0.6028\n",
      "Epoch [5/10], Batch [671/938], Loss: 0.5804\n",
      "Epoch [5/10], Batch [672/938], Loss: 0.5940\n",
      "Epoch [5/10], Batch [673/938], Loss: 0.5543\n",
      "Epoch [5/10], Batch [674/938], Loss: 0.5885\n",
      "Epoch [5/10], Batch [675/938], Loss: 0.5984\n",
      "Epoch [5/10], Batch [676/938], Loss: 0.5746\n",
      "Epoch [5/10], Batch [677/938], Loss: 0.5663\n",
      "Epoch [5/10], Batch [678/938], Loss: 0.5811\n",
      "Epoch [5/10], Batch [679/938], Loss: 0.5920\n",
      "Epoch [5/10], Batch [680/938], Loss: 0.5981\n",
      "Epoch [5/10], Batch [681/938], Loss: 0.5714\n",
      "Epoch [5/10], Batch [682/938], Loss: 0.5979\n",
      "Epoch [5/10], Batch [683/938], Loss: 0.5852\n",
      "Epoch [5/10], Batch [684/938], Loss: 0.5888\n",
      "Epoch [5/10], Batch [685/938], Loss: 0.5898\n",
      "Epoch [5/10], Batch [686/938], Loss: 0.5666\n",
      "Epoch [5/10], Batch [687/938], Loss: 0.5959\n",
      "Epoch [5/10], Batch [688/938], Loss: 0.5912\n",
      "Epoch [5/10], Batch [689/938], Loss: 0.5697\n",
      "Epoch [5/10], Batch [690/938], Loss: 0.5934\n",
      "Epoch [5/10], Batch [691/938], Loss: 0.5705\n",
      "Epoch [5/10], Batch [692/938], Loss: 0.5637\n",
      "Epoch [5/10], Batch [693/938], Loss: 0.6001\n",
      "Epoch [5/10], Batch [694/938], Loss: 0.5955\n",
      "Epoch [5/10], Batch [695/938], Loss: 0.5996\n",
      "Epoch [5/10], Batch [696/938], Loss: 0.6040\n",
      "Epoch [5/10], Batch [697/938], Loss: 0.5756\n",
      "Epoch [5/10], Batch [698/938], Loss: 0.5895\n",
      "Epoch [5/10], Batch [699/938], Loss: 0.5847\n",
      "Epoch [5/10], Batch [700/938], Loss: 0.6133\n",
      "Epoch [5/10], Batch [701/938], Loss: 0.6351\n",
      "Epoch [5/10], Batch [702/938], Loss: 0.5992\n",
      "Epoch [5/10], Batch [703/938], Loss: 0.5661\n",
      "Epoch [5/10], Batch [704/938], Loss: 0.5837\n",
      "Epoch [5/10], Batch [705/938], Loss: 0.5724\n",
      "Epoch [5/10], Batch [706/938], Loss: 0.5805\n",
      "Epoch [5/10], Batch [707/938], Loss: 0.5978\n",
      "Epoch [5/10], Batch [708/938], Loss: 0.5927\n",
      "Epoch [5/10], Batch [709/938], Loss: 0.6004\n",
      "Epoch [5/10], Batch [710/938], Loss: 0.5873\n",
      "Epoch [5/10], Batch [711/938], Loss: 0.5993\n",
      "Epoch [5/10], Batch [712/938], Loss: 0.5676\n",
      "Epoch [5/10], Batch [713/938], Loss: 0.5906\n",
      "Epoch [5/10], Batch [714/938], Loss: 0.5923\n",
      "Epoch [5/10], Batch [715/938], Loss: 0.6053\n",
      "Epoch [5/10], Batch [716/938], Loss: 0.6105\n",
      "Epoch [5/10], Batch [717/938], Loss: 0.5912\n",
      "Epoch [5/10], Batch [718/938], Loss: 0.5764\n",
      "Epoch [5/10], Batch [719/938], Loss: 0.6297\n",
      "Epoch [5/10], Batch [720/938], Loss: 0.6017\n",
      "Epoch [5/10], Batch [721/938], Loss: 0.6014\n",
      "Epoch [5/10], Batch [722/938], Loss: 0.5814\n",
      "Epoch [5/10], Batch [723/938], Loss: 0.5837\n",
      "Epoch [5/10], Batch [724/938], Loss: 0.6138\n",
      "Epoch [5/10], Batch [725/938], Loss: 0.6029\n",
      "Epoch [5/10], Batch [726/938], Loss: 0.5871\n",
      "Epoch [5/10], Batch [727/938], Loss: 0.5660\n",
      "Epoch [5/10], Batch [728/938], Loss: 0.5826\n",
      "Epoch [5/10], Batch [729/938], Loss: 0.6000\n",
      "Epoch [5/10], Batch [730/938], Loss: 0.5993\n",
      "Epoch [5/10], Batch [731/938], Loss: 0.5764\n",
      "Epoch [5/10], Batch [732/938], Loss: 0.5627\n",
      "Epoch [5/10], Batch [733/938], Loss: 0.5970\n",
      "Epoch [5/10], Batch [734/938], Loss: 0.5863\n",
      "Epoch [5/10], Batch [735/938], Loss: 0.6001\n",
      "Epoch [5/10], Batch [736/938], Loss: 0.5908\n",
      "Epoch [5/10], Batch [737/938], Loss: 0.5882\n",
      "Epoch [5/10], Batch [738/938], Loss: 0.6028\n",
      "Epoch [5/10], Batch [739/938], Loss: 0.5728\n",
      "Epoch [5/10], Batch [740/938], Loss: 0.6056\n",
      "Epoch [5/10], Batch [741/938], Loss: 0.5643\n",
      "Epoch [5/10], Batch [742/938], Loss: 0.6110\n",
      "Epoch [5/10], Batch [743/938], Loss: 0.5810\n",
      "Epoch [5/10], Batch [744/938], Loss: 0.6013\n",
      "Epoch [5/10], Batch [745/938], Loss: 0.5869\n",
      "Epoch [5/10], Batch [746/938], Loss: 0.6008\n",
      "Epoch [5/10], Batch [747/938], Loss: 0.5804\n",
      "Epoch [5/10], Batch [748/938], Loss: 0.5763\n",
      "Epoch [5/10], Batch [749/938], Loss: 0.5872\n",
      "Epoch [5/10], Batch [750/938], Loss: 0.5821\n",
      "Epoch [5/10], Batch [751/938], Loss: 0.6130\n",
      "Epoch [5/10], Batch [752/938], Loss: 0.5872\n",
      "Epoch [5/10], Batch [753/938], Loss: 0.5884\n",
      "Epoch [5/10], Batch [754/938], Loss: 0.5870\n",
      "Epoch [5/10], Batch [755/938], Loss: 0.5886\n",
      "Epoch [5/10], Batch [756/938], Loss: 0.5835\n",
      "Epoch [5/10], Batch [757/938], Loss: 0.6139\n",
      "Epoch [5/10], Batch [758/938], Loss: 0.6055\n",
      "Epoch [5/10], Batch [759/938], Loss: 0.5879\n",
      "Epoch [5/10], Batch [760/938], Loss: 0.6062\n",
      "Epoch [5/10], Batch [761/938], Loss: 0.5818\n",
      "Epoch [5/10], Batch [762/938], Loss: 0.5856\n",
      "Epoch [5/10], Batch [763/938], Loss: 0.6058\n",
      "Epoch [5/10], Batch [764/938], Loss: 0.6071\n",
      "Epoch [5/10], Batch [765/938], Loss: 0.6365\n",
      "Epoch [5/10], Batch [766/938], Loss: 0.6107\n",
      "Epoch [5/10], Batch [767/938], Loss: 0.5884\n",
      "Epoch [5/10], Batch [768/938], Loss: 0.5789\n",
      "Epoch [5/10], Batch [769/938], Loss: 0.6141\n",
      "Epoch [5/10], Batch [770/938], Loss: 0.5829\n",
      "Epoch [5/10], Batch [771/938], Loss: 0.5825\n",
      "Epoch [5/10], Batch [772/938], Loss: 0.6167\n",
      "Epoch [5/10], Batch [773/938], Loss: 0.6101\n",
      "Epoch [5/10], Batch [774/938], Loss: 0.6158\n",
      "Epoch [5/10], Batch [775/938], Loss: 0.5636\n",
      "Epoch [5/10], Batch [776/938], Loss: 0.5843\n",
      "Epoch [5/10], Batch [777/938], Loss: 0.5582\n",
      "Epoch [5/10], Batch [778/938], Loss: 0.5819\n",
      "Epoch [5/10], Batch [779/938], Loss: 0.5874\n",
      "Epoch [5/10], Batch [780/938], Loss: 0.6073\n",
      "Epoch [5/10], Batch [781/938], Loss: 0.5972\n",
      "Epoch [5/10], Batch [782/938], Loss: 0.6120\n",
      "Epoch [5/10], Batch [783/938], Loss: 0.5830\n",
      "Epoch [5/10], Batch [784/938], Loss: 0.5913\n",
      "Epoch [5/10], Batch [785/938], Loss: 0.6113\n",
      "Epoch [5/10], Batch [786/938], Loss: 0.5937\n",
      "Epoch [5/10], Batch [787/938], Loss: 0.6091\n",
      "Epoch [5/10], Batch [788/938], Loss: 0.6042\n",
      "Epoch [5/10], Batch [789/938], Loss: 0.5942\n",
      "Epoch [5/10], Batch [790/938], Loss: 0.6126\n",
      "Epoch [5/10], Batch [791/938], Loss: 0.5931\n",
      "Epoch [5/10], Batch [792/938], Loss: 0.5769\n",
      "Epoch [5/10], Batch [793/938], Loss: 0.6061\n",
      "Epoch [5/10], Batch [794/938], Loss: 0.6036\n",
      "Epoch [5/10], Batch [795/938], Loss: 0.5599\n",
      "Epoch [5/10], Batch [796/938], Loss: 0.5787\n",
      "Epoch [5/10], Batch [797/938], Loss: 0.5870\n",
      "Epoch [5/10], Batch [798/938], Loss: 0.6324\n",
      "Epoch [5/10], Batch [799/938], Loss: 0.5993\n",
      "Epoch [5/10], Batch [800/938], Loss: 0.6027\n",
      "Epoch [5/10], Batch [801/938], Loss: 0.6127\n",
      "Epoch [5/10], Batch [802/938], Loss: 0.5752\n",
      "Epoch [5/10], Batch [803/938], Loss: 0.5627\n",
      "Epoch [5/10], Batch [804/938], Loss: 0.6091\n",
      "Epoch [5/10], Batch [805/938], Loss: 0.5853\n",
      "Epoch [5/10], Batch [806/938], Loss: 0.5930\n",
      "Epoch [5/10], Batch [807/938], Loss: 0.6251\n",
      "Epoch [5/10], Batch [808/938], Loss: 0.6002\n",
      "Epoch [5/10], Batch [809/938], Loss: 0.6252\n",
      "Epoch [5/10], Batch [810/938], Loss: 0.6105\n",
      "Epoch [5/10], Batch [811/938], Loss: 0.6077\n",
      "Epoch [5/10], Batch [812/938], Loss: 0.6039\n",
      "Epoch [5/10], Batch [813/938], Loss: 0.5800\n",
      "Epoch [5/10], Batch [814/938], Loss: 0.5997\n",
      "Epoch [5/10], Batch [815/938], Loss: 0.5835\n",
      "Epoch [5/10], Batch [816/938], Loss: 0.5994\n",
      "Epoch [5/10], Batch [817/938], Loss: 0.5815\n",
      "Epoch [5/10], Batch [818/938], Loss: 0.5829\n",
      "Epoch [5/10], Batch [819/938], Loss: 0.5644\n",
      "Epoch [5/10], Batch [820/938], Loss: 0.5844\n",
      "Epoch [5/10], Batch [821/938], Loss: 0.6096\n",
      "Epoch [5/10], Batch [822/938], Loss: 0.5975\n",
      "Epoch [5/10], Batch [823/938], Loss: 0.6066\n",
      "Epoch [5/10], Batch [824/938], Loss: 0.6031\n",
      "Epoch [5/10], Batch [825/938], Loss: 0.5870\n",
      "Epoch [5/10], Batch [826/938], Loss: 0.5921\n",
      "Epoch [5/10], Batch [827/938], Loss: 0.5970\n",
      "Epoch [5/10], Batch [828/938], Loss: 0.6170\n",
      "Epoch [5/10], Batch [829/938], Loss: 0.5901\n",
      "Epoch [5/10], Batch [830/938], Loss: 0.6004\n",
      "Epoch [5/10], Batch [831/938], Loss: 0.6220\n",
      "Epoch [5/10], Batch [832/938], Loss: 0.5985\n",
      "Epoch [5/10], Batch [833/938], Loss: 0.5967\n",
      "Epoch [5/10], Batch [834/938], Loss: 0.6023\n",
      "Epoch [5/10], Batch [835/938], Loss: 0.5791\n",
      "Epoch [5/10], Batch [836/938], Loss: 0.5789\n",
      "Epoch [5/10], Batch [837/938], Loss: 0.5924\n",
      "Epoch [5/10], Batch [838/938], Loss: 0.6093\n",
      "Epoch [5/10], Batch [839/938], Loss: 0.5865\n",
      "Epoch [5/10], Batch [840/938], Loss: 0.5743\n",
      "Epoch [5/10], Batch [841/938], Loss: 0.5940\n",
      "Epoch [5/10], Batch [842/938], Loss: 0.5764\n",
      "Epoch [5/10], Batch [843/938], Loss: 0.6075\n",
      "Epoch [5/10], Batch [844/938], Loss: 0.5866\n",
      "Epoch [5/10], Batch [845/938], Loss: 0.6065\n",
      "Epoch [5/10], Batch [846/938], Loss: 0.5982\n",
      "Epoch [5/10], Batch [847/938], Loss: 0.5884\n",
      "Epoch [5/10], Batch [848/938], Loss: 0.5956\n",
      "Epoch [5/10], Batch [849/938], Loss: 0.5969\n",
      "Epoch [5/10], Batch [850/938], Loss: 0.6131\n",
      "Epoch [5/10], Batch [851/938], Loss: 0.5866\n",
      "Epoch [5/10], Batch [852/938], Loss: 0.5885\n",
      "Epoch [5/10], Batch [853/938], Loss: 0.5732\n",
      "Epoch [5/10], Batch [854/938], Loss: 0.5963\n",
      "Epoch [5/10], Batch [855/938], Loss: 0.5859\n",
      "Epoch [5/10], Batch [856/938], Loss: 0.5808\n",
      "Epoch [5/10], Batch [857/938], Loss: 0.5774\n",
      "Epoch [5/10], Batch [858/938], Loss: 0.6029\n",
      "Epoch [5/10], Batch [859/938], Loss: 0.6044\n",
      "Epoch [5/10], Batch [860/938], Loss: 0.6289\n",
      "Epoch [5/10], Batch [861/938], Loss: 0.5760\n",
      "Epoch [5/10], Batch [862/938], Loss: 0.5950\n",
      "Epoch [5/10], Batch [863/938], Loss: 0.5980\n",
      "Epoch [5/10], Batch [864/938], Loss: 0.5860\n",
      "Epoch [5/10], Batch [865/938], Loss: 0.5701\n",
      "Epoch [5/10], Batch [866/938], Loss: 0.5988\n",
      "Epoch [5/10], Batch [867/938], Loss: 0.6309\n",
      "Epoch [5/10], Batch [868/938], Loss: 0.5957\n",
      "Epoch [5/10], Batch [869/938], Loss: 0.5977\n",
      "Epoch [5/10], Batch [870/938], Loss: 0.5870\n",
      "Epoch [5/10], Batch [871/938], Loss: 0.5796\n",
      "Epoch [5/10], Batch [872/938], Loss: 0.6143\n",
      "Epoch [5/10], Batch [873/938], Loss: 0.5841\n",
      "Epoch [5/10], Batch [874/938], Loss: 0.5863\n",
      "Epoch [5/10], Batch [875/938], Loss: 0.6020\n",
      "Epoch [5/10], Batch [876/938], Loss: 0.5827\n",
      "Epoch [5/10], Batch [877/938], Loss: 0.5871\n",
      "Epoch [5/10], Batch [878/938], Loss: 0.6053\n",
      "Epoch [5/10], Batch [879/938], Loss: 0.5893\n",
      "Epoch [5/10], Batch [880/938], Loss: 0.6014\n",
      "Epoch [5/10], Batch [881/938], Loss: 0.6326\n",
      "Epoch [5/10], Batch [882/938], Loss: 0.5888\n",
      "Epoch [5/10], Batch [883/938], Loss: 0.5967\n",
      "Epoch [5/10], Batch [884/938], Loss: 0.6100\n",
      "Epoch [5/10], Batch [885/938], Loss: 0.6009\n",
      "Epoch [5/10], Batch [886/938], Loss: 0.5794\n",
      "Epoch [5/10], Batch [887/938], Loss: 0.5874\n",
      "Epoch [5/10], Batch [888/938], Loss: 0.5930\n",
      "Epoch [5/10], Batch [889/938], Loss: 0.5879\n",
      "Epoch [5/10], Batch [890/938], Loss: 0.6029\n",
      "Epoch [5/10], Batch [891/938], Loss: 0.6071\n",
      "Epoch [5/10], Batch [892/938], Loss: 0.5980\n",
      "Epoch [5/10], Batch [893/938], Loss: 0.5982\n",
      "Epoch [5/10], Batch [894/938], Loss: 0.6019\n",
      "Epoch [5/10], Batch [895/938], Loss: 0.6308\n",
      "Epoch [5/10], Batch [896/938], Loss: 0.6017\n",
      "Epoch [5/10], Batch [897/938], Loss: 0.6261\n",
      "Epoch [5/10], Batch [898/938], Loss: 0.5716\n",
      "Epoch [5/10], Batch [899/938], Loss: 0.5996\n",
      "Epoch [5/10], Batch [900/938], Loss: 0.5937\n",
      "Epoch [5/10], Batch [901/938], Loss: 0.5975\n",
      "Epoch [5/10], Batch [902/938], Loss: 0.6097\n",
      "Epoch [5/10], Batch [903/938], Loss: 0.6012\n",
      "Epoch [5/10], Batch [904/938], Loss: 0.6091\n",
      "Epoch [5/10], Batch [905/938], Loss: 0.6327\n",
      "Epoch [5/10], Batch [906/938], Loss: 0.6039\n",
      "Epoch [5/10], Batch [907/938], Loss: 0.6030\n",
      "Epoch [5/10], Batch [908/938], Loss: 0.6097\n",
      "Epoch [5/10], Batch [909/938], Loss: 0.5852\n",
      "Epoch [5/10], Batch [910/938], Loss: 0.5791\n",
      "Epoch [5/10], Batch [911/938], Loss: 0.5730\n",
      "Epoch [5/10], Batch [912/938], Loss: 0.6152\n",
      "Epoch [5/10], Batch [913/938], Loss: 0.6308\n",
      "Epoch [5/10], Batch [914/938], Loss: 0.5867\n",
      "Epoch [5/10], Batch [915/938], Loss: 0.6192\n",
      "Epoch [5/10], Batch [916/938], Loss: 0.5976\n",
      "Epoch [5/10], Batch [917/938], Loss: 0.5859\n",
      "Epoch [5/10], Batch [918/938], Loss: 0.5638\n",
      "Epoch [5/10], Batch [919/938], Loss: 0.5756\n",
      "Epoch [5/10], Batch [920/938], Loss: 0.6087\n",
      "Epoch [5/10], Batch [921/938], Loss: 0.6131\n",
      "Epoch [5/10], Batch [922/938], Loss: 0.5824\n",
      "Epoch [5/10], Batch [923/938], Loss: 0.6041\n",
      "Epoch [5/10], Batch [924/938], Loss: 0.5777\n",
      "Epoch [5/10], Batch [925/938], Loss: 0.5973\n",
      "Epoch [5/10], Batch [926/938], Loss: 0.5775\n",
      "Epoch [5/10], Batch [927/938], Loss: 0.5667\n",
      "Epoch [5/10], Batch [928/938], Loss: 0.5861\n",
      "Epoch [5/10], Batch [929/938], Loss: 0.5807\n",
      "Epoch [5/10], Batch [930/938], Loss: 0.5709\n",
      "Epoch [5/10], Batch [931/938], Loss: 0.5659\n",
      "Epoch [5/10], Batch [932/938], Loss: 0.6008\n",
      "Epoch [5/10], Batch [933/938], Loss: 0.6147\n",
      "Epoch [5/10], Batch [934/938], Loss: 0.5821\n",
      "Epoch [5/10], Batch [935/938], Loss: 0.5834\n",
      "Epoch [5/10], Batch [936/938], Loss: 0.5967\n",
      "Epoch [5/10], Batch [937/938], Loss: 0.5959\n",
      "Epoch [5/10], Batch [938/938], Loss: 0.6565\n",
      "Epoch [5/10], Loss: 0.6565\n",
      "Epoch [6/10], Batch [1/938], Loss: 0.6018\n",
      "Epoch [6/10], Batch [2/938], Loss: 0.5993\n",
      "Epoch [6/10], Batch [3/938], Loss: 0.5896\n",
      "Epoch [6/10], Batch [4/938], Loss: 0.6002\n",
      "Epoch [6/10], Batch [5/938], Loss: 0.5656\n",
      "Epoch [6/10], Batch [6/938], Loss: 0.5977\n",
      "Epoch [6/10], Batch [7/938], Loss: 0.6136\n",
      "Epoch [6/10], Batch [8/938], Loss: 0.5958\n",
      "Epoch [6/10], Batch [9/938], Loss: 0.5822\n",
      "Epoch [6/10], Batch [10/938], Loss: 0.5666\n",
      "Epoch [6/10], Batch [11/938], Loss: 0.6048\n",
      "Epoch [6/10], Batch [12/938], Loss: 0.5935\n",
      "Epoch [6/10], Batch [13/938], Loss: 0.6062\n",
      "Epoch [6/10], Batch [14/938], Loss: 0.5962\n",
      "Epoch [6/10], Batch [15/938], Loss: 0.5987\n",
      "Epoch [6/10], Batch [16/938], Loss: 0.5791\n",
      "Epoch [6/10], Batch [17/938], Loss: 0.5711\n",
      "Epoch [6/10], Batch [18/938], Loss: 0.5815\n",
      "Epoch [6/10], Batch [19/938], Loss: 0.5673\n",
      "Epoch [6/10], Batch [20/938], Loss: 0.5838\n",
      "Epoch [6/10], Batch [21/938], Loss: 0.5880\n",
      "Epoch [6/10], Batch [22/938], Loss: 0.5810\n",
      "Epoch [6/10], Batch [23/938], Loss: 0.5911\n",
      "Epoch [6/10], Batch [24/938], Loss: 0.5878\n",
      "Epoch [6/10], Batch [25/938], Loss: 0.5933\n",
      "Epoch [6/10], Batch [26/938], Loss: 0.6172\n",
      "Epoch [6/10], Batch [27/938], Loss: 0.5995\n",
      "Epoch [6/10], Batch [28/938], Loss: 0.6060\n",
      "Epoch [6/10], Batch [29/938], Loss: 0.6111\n",
      "Epoch [6/10], Batch [30/938], Loss: 0.5659\n",
      "Epoch [6/10], Batch [31/938], Loss: 0.5771\n",
      "Epoch [6/10], Batch [32/938], Loss: 0.5499\n",
      "Epoch [6/10], Batch [33/938], Loss: 0.5814\n",
      "Epoch [6/10], Batch [34/938], Loss: 0.6045\n",
      "Epoch [6/10], Batch [35/938], Loss: 0.5989\n",
      "Epoch [6/10], Batch [36/938], Loss: 0.6154\n",
      "Epoch [6/10], Batch [37/938], Loss: 0.5772\n",
      "Epoch [6/10], Batch [38/938], Loss: 0.5861\n",
      "Epoch [6/10], Batch [39/938], Loss: 0.6019\n",
      "Epoch [6/10], Batch [40/938], Loss: 0.5934\n",
      "Epoch [6/10], Batch [41/938], Loss: 0.6109\n",
      "Epoch [6/10], Batch [42/938], Loss: 0.5880\n",
      "Epoch [6/10], Batch [43/938], Loss: 0.6005\n",
      "Epoch [6/10], Batch [44/938], Loss: 0.5778\n",
      "Epoch [6/10], Batch [45/938], Loss: 0.5908\n",
      "Epoch [6/10], Batch [46/938], Loss: 0.5712\n",
      "Epoch [6/10], Batch [47/938], Loss: 0.5975\n",
      "Epoch [6/10], Batch [48/938], Loss: 0.5692\n",
      "Epoch [6/10], Batch [49/938], Loss: 0.5922\n",
      "Epoch [6/10], Batch [50/938], Loss: 0.5849\n",
      "Epoch [6/10], Batch [51/938], Loss: 0.5710\n",
      "Epoch [6/10], Batch [52/938], Loss: 0.5980\n",
      "Epoch [6/10], Batch [53/938], Loss: 0.5741\n",
      "Epoch [6/10], Batch [54/938], Loss: 0.5779\n",
      "Epoch [6/10], Batch [55/938], Loss: 0.5868\n",
      "Epoch [6/10], Batch [56/938], Loss: 0.5792\n",
      "Epoch [6/10], Batch [57/938], Loss: 0.6312\n",
      "Epoch [6/10], Batch [58/938], Loss: 0.5923\n",
      "Epoch [6/10], Batch [59/938], Loss: 0.6003\n",
      "Epoch [6/10], Batch [60/938], Loss: 0.5660\n",
      "Epoch [6/10], Batch [61/938], Loss: 0.6098\n",
      "Epoch [6/10], Batch [62/938], Loss: 0.5868\n",
      "Epoch [6/10], Batch [63/938], Loss: 0.6116\n",
      "Epoch [6/10], Batch [64/938], Loss: 0.5552\n",
      "Epoch [6/10], Batch [65/938], Loss: 0.5754\n",
      "Epoch [6/10], Batch [66/938], Loss: 0.5873\n",
      "Epoch [6/10], Batch [67/938], Loss: 0.5835\n",
      "Epoch [6/10], Batch [68/938], Loss: 0.5792\n",
      "Epoch [6/10], Batch [69/938], Loss: 0.5917\n",
      "Epoch [6/10], Batch [70/938], Loss: 0.5784\n",
      "Epoch [6/10], Batch [71/938], Loss: 0.5429\n",
      "Epoch [6/10], Batch [72/938], Loss: 0.5784\n",
      "Epoch [6/10], Batch [73/938], Loss: 0.6179\n",
      "Epoch [6/10], Batch [74/938], Loss: 0.5685\n",
      "Epoch [6/10], Batch [75/938], Loss: 0.5808\n",
      "Epoch [6/10], Batch [76/938], Loss: 0.6177\n",
      "Epoch [6/10], Batch [77/938], Loss: 0.5769\n",
      "Epoch [6/10], Batch [78/938], Loss: 0.6128\n",
      "Epoch [6/10], Batch [79/938], Loss: 0.5969\n",
      "Epoch [6/10], Batch [80/938], Loss: 0.6288\n",
      "Epoch [6/10], Batch [81/938], Loss: 0.5923\n",
      "Epoch [6/10], Batch [82/938], Loss: 0.5879\n",
      "Epoch [6/10], Batch [83/938], Loss: 0.6024\n",
      "Epoch [6/10], Batch [84/938], Loss: 0.5953\n",
      "Epoch [6/10], Batch [85/938], Loss: 0.5754\n",
      "Epoch [6/10], Batch [86/938], Loss: 0.5917\n",
      "Epoch [6/10], Batch [87/938], Loss: 0.6098\n",
      "Epoch [6/10], Batch [88/938], Loss: 0.6395\n",
      "Epoch [6/10], Batch [89/938], Loss: 0.6057\n",
      "Epoch [6/10], Batch [90/938], Loss: 0.5826\n",
      "Epoch [6/10], Batch [91/938], Loss: 0.5853\n",
      "Epoch [6/10], Batch [92/938], Loss: 0.6113\n",
      "Epoch [6/10], Batch [93/938], Loss: 0.5787\n",
      "Epoch [6/10], Batch [94/938], Loss: 0.5878\n",
      "Epoch [6/10], Batch [95/938], Loss: 0.5866\n",
      "Epoch [6/10], Batch [96/938], Loss: 0.5816\n",
      "Epoch [6/10], Batch [97/938], Loss: 0.6089\n",
      "Epoch [6/10], Batch [98/938], Loss: 0.5794\n",
      "Epoch [6/10], Batch [99/938], Loss: 0.5672\n",
      "Epoch [6/10], Batch [100/938], Loss: 0.5917\n",
      "Epoch [6/10], Batch [101/938], Loss: 0.6179\n",
      "Epoch [6/10], Batch [102/938], Loss: 0.5786\n",
      "Epoch [6/10], Batch [103/938], Loss: 0.5922\n",
      "Epoch [6/10], Batch [104/938], Loss: 0.5896\n",
      "Epoch [6/10], Batch [105/938], Loss: 0.5975\n",
      "Epoch [6/10], Batch [106/938], Loss: 0.6083\n",
      "Epoch [6/10], Batch [107/938], Loss: 0.5960\n",
      "Epoch [6/10], Batch [108/938], Loss: 0.5766\n",
      "Epoch [6/10], Batch [109/938], Loss: 0.5907\n",
      "Epoch [6/10], Batch [110/938], Loss: 0.6079\n",
      "Epoch [6/10], Batch [111/938], Loss: 0.6129\n",
      "Epoch [6/10], Batch [112/938], Loss: 0.5974\n",
      "Epoch [6/10], Batch [113/938], Loss: 0.5658\n",
      "Epoch [6/10], Batch [114/938], Loss: 0.6043\n",
      "Epoch [6/10], Batch [115/938], Loss: 0.6115\n",
      "Epoch [6/10], Batch [116/938], Loss: 0.6012\n",
      "Epoch [6/10], Batch [117/938], Loss: 0.6040\n",
      "Epoch [6/10], Batch [118/938], Loss: 0.6160\n",
      "Epoch [6/10], Batch [119/938], Loss: 0.5981\n",
      "Epoch [6/10], Batch [120/938], Loss: 0.5765\n",
      "Epoch [6/10], Batch [121/938], Loss: 0.5969\n",
      "Epoch [6/10], Batch [122/938], Loss: 0.5857\n",
      "Epoch [6/10], Batch [123/938], Loss: 0.5518\n",
      "Epoch [6/10], Batch [124/938], Loss: 0.5749\n",
      "Epoch [6/10], Batch [125/938], Loss: 0.5706\n",
      "Epoch [6/10], Batch [126/938], Loss: 0.6058\n",
      "Epoch [6/10], Batch [127/938], Loss: 0.6134\n",
      "Epoch [6/10], Batch [128/938], Loss: 0.5948\n",
      "Epoch [6/10], Batch [129/938], Loss: 0.6018\n",
      "Epoch [6/10], Batch [130/938], Loss: 0.5898\n",
      "Epoch [6/10], Batch [131/938], Loss: 0.6241\n",
      "Epoch [6/10], Batch [132/938], Loss: 0.5904\n",
      "Epoch [6/10], Batch [133/938], Loss: 0.5773\n",
      "Epoch [6/10], Batch [134/938], Loss: 0.6128\n",
      "Epoch [6/10], Batch [135/938], Loss: 0.5929\n",
      "Epoch [6/10], Batch [136/938], Loss: 0.6052\n",
      "Epoch [6/10], Batch [137/938], Loss: 0.5739\n",
      "Epoch [6/10], Batch [138/938], Loss: 0.5899\n",
      "Epoch [6/10], Batch [139/938], Loss: 0.5888\n",
      "Epoch [6/10], Batch [140/938], Loss: 0.5970\n",
      "Epoch [6/10], Batch [141/938], Loss: 0.5806\n",
      "Epoch [6/10], Batch [142/938], Loss: 0.5848\n",
      "Epoch [6/10], Batch [143/938], Loss: 0.5782\n",
      "Epoch [6/10], Batch [144/938], Loss: 0.6112\n",
      "Epoch [6/10], Batch [145/938], Loss: 0.5923\n",
      "Epoch [6/10], Batch [146/938], Loss: 0.5918\n",
      "Epoch [6/10], Batch [147/938], Loss: 0.6113\n",
      "Epoch [6/10], Batch [148/938], Loss: 0.5970\n",
      "Epoch [6/10], Batch [149/938], Loss: 0.5882\n",
      "Epoch [6/10], Batch [150/938], Loss: 0.5829\n",
      "Epoch [6/10], Batch [151/938], Loss: 0.6005\n",
      "Epoch [6/10], Batch [152/938], Loss: 0.5895\n",
      "Epoch [6/10], Batch [153/938], Loss: 0.6069\n",
      "Epoch [6/10], Batch [154/938], Loss: 0.5883\n",
      "Epoch [6/10], Batch [155/938], Loss: 0.5738\n",
      "Epoch [6/10], Batch [156/938], Loss: 0.5938\n",
      "Epoch [6/10], Batch [157/938], Loss: 0.5726\n",
      "Epoch [6/10], Batch [158/938], Loss: 0.6001\n",
      "Epoch [6/10], Batch [159/938], Loss: 0.5836\n",
      "Epoch [6/10], Batch [160/938], Loss: 0.6001\n",
      "Epoch [6/10], Batch [161/938], Loss: 0.5890\n",
      "Epoch [6/10], Batch [162/938], Loss: 0.5603\n",
      "Epoch [6/10], Batch [163/938], Loss: 0.5773\n",
      "Epoch [6/10], Batch [164/938], Loss: 0.5848\n",
      "Epoch [6/10], Batch [165/938], Loss: 0.6166\n",
      "Epoch [6/10], Batch [166/938], Loss: 0.6012\n",
      "Epoch [6/10], Batch [167/938], Loss: 0.6030\n",
      "Epoch [6/10], Batch [168/938], Loss: 0.6069\n",
      "Epoch [6/10], Batch [169/938], Loss: 0.6066\n",
      "Epoch [6/10], Batch [170/938], Loss: 0.6191\n",
      "Epoch [6/10], Batch [171/938], Loss: 0.5883\n",
      "Epoch [6/10], Batch [172/938], Loss: 0.5830\n",
      "Epoch [6/10], Batch [173/938], Loss: 0.5739\n",
      "Epoch [6/10], Batch [174/938], Loss: 0.5842\n",
      "Epoch [6/10], Batch [175/938], Loss: 0.6118\n",
      "Epoch [6/10], Batch [176/938], Loss: 0.6049\n",
      "Epoch [6/10], Batch [177/938], Loss: 0.5898\n",
      "Epoch [6/10], Batch [178/938], Loss: 0.6163\n",
      "Epoch [6/10], Batch [179/938], Loss: 0.6076\n",
      "Epoch [6/10], Batch [180/938], Loss: 0.5895\n",
      "Epoch [6/10], Batch [181/938], Loss: 0.6023\n",
      "Epoch [6/10], Batch [182/938], Loss: 0.6016\n",
      "Epoch [6/10], Batch [183/938], Loss: 0.5940\n",
      "Epoch [6/10], Batch [184/938], Loss: 0.5821\n",
      "Epoch [6/10], Batch [185/938], Loss: 0.5699\n",
      "Epoch [6/10], Batch [186/938], Loss: 0.5936\n",
      "Epoch [6/10], Batch [187/938], Loss: 0.6119\n",
      "Epoch [6/10], Batch [188/938], Loss: 0.5704\n",
      "Epoch [6/10], Batch [189/938], Loss: 0.6288\n",
      "Epoch [6/10], Batch [190/938], Loss: 0.5909\n",
      "Epoch [6/10], Batch [191/938], Loss: 0.5721\n",
      "Epoch [6/10], Batch [192/938], Loss: 0.5965\n",
      "Epoch [6/10], Batch [193/938], Loss: 0.5856\n",
      "Epoch [6/10], Batch [194/938], Loss: 0.5792\n",
      "Epoch [6/10], Batch [195/938], Loss: 0.6123\n",
      "Epoch [6/10], Batch [196/938], Loss: 0.6075\n",
      "Epoch [6/10], Batch [197/938], Loss: 0.5881\n",
      "Epoch [6/10], Batch [198/938], Loss: 0.6166\n",
      "Epoch [6/10], Batch [199/938], Loss: 0.5667\n",
      "Epoch [6/10], Batch [200/938], Loss: 0.5770\n",
      "Epoch [6/10], Batch [201/938], Loss: 0.5726\n",
      "Epoch [6/10], Batch [202/938], Loss: 0.5985\n",
      "Epoch [6/10], Batch [203/938], Loss: 0.5950\n",
      "Epoch [6/10], Batch [204/938], Loss: 0.6070\n",
      "Epoch [6/10], Batch [205/938], Loss: 0.6174\n",
      "Epoch [6/10], Batch [206/938], Loss: 0.5919\n",
      "Epoch [6/10], Batch [207/938], Loss: 0.6156\n",
      "Epoch [6/10], Batch [208/938], Loss: 0.6078\n",
      "Epoch [6/10], Batch [209/938], Loss: 0.5725\n",
      "Epoch [6/10], Batch [210/938], Loss: 0.6146\n",
      "Epoch [6/10], Batch [211/938], Loss: 0.5712\n",
      "Epoch [6/10], Batch [212/938], Loss: 0.5939\n",
      "Epoch [6/10], Batch [213/938], Loss: 0.5793\n",
      "Epoch [6/10], Batch [214/938], Loss: 0.5843\n",
      "Epoch [6/10], Batch [215/938], Loss: 0.5711\n",
      "Epoch [6/10], Batch [216/938], Loss: 0.6071\n",
      "Epoch [6/10], Batch [217/938], Loss: 0.6059\n",
      "Epoch [6/10], Batch [218/938], Loss: 0.5772\n",
      "Epoch [6/10], Batch [219/938], Loss: 0.6073\n",
      "Epoch [6/10], Batch [220/938], Loss: 0.5911\n",
      "Epoch [6/10], Batch [221/938], Loss: 0.6040\n",
      "Epoch [6/10], Batch [222/938], Loss: 0.6044\n",
      "Epoch [6/10], Batch [223/938], Loss: 0.5803\n",
      "Epoch [6/10], Batch [224/938], Loss: 0.6033\n",
      "Epoch [6/10], Batch [225/938], Loss: 0.5997\n",
      "Epoch [6/10], Batch [226/938], Loss: 0.5929\n",
      "Epoch [6/10], Batch [227/938], Loss: 0.5848\n",
      "Epoch [6/10], Batch [228/938], Loss: 0.5765\n",
      "Epoch [6/10], Batch [229/938], Loss: 0.5758\n",
      "Epoch [6/10], Batch [230/938], Loss: 0.5882\n",
      "Epoch [6/10], Batch [231/938], Loss: 0.5911\n",
      "Epoch [6/10], Batch [232/938], Loss: 0.6001\n",
      "Epoch [6/10], Batch [233/938], Loss: 0.6000\n",
      "Epoch [6/10], Batch [234/938], Loss: 0.6299\n",
      "Epoch [6/10], Batch [235/938], Loss: 0.5900\n",
      "Epoch [6/10], Batch [236/938], Loss: 0.5746\n",
      "Epoch [6/10], Batch [237/938], Loss: 0.5955\n",
      "Epoch [6/10], Batch [238/938], Loss: 0.6032\n",
      "Epoch [6/10], Batch [239/938], Loss: 0.5922\n",
      "Epoch [6/10], Batch [240/938], Loss: 0.6074\n",
      "Epoch [6/10], Batch [241/938], Loss: 0.6030\n",
      "Epoch [6/10], Batch [242/938], Loss: 0.5848\n",
      "Epoch [6/10], Batch [243/938], Loss: 0.5956\n",
      "Epoch [6/10], Batch [244/938], Loss: 0.5649\n",
      "Epoch [6/10], Batch [245/938], Loss: 0.6219\n",
      "Epoch [6/10], Batch [246/938], Loss: 0.5938\n",
      "Epoch [6/10], Batch [247/938], Loss: 0.5825\n",
      "Epoch [6/10], Batch [248/938], Loss: 0.6030\n",
      "Epoch [6/10], Batch [249/938], Loss: 0.5978\n",
      "Epoch [6/10], Batch [250/938], Loss: 0.5868\n",
      "Epoch [6/10], Batch [251/938], Loss: 0.5918\n",
      "Epoch [6/10], Batch [252/938], Loss: 0.5910\n",
      "Epoch [6/10], Batch [253/938], Loss: 0.5926\n",
      "Epoch [6/10], Batch [254/938], Loss: 0.6121\n",
      "Epoch [6/10], Batch [255/938], Loss: 0.6050\n",
      "Epoch [6/10], Batch [256/938], Loss: 0.5694\n",
      "Epoch [6/10], Batch [257/938], Loss: 0.5692\n",
      "Epoch [6/10], Batch [258/938], Loss: 0.5864\n",
      "Epoch [6/10], Batch [259/938], Loss: 0.5900\n",
      "Epoch [6/10], Batch [260/938], Loss: 0.5922\n",
      "Epoch [6/10], Batch [261/938], Loss: 0.5880\n",
      "Epoch [6/10], Batch [262/938], Loss: 0.5949\n",
      "Epoch [6/10], Batch [263/938], Loss: 0.5807\n",
      "Epoch [6/10], Batch [264/938], Loss: 0.5850\n",
      "Epoch [6/10], Batch [265/938], Loss: 0.5798\n",
      "Epoch [6/10], Batch [266/938], Loss: 0.6079\n",
      "Epoch [6/10], Batch [267/938], Loss: 0.5968\n",
      "Epoch [6/10], Batch [268/938], Loss: 0.5974\n",
      "Epoch [6/10], Batch [269/938], Loss: 0.6202\n",
      "Epoch [6/10], Batch [270/938], Loss: 0.6015\n",
      "Epoch [6/10], Batch [271/938], Loss: 0.5842\n",
      "Epoch [6/10], Batch [272/938], Loss: 0.6131\n",
      "Epoch [6/10], Batch [273/938], Loss: 0.5928\n",
      "Epoch [6/10], Batch [274/938], Loss: 0.5748\n",
      "Epoch [6/10], Batch [275/938], Loss: 0.5740\n",
      "Epoch [6/10], Batch [276/938], Loss: 0.5683\n",
      "Epoch [6/10], Batch [277/938], Loss: 0.6014\n",
      "Epoch [6/10], Batch [278/938], Loss: 0.5885\n",
      "Epoch [6/10], Batch [279/938], Loss: 0.5869\n",
      "Epoch [6/10], Batch [280/938], Loss: 0.6139\n",
      "Epoch [6/10], Batch [281/938], Loss: 0.5996\n",
      "Epoch [6/10], Batch [282/938], Loss: 0.5846\n",
      "Epoch [6/10], Batch [283/938], Loss: 0.5660\n",
      "Epoch [6/10], Batch [284/938], Loss: 0.5656\n",
      "Epoch [6/10], Batch [285/938], Loss: 0.5881\n",
      "Epoch [6/10], Batch [286/938], Loss: 0.5718\n",
      "Epoch [6/10], Batch [287/938], Loss: 0.6482\n",
      "Epoch [6/10], Batch [288/938], Loss: 0.5707\n",
      "Epoch [6/10], Batch [289/938], Loss: 0.5935\n",
      "Epoch [6/10], Batch [290/938], Loss: 0.5830\n",
      "Epoch [6/10], Batch [291/938], Loss: 0.5975\n",
      "Epoch [6/10], Batch [292/938], Loss: 0.5759\n",
      "Epoch [6/10], Batch [293/938], Loss: 0.6061\n",
      "Epoch [6/10], Batch [294/938], Loss: 0.5933\n",
      "Epoch [6/10], Batch [295/938], Loss: 0.5689\n",
      "Epoch [6/10], Batch [296/938], Loss: 0.6308\n",
      "Epoch [6/10], Batch [297/938], Loss: 0.6092\n",
      "Epoch [6/10], Batch [298/938], Loss: 0.5829\n",
      "Epoch [6/10], Batch [299/938], Loss: 0.5672\n",
      "Epoch [6/10], Batch [300/938], Loss: 0.5967\n",
      "Epoch [6/10], Batch [301/938], Loss: 0.5981\n",
      "Epoch [6/10], Batch [302/938], Loss: 0.5875\n",
      "Epoch [6/10], Batch [303/938], Loss: 0.6175\n",
      "Epoch [6/10], Batch [304/938], Loss: 0.5689\n",
      "Epoch [6/10], Batch [305/938], Loss: 0.5792\n",
      "Epoch [6/10], Batch [306/938], Loss: 0.5999\n",
      "Epoch [6/10], Batch [307/938], Loss: 0.5735\n",
      "Epoch [6/10], Batch [308/938], Loss: 0.6059\n",
      "Epoch [6/10], Batch [309/938], Loss: 0.5612\n",
      "Epoch [6/10], Batch [310/938], Loss: 0.5749\n",
      "Epoch [6/10], Batch [311/938], Loss: 0.5770\n",
      "Epoch [6/10], Batch [312/938], Loss: 0.6079\n",
      "Epoch [6/10], Batch [313/938], Loss: 0.5897\n",
      "Epoch [6/10], Batch [314/938], Loss: 0.5822\n",
      "Epoch [6/10], Batch [315/938], Loss: 0.5771\n",
      "Epoch [6/10], Batch [316/938], Loss: 0.6120\n",
      "Epoch [6/10], Batch [317/938], Loss: 0.5854\n",
      "Epoch [6/10], Batch [318/938], Loss: 0.5680\n",
      "Epoch [6/10], Batch [319/938], Loss: 0.6011\n",
      "Epoch [6/10], Batch [320/938], Loss: 0.5986\n",
      "Epoch [6/10], Batch [321/938], Loss: 0.6022\n",
      "Epoch [6/10], Batch [322/938], Loss: 0.5793\n",
      "Epoch [6/10], Batch [323/938], Loss: 0.5714\n",
      "Epoch [6/10], Batch [324/938], Loss: 0.5966\n",
      "Epoch [6/10], Batch [325/938], Loss: 0.5667\n",
      "Epoch [6/10], Batch [326/938], Loss: 0.5925\n",
      "Epoch [6/10], Batch [327/938], Loss: 0.6078\n",
      "Epoch [6/10], Batch [328/938], Loss: 0.5974\n",
      "Epoch [6/10], Batch [329/938], Loss: 0.5784\n",
      "Epoch [6/10], Batch [330/938], Loss: 0.5943\n",
      "Epoch [6/10], Batch [331/938], Loss: 0.5616\n",
      "Epoch [6/10], Batch [332/938], Loss: 0.5969\n",
      "Epoch [6/10], Batch [333/938], Loss: 0.6004\n",
      "Epoch [6/10], Batch [334/938], Loss: 0.5942\n",
      "Epoch [6/10], Batch [335/938], Loss: 0.5791\n",
      "Epoch [6/10], Batch [336/938], Loss: 0.5914\n",
      "Epoch [6/10], Batch [337/938], Loss: 0.5851\n",
      "Epoch [6/10], Batch [338/938], Loss: 0.5924\n",
      "Epoch [6/10], Batch [339/938], Loss: 0.5887\n",
      "Epoch [6/10], Batch [340/938], Loss: 0.5730\n",
      "Epoch [6/10], Batch [341/938], Loss: 0.6203\n",
      "Epoch [6/10], Batch [342/938], Loss: 0.5814\n",
      "Epoch [6/10], Batch [343/938], Loss: 0.5835\n",
      "Epoch [6/10], Batch [344/938], Loss: 0.5921\n",
      "Epoch [6/10], Batch [345/938], Loss: 0.5837\n",
      "Epoch [6/10], Batch [346/938], Loss: 0.5820\n",
      "Epoch [6/10], Batch [347/938], Loss: 0.5992\n",
      "Epoch [6/10], Batch [348/938], Loss: 0.5711\n",
      "Epoch [6/10], Batch [349/938], Loss: 0.5814\n",
      "Epoch [6/10], Batch [350/938], Loss: 0.6026\n",
      "Epoch [6/10], Batch [351/938], Loss: 0.6083\n",
      "Epoch [6/10], Batch [352/938], Loss: 0.5968\n",
      "Epoch [6/10], Batch [353/938], Loss: 0.5882\n",
      "Epoch [6/10], Batch [354/938], Loss: 0.6020\n",
      "Epoch [6/10], Batch [355/938], Loss: 0.5940\n",
      "Epoch [6/10], Batch [356/938], Loss: 0.5638\n",
      "Epoch [6/10], Batch [357/938], Loss: 0.5831\n",
      "Epoch [6/10], Batch [358/938], Loss: 0.5710\n",
      "Epoch [6/10], Batch [359/938], Loss: 0.5840\n",
      "Epoch [6/10], Batch [360/938], Loss: 0.5990\n",
      "Epoch [6/10], Batch [361/938], Loss: 0.5937\n",
      "Epoch [6/10], Batch [362/938], Loss: 0.5700\n",
      "Epoch [6/10], Batch [363/938], Loss: 0.5752\n",
      "Epoch [6/10], Batch [364/938], Loss: 0.6025\n",
      "Epoch [6/10], Batch [365/938], Loss: 0.5921\n",
      "Epoch [6/10], Batch [366/938], Loss: 0.5921\n",
      "Epoch [6/10], Batch [367/938], Loss: 0.5700\n",
      "Epoch [6/10], Batch [368/938], Loss: 0.5995\n",
      "Epoch [6/10], Batch [369/938], Loss: 0.5992\n",
      "Epoch [6/10], Batch [370/938], Loss: 0.5903\n",
      "Epoch [6/10], Batch [371/938], Loss: 0.5743\n",
      "Epoch [6/10], Batch [372/938], Loss: 0.5924\n",
      "Epoch [6/10], Batch [373/938], Loss: 0.5834\n",
      "Epoch [6/10], Batch [374/938], Loss: 0.6133\n",
      "Epoch [6/10], Batch [375/938], Loss: 0.5750\n",
      "Epoch [6/10], Batch [376/938], Loss: 0.5991\n",
      "Epoch [6/10], Batch [377/938], Loss: 0.6059\n",
      "Epoch [6/10], Batch [378/938], Loss: 0.6033\n",
      "Epoch [6/10], Batch [379/938], Loss: 0.5862\n",
      "Epoch [6/10], Batch [380/938], Loss: 0.5754\n",
      "Epoch [6/10], Batch [381/938], Loss: 0.5862\n",
      "Epoch [6/10], Batch [382/938], Loss: 0.5618\n",
      "Epoch [6/10], Batch [383/938], Loss: 0.5840\n",
      "Epoch [6/10], Batch [384/938], Loss: 0.6341\n",
      "Epoch [6/10], Batch [385/938], Loss: 0.6134\n",
      "Epoch [6/10], Batch [386/938], Loss: 0.6163\n",
      "Epoch [6/10], Batch [387/938], Loss: 0.5895\n",
      "Epoch [6/10], Batch [388/938], Loss: 0.6054\n",
      "Epoch [6/10], Batch [389/938], Loss: 0.5783\n",
      "Epoch [6/10], Batch [390/938], Loss: 0.5750\n",
      "Epoch [6/10], Batch [391/938], Loss: 0.5429\n",
      "Epoch [6/10], Batch [392/938], Loss: 0.5984\n",
      "Epoch [6/10], Batch [393/938], Loss: 0.5900\n",
      "Epoch [6/10], Batch [394/938], Loss: 0.6049\n",
      "Epoch [6/10], Batch [395/938], Loss: 0.5736\n",
      "Epoch [6/10], Batch [396/938], Loss: 0.5994\n",
      "Epoch [6/10], Batch [397/938], Loss: 0.6354\n",
      "Epoch [6/10], Batch [398/938], Loss: 0.6041\n",
      "Epoch [6/10], Batch [399/938], Loss: 0.5667\n",
      "Epoch [6/10], Batch [400/938], Loss: 0.6059\n",
      "Epoch [6/10], Batch [401/938], Loss: 0.6166\n",
      "Epoch [6/10], Batch [402/938], Loss: 0.5737\n",
      "Epoch [6/10], Batch [403/938], Loss: 0.5790\n",
      "Epoch [6/10], Batch [404/938], Loss: 0.6103\n",
      "Epoch [6/10], Batch [405/938], Loss: 0.5765\n",
      "Epoch [6/10], Batch [406/938], Loss: 0.6116\n",
      "Epoch [6/10], Batch [407/938], Loss: 0.5900\n",
      "Epoch [6/10], Batch [408/938], Loss: 0.5841\n",
      "Epoch [6/10], Batch [409/938], Loss: 0.6184\n",
      "Epoch [6/10], Batch [410/938], Loss: 0.5923\n",
      "Epoch [6/10], Batch [411/938], Loss: 0.6024\n",
      "Epoch [6/10], Batch [412/938], Loss: 0.6296\n",
      "Epoch [6/10], Batch [413/938], Loss: 0.5768\n",
      "Epoch [6/10], Batch [414/938], Loss: 0.5880\n",
      "Epoch [6/10], Batch [415/938], Loss: 0.5924\n",
      "Epoch [6/10], Batch [416/938], Loss: 0.5629\n",
      "Epoch [6/10], Batch [417/938], Loss: 0.5987\n",
      "Epoch [6/10], Batch [418/938], Loss: 0.5697\n",
      "Epoch [6/10], Batch [419/938], Loss: 0.6061\n",
      "Epoch [6/10], Batch [420/938], Loss: 0.6042\n",
      "Epoch [6/10], Batch [421/938], Loss: 0.6229\n",
      "Epoch [6/10], Batch [422/938], Loss: 0.6153\n",
      "Epoch [6/10], Batch [423/938], Loss: 0.6072\n",
      "Epoch [6/10], Batch [424/938], Loss: 0.5969\n",
      "Epoch [6/10], Batch [425/938], Loss: 0.5937\n",
      "Epoch [6/10], Batch [426/938], Loss: 0.6181\n",
      "Epoch [6/10], Batch [427/938], Loss: 0.6157\n",
      "Epoch [6/10], Batch [428/938], Loss: 0.5805\n",
      "Epoch [6/10], Batch [429/938], Loss: 0.6105\n",
      "Epoch [6/10], Batch [430/938], Loss: 0.5844\n",
      "Epoch [6/10], Batch [431/938], Loss: 0.5616\n",
      "Epoch [6/10], Batch [432/938], Loss: 0.5723\n",
      "Epoch [6/10], Batch [433/938], Loss: 0.5728\n",
      "Epoch [6/10], Batch [434/938], Loss: 0.5637\n",
      "Epoch [6/10], Batch [435/938], Loss: 0.5689\n",
      "Epoch [6/10], Batch [436/938], Loss: 0.5937\n",
      "Epoch [6/10], Batch [437/938], Loss: 0.6144\n",
      "Epoch [6/10], Batch [438/938], Loss: 0.5678\n",
      "Epoch [6/10], Batch [439/938], Loss: 0.6099\n",
      "Epoch [6/10], Batch [440/938], Loss: 0.5933\n",
      "Epoch [6/10], Batch [441/938], Loss: 0.6037\n",
      "Epoch [6/10], Batch [442/938], Loss: 0.5984\n",
      "Epoch [6/10], Batch [443/938], Loss: 0.5705\n",
      "Epoch [6/10], Batch [444/938], Loss: 0.5895\n",
      "Epoch [6/10], Batch [445/938], Loss: 0.5950\n",
      "Epoch [6/10], Batch [446/938], Loss: 0.5962\n",
      "Epoch [6/10], Batch [447/938], Loss: 0.6071\n",
      "Epoch [6/10], Batch [448/938], Loss: 0.6209\n",
      "Epoch [6/10], Batch [449/938], Loss: 0.6138\n",
      "Epoch [6/10], Batch [450/938], Loss: 0.5931\n",
      "Epoch [6/10], Batch [451/938], Loss: 0.6147\n",
      "Epoch [6/10], Batch [452/938], Loss: 0.5780\n",
      "Epoch [6/10], Batch [453/938], Loss: 0.6050\n",
      "Epoch [6/10], Batch [454/938], Loss: 0.5978\n",
      "Epoch [6/10], Batch [455/938], Loss: 0.5987\n",
      "Epoch [6/10], Batch [456/938], Loss: 0.6200\n",
      "Epoch [6/10], Batch [457/938], Loss: 0.5759\n",
      "Epoch [6/10], Batch [458/938], Loss: 0.5785\n",
      "Epoch [6/10], Batch [459/938], Loss: 0.5864\n",
      "Epoch [6/10], Batch [460/938], Loss: 0.5998\n",
      "Epoch [6/10], Batch [461/938], Loss: 0.5913\n",
      "Epoch [6/10], Batch [462/938], Loss: 0.6314\n",
      "Epoch [6/10], Batch [463/938], Loss: 0.5857\n",
      "Epoch [6/10], Batch [464/938], Loss: 0.5812\n",
      "Epoch [6/10], Batch [465/938], Loss: 0.5885\n",
      "Epoch [6/10], Batch [466/938], Loss: 0.5912\n",
      "Epoch [6/10], Batch [467/938], Loss: 0.5755\n",
      "Epoch [6/10], Batch [468/938], Loss: 0.6111\n",
      "Epoch [6/10], Batch [469/938], Loss: 0.5943\n",
      "Epoch [6/10], Batch [470/938], Loss: 0.5990\n",
      "Epoch [6/10], Batch [471/938], Loss: 0.6098\n",
      "Epoch [6/10], Batch [472/938], Loss: 0.5995\n",
      "Epoch [6/10], Batch [473/938], Loss: 0.5788\n",
      "Epoch [6/10], Batch [474/938], Loss: 0.5981\n",
      "Epoch [6/10], Batch [475/938], Loss: 0.6046\n",
      "Epoch [6/10], Batch [476/938], Loss: 0.5597\n",
      "Epoch [6/10], Batch [477/938], Loss: 0.6006\n",
      "Epoch [6/10], Batch [478/938], Loss: 0.6294\n",
      "Epoch [6/10], Batch [479/938], Loss: 0.5896\n",
      "Epoch [6/10], Batch [480/938], Loss: 0.5898\n",
      "Epoch [6/10], Batch [481/938], Loss: 0.6399\n",
      "Epoch [6/10], Batch [482/938], Loss: 0.5711\n",
      "Epoch [6/10], Batch [483/938], Loss: 0.5878\n",
      "Epoch [6/10], Batch [484/938], Loss: 0.6161\n",
      "Epoch [6/10], Batch [485/938], Loss: 0.5910\n",
      "Epoch [6/10], Batch [486/938], Loss: 0.5845\n",
      "Epoch [6/10], Batch [487/938], Loss: 0.5610\n",
      "Epoch [6/10], Batch [488/938], Loss: 0.6074\n",
      "Epoch [6/10], Batch [489/938], Loss: 0.6161\n",
      "Epoch [6/10], Batch [490/938], Loss: 0.5857\n",
      "Epoch [6/10], Batch [491/938], Loss: 0.5670\n",
      "Epoch [6/10], Batch [492/938], Loss: 0.5795\n",
      "Epoch [6/10], Batch [493/938], Loss: 0.6036\n",
      "Epoch [6/10], Batch [494/938], Loss: 0.6137\n",
      "Epoch [6/10], Batch [495/938], Loss: 0.6197\n",
      "Epoch [6/10], Batch [496/938], Loss: 0.6059\n",
      "Epoch [6/10], Batch [497/938], Loss: 0.5939\n",
      "Epoch [6/10], Batch [498/938], Loss: 0.6161\n",
      "Epoch [6/10], Batch [499/938], Loss: 0.5851\n",
      "Epoch [6/10], Batch [500/938], Loss: 0.6079\n",
      "Epoch [6/10], Batch [501/938], Loss: 0.6238\n",
      "Epoch [6/10], Batch [502/938], Loss: 0.5617\n",
      "Epoch [6/10], Batch [503/938], Loss: 0.6046\n",
      "Epoch [6/10], Batch [504/938], Loss: 0.5999\n",
      "Epoch [6/10], Batch [505/938], Loss: 0.6002\n",
      "Epoch [6/10], Batch [506/938], Loss: 0.6295\n",
      "Epoch [6/10], Batch [507/938], Loss: 0.6084\n",
      "Epoch [6/10], Batch [508/938], Loss: 0.5879\n",
      "Epoch [6/10], Batch [509/938], Loss: 0.6103\n",
      "Epoch [6/10], Batch [510/938], Loss: 0.5819\n",
      "Epoch [6/10], Batch [511/938], Loss: 0.5726\n",
      "Epoch [6/10], Batch [512/938], Loss: 0.5987\n",
      "Epoch [6/10], Batch [513/938], Loss: 0.6141\n",
      "Epoch [6/10], Batch [514/938], Loss: 0.5920\n",
      "Epoch [6/10], Batch [515/938], Loss: 0.5825\n",
      "Epoch [6/10], Batch [516/938], Loss: 0.6140\n",
      "Epoch [6/10], Batch [517/938], Loss: 0.5830\n",
      "Epoch [6/10], Batch [518/938], Loss: 0.5956\n",
      "Epoch [6/10], Batch [519/938], Loss: 0.5992\n",
      "Epoch [6/10], Batch [520/938], Loss: 0.5948\n",
      "Epoch [6/10], Batch [521/938], Loss: 0.5781\n",
      "Epoch [6/10], Batch [522/938], Loss: 0.5855\n",
      "Epoch [6/10], Batch [523/938], Loss: 0.5891\n",
      "Epoch [6/10], Batch [524/938], Loss: 0.5931\n",
      "Epoch [6/10], Batch [525/938], Loss: 0.5985\n",
      "Epoch [6/10], Batch [526/938], Loss: 0.5981\n",
      "Epoch [6/10], Batch [527/938], Loss: 0.5710\n",
      "Epoch [6/10], Batch [528/938], Loss: 0.5836\n",
      "Epoch [6/10], Batch [529/938], Loss: 0.5745\n",
      "Epoch [6/10], Batch [530/938], Loss: 0.5534\n",
      "Epoch [6/10], Batch [531/938], Loss: 0.5602\n",
      "Epoch [6/10], Batch [532/938], Loss: 0.6131\n",
      "Epoch [6/10], Batch [533/938], Loss: 0.6117\n",
      "Epoch [6/10], Batch [534/938], Loss: 0.6153\n",
      "Epoch [6/10], Batch [535/938], Loss: 0.5864\n",
      "Epoch [6/10], Batch [536/938], Loss: 0.6229\n",
      "Epoch [6/10], Batch [537/938], Loss: 0.5876\n",
      "Epoch [6/10], Batch [538/938], Loss: 0.5981\n",
      "Epoch [6/10], Batch [539/938], Loss: 0.5645\n",
      "Epoch [6/10], Batch [540/938], Loss: 0.5687\n",
      "Epoch [6/10], Batch [541/938], Loss: 0.5724\n",
      "Epoch [6/10], Batch [542/938], Loss: 0.5925\n",
      "Epoch [6/10], Batch [543/938], Loss: 0.6115\n",
      "Epoch [6/10], Batch [544/938], Loss: 0.5747\n",
      "Epoch [6/10], Batch [545/938], Loss: 0.5985\n",
      "Epoch [6/10], Batch [546/938], Loss: 0.5905\n",
      "Epoch [6/10], Batch [547/938], Loss: 0.5970\n",
      "Epoch [6/10], Batch [548/938], Loss: 0.5724\n",
      "Epoch [6/10], Batch [549/938], Loss: 0.6052\n",
      "Epoch [6/10], Batch [550/938], Loss: 0.6001\n",
      "Epoch [6/10], Batch [551/938], Loss: 0.5853\n",
      "Epoch [6/10], Batch [552/938], Loss: 0.6204\n",
      "Epoch [6/10], Batch [553/938], Loss: 0.6053\n",
      "Epoch [6/10], Batch [554/938], Loss: 0.5867\n",
      "Epoch [6/10], Batch [555/938], Loss: 0.6025\n",
      "Epoch [6/10], Batch [556/938], Loss: 0.5620\n",
      "Epoch [6/10], Batch [557/938], Loss: 0.6038\n",
      "Epoch [6/10], Batch [558/938], Loss: 0.6307\n",
      "Epoch [6/10], Batch [559/938], Loss: 0.5673\n",
      "Epoch [6/10], Batch [560/938], Loss: 0.6059\n",
      "Epoch [6/10], Batch [561/938], Loss: 0.6009\n",
      "Epoch [6/10], Batch [562/938], Loss: 0.5825\n",
      "Epoch [6/10], Batch [563/938], Loss: 0.5841\n",
      "Epoch [6/10], Batch [564/938], Loss: 0.6006\n",
      "Epoch [6/10], Batch [565/938], Loss: 0.5788\n",
      "Epoch [6/10], Batch [566/938], Loss: 0.6010\n",
      "Epoch [6/10], Batch [567/938], Loss: 0.5827\n",
      "Epoch [6/10], Batch [568/938], Loss: 0.5801\n",
      "Epoch [6/10], Batch [569/938], Loss: 0.5919\n",
      "Epoch [6/10], Batch [570/938], Loss: 0.5801\n",
      "Epoch [6/10], Batch [571/938], Loss: 0.6001\n",
      "Epoch [6/10], Batch [572/938], Loss: 0.6108\n",
      "Epoch [6/10], Batch [573/938], Loss: 0.5824\n",
      "Epoch [6/10], Batch [574/938], Loss: 0.6150\n",
      "Epoch [6/10], Batch [575/938], Loss: 0.5978\n",
      "Epoch [6/10], Batch [576/938], Loss: 0.6036\n",
      "Epoch [6/10], Batch [577/938], Loss: 0.5727\n",
      "Epoch [6/10], Batch [578/938], Loss: 0.6014\n",
      "Epoch [6/10], Batch [579/938], Loss: 0.6004\n",
      "Epoch [6/10], Batch [580/938], Loss: 0.5932\n",
      "Epoch [6/10], Batch [581/938], Loss: 0.5653\n",
      "Epoch [6/10], Batch [582/938], Loss: 0.6071\n",
      "Epoch [6/10], Batch [583/938], Loss: 0.5981\n",
      "Epoch [6/10], Batch [584/938], Loss: 0.6017\n",
      "Epoch [6/10], Batch [585/938], Loss: 0.6137\n",
      "Epoch [6/10], Batch [586/938], Loss: 0.6048\n",
      "Epoch [6/10], Batch [587/938], Loss: 0.5735\n",
      "Epoch [6/10], Batch [588/938], Loss: 0.6115\n",
      "Epoch [6/10], Batch [589/938], Loss: 0.5947\n",
      "Epoch [6/10], Batch [590/938], Loss: 0.6244\n",
      "Epoch [6/10], Batch [591/938], Loss: 0.5710\n",
      "Epoch [6/10], Batch [592/938], Loss: 0.6125\n",
      "Epoch [6/10], Batch [593/938], Loss: 0.6022\n",
      "Epoch [6/10], Batch [594/938], Loss: 0.5939\n",
      "Epoch [6/10], Batch [595/938], Loss: 0.6097\n",
      "Epoch [6/10], Batch [596/938], Loss: 0.5958\n",
      "Epoch [6/10], Batch [597/938], Loss: 0.6021\n",
      "Epoch [6/10], Batch [598/938], Loss: 0.5849\n",
      "Epoch [6/10], Batch [599/938], Loss: 0.5711\n",
      "Epoch [6/10], Batch [600/938], Loss: 0.5738\n",
      "Epoch [6/10], Batch [601/938], Loss: 0.5987\n",
      "Epoch [6/10], Batch [602/938], Loss: 0.5858\n",
      "Epoch [6/10], Batch [603/938], Loss: 0.5907\n",
      "Epoch [6/10], Batch [604/938], Loss: 0.6052\n",
      "Epoch [6/10], Batch [605/938], Loss: 0.5820\n",
      "Epoch [6/10], Batch [606/938], Loss: 0.5818\n",
      "Epoch [6/10], Batch [607/938], Loss: 0.6113\n",
      "Epoch [6/10], Batch [608/938], Loss: 0.5860\n",
      "Epoch [6/10], Batch [609/938], Loss: 0.5889\n",
      "Epoch [6/10], Batch [610/938], Loss: 0.6056\n",
      "Epoch [6/10], Batch [611/938], Loss: 0.6008\n",
      "Epoch [6/10], Batch [612/938], Loss: 0.5753\n",
      "Epoch [6/10], Batch [613/938], Loss: 0.5990\n",
      "Epoch [6/10], Batch [614/938], Loss: 0.6067\n",
      "Epoch [6/10], Batch [615/938], Loss: 0.6069\n",
      "Epoch [6/10], Batch [616/938], Loss: 0.5822\n",
      "Epoch [6/10], Batch [617/938], Loss: 0.5984\n",
      "Epoch [6/10], Batch [618/938], Loss: 0.5804\n",
      "Epoch [6/10], Batch [619/938], Loss: 0.5988\n",
      "Epoch [6/10], Batch [620/938], Loss: 0.6159\n",
      "Epoch [6/10], Batch [621/938], Loss: 0.5911\n",
      "Epoch [6/10], Batch [622/938], Loss: 0.5903\n",
      "Epoch [6/10], Batch [623/938], Loss: 0.5920\n",
      "Epoch [6/10], Batch [624/938], Loss: 0.5998\n",
      "Epoch [6/10], Batch [625/938], Loss: 0.5761\n",
      "Epoch [6/10], Batch [626/938], Loss: 0.5981\n",
      "Epoch [6/10], Batch [627/938], Loss: 0.5603\n",
      "Epoch [6/10], Batch [628/938], Loss: 0.5698\n",
      "Epoch [6/10], Batch [629/938], Loss: 0.5823\n",
      "Epoch [6/10], Batch [630/938], Loss: 0.5884\n",
      "Epoch [6/10], Batch [631/938], Loss: 0.5900\n",
      "Epoch [6/10], Batch [632/938], Loss: 0.5941\n",
      "Epoch [6/10], Batch [633/938], Loss: 0.6239\n",
      "Epoch [6/10], Batch [634/938], Loss: 0.5948\n",
      "Epoch [6/10], Batch [635/938], Loss: 0.5918\n",
      "Epoch [6/10], Batch [636/938], Loss: 0.5656\n",
      "Epoch [6/10], Batch [637/938], Loss: 0.5928\n",
      "Epoch [6/10], Batch [638/938], Loss: 0.5814\n",
      "Epoch [6/10], Batch [639/938], Loss: 0.5852\n",
      "Epoch [6/10], Batch [640/938], Loss: 0.6005\n",
      "Epoch [6/10], Batch [641/938], Loss: 0.5717\n",
      "Epoch [6/10], Batch [642/938], Loss: 0.5851\n",
      "Epoch [6/10], Batch [643/938], Loss: 0.5791\n",
      "Epoch [6/10], Batch [644/938], Loss: 0.5891\n",
      "Epoch [6/10], Batch [645/938], Loss: 0.5712\n",
      "Epoch [6/10], Batch [646/938], Loss: 0.5954\n",
      "Epoch [6/10], Batch [647/938], Loss: 0.5993\n",
      "Epoch [6/10], Batch [648/938], Loss: 0.5792\n",
      "Epoch [6/10], Batch [649/938], Loss: 0.6062\n",
      "Epoch [6/10], Batch [650/938], Loss: 0.5910\n",
      "Epoch [6/10], Batch [651/938], Loss: 0.6086\n",
      "Epoch [6/10], Batch [652/938], Loss: 0.5632\n",
      "Epoch [6/10], Batch [653/938], Loss: 0.5913\n",
      "Epoch [6/10], Batch [654/938], Loss: 0.5840\n",
      "Epoch [6/10], Batch [655/938], Loss: 0.5894\n",
      "Epoch [6/10], Batch [656/938], Loss: 0.6039\n",
      "Epoch [6/10], Batch [657/938], Loss: 0.6103\n",
      "Epoch [6/10], Batch [658/938], Loss: 0.5848\n",
      "Epoch [6/10], Batch [659/938], Loss: 0.5937\n",
      "Epoch [6/10], Batch [660/938], Loss: 0.5742\n",
      "Epoch [6/10], Batch [661/938], Loss: 0.5965\n",
      "Epoch [6/10], Batch [662/938], Loss: 0.6007\n",
      "Epoch [6/10], Batch [663/938], Loss: 0.6010\n",
      "Epoch [6/10], Batch [664/938], Loss: 0.5992\n",
      "Epoch [6/10], Batch [665/938], Loss: 0.5623\n",
      "Epoch [6/10], Batch [666/938], Loss: 0.5886\n",
      "Epoch [6/10], Batch [667/938], Loss: 0.6101\n",
      "Epoch [6/10], Batch [668/938], Loss: 0.6001\n",
      "Epoch [6/10], Batch [669/938], Loss: 0.5615\n",
      "Epoch [6/10], Batch [670/938], Loss: 0.5838\n",
      "Epoch [6/10], Batch [671/938], Loss: 0.6013\n",
      "Epoch [6/10], Batch [672/938], Loss: 0.5829\n",
      "Epoch [6/10], Batch [673/938], Loss: 0.5750\n",
      "Epoch [6/10], Batch [674/938], Loss: 0.5878\n",
      "Epoch [6/10], Batch [675/938], Loss: 0.5881\n",
      "Epoch [6/10], Batch [676/938], Loss: 0.5908\n",
      "Epoch [6/10], Batch [677/938], Loss: 0.5785\n",
      "Epoch [6/10], Batch [678/938], Loss: 0.6073\n",
      "Epoch [6/10], Batch [679/938], Loss: 0.5837\n",
      "Epoch [6/10], Batch [680/938], Loss: 0.5794\n",
      "Epoch [6/10], Batch [681/938], Loss: 0.5771\n",
      "Epoch [6/10], Batch [682/938], Loss: 0.6262\n",
      "Epoch [6/10], Batch [683/938], Loss: 0.6030\n",
      "Epoch [6/10], Batch [684/938], Loss: 0.5931\n",
      "Epoch [6/10], Batch [685/938], Loss: 0.6005\n",
      "Epoch [6/10], Batch [686/938], Loss: 0.5989\n",
      "Epoch [6/10], Batch [687/938], Loss: 0.6151\n",
      "Epoch [6/10], Batch [688/938], Loss: 0.6012\n",
      "Epoch [6/10], Batch [689/938], Loss: 0.6336\n",
      "Epoch [6/10], Batch [690/938], Loss: 0.6028\n",
      "Epoch [6/10], Batch [691/938], Loss: 0.5891\n",
      "Epoch [6/10], Batch [692/938], Loss: 0.6089\n",
      "Epoch [6/10], Batch [693/938], Loss: 0.5823\n",
      "Epoch [6/10], Batch [694/938], Loss: 0.5742\n",
      "Epoch [6/10], Batch [695/938], Loss: 0.5972\n",
      "Epoch [6/10], Batch [696/938], Loss: 0.5832\n",
      "Epoch [6/10], Batch [697/938], Loss: 0.6062\n",
      "Epoch [6/10], Batch [698/938], Loss: 0.5886\n",
      "Epoch [6/10], Batch [699/938], Loss: 0.6069\n",
      "Epoch [6/10], Batch [700/938], Loss: 0.5960\n",
      "Epoch [6/10], Batch [701/938], Loss: 0.6097\n",
      "Epoch [6/10], Batch [702/938], Loss: 0.5797\n",
      "Epoch [6/10], Batch [703/938], Loss: 0.5840\n",
      "Epoch [6/10], Batch [704/938], Loss: 0.5773\n",
      "Epoch [6/10], Batch [705/938], Loss: 0.5608\n",
      "Epoch [6/10], Batch [706/938], Loss: 0.5839\n",
      "Epoch [6/10], Batch [707/938], Loss: 0.5709\n",
      "Epoch [6/10], Batch [708/938], Loss: 0.5862\n",
      "Epoch [6/10], Batch [709/938], Loss: 0.6161\n",
      "Epoch [6/10], Batch [710/938], Loss: 0.5981\n",
      "Epoch [6/10], Batch [711/938], Loss: 0.6019\n",
      "Epoch [6/10], Batch [712/938], Loss: 0.5894\n",
      "Epoch [6/10], Batch [713/938], Loss: 0.5764\n",
      "Epoch [6/10], Batch [714/938], Loss: 0.6069\n",
      "Epoch [6/10], Batch [715/938], Loss: 0.5998\n",
      "Epoch [6/10], Batch [716/938], Loss: 0.5953\n",
      "Epoch [6/10], Batch [717/938], Loss: 0.5810\n",
      "Epoch [6/10], Batch [718/938], Loss: 0.6004\n",
      "Epoch [6/10], Batch [719/938], Loss: 0.5734\n",
      "Epoch [6/10], Batch [720/938], Loss: 0.5909\n",
      "Epoch [6/10], Batch [721/938], Loss: 0.6211\n",
      "Epoch [6/10], Batch [722/938], Loss: 0.5769\n",
      "Epoch [6/10], Batch [723/938], Loss: 0.6067\n",
      "Epoch [6/10], Batch [724/938], Loss: 0.5831\n",
      "Epoch [6/10], Batch [725/938], Loss: 0.5478\n",
      "Epoch [6/10], Batch [726/938], Loss: 0.5845\n",
      "Epoch [6/10], Batch [727/938], Loss: 0.5817\n",
      "Epoch [6/10], Batch [728/938], Loss: 0.5842\n",
      "Epoch [6/10], Batch [729/938], Loss: 0.6006\n",
      "Epoch [6/10], Batch [730/938], Loss: 0.5581\n",
      "Epoch [6/10], Batch [731/938], Loss: 0.5812\n",
      "Epoch [6/10], Batch [732/938], Loss: 0.5864\n",
      "Epoch [6/10], Batch [733/938], Loss: 0.6203\n",
      "Epoch [6/10], Batch [734/938], Loss: 0.5784\n",
      "Epoch [6/10], Batch [735/938], Loss: 0.6065\n",
      "Epoch [6/10], Batch [736/938], Loss: 0.5729\n",
      "Epoch [6/10], Batch [737/938], Loss: 0.5996\n",
      "Epoch [6/10], Batch [738/938], Loss: 0.5615\n",
      "Epoch [6/10], Batch [739/938], Loss: 0.5875\n",
      "Epoch [6/10], Batch [740/938], Loss: 0.6022\n",
      "Epoch [6/10], Batch [741/938], Loss: 0.5981\n",
      "Epoch [6/10], Batch [742/938], Loss: 0.5554\n",
      "Epoch [6/10], Batch [743/938], Loss: 0.5996\n",
      "Epoch [6/10], Batch [744/938], Loss: 0.5983\n",
      "Epoch [6/10], Batch [745/938], Loss: 0.5696\n",
      "Epoch [6/10], Batch [746/938], Loss: 0.6074\n",
      "Epoch [6/10], Batch [747/938], Loss: 0.6053\n",
      "Epoch [6/10], Batch [748/938], Loss: 0.6204\n",
      "Epoch [6/10], Batch [749/938], Loss: 0.5942\n",
      "Epoch [6/10], Batch [750/938], Loss: 0.5947\n",
      "Epoch [6/10], Batch [751/938], Loss: 0.5652\n",
      "Epoch [6/10], Batch [752/938], Loss: 0.5939\n",
      "Epoch [6/10], Batch [753/938], Loss: 0.5670\n",
      "Epoch [6/10], Batch [754/938], Loss: 0.5967\n",
      "Epoch [6/10], Batch [755/938], Loss: 0.5836\n",
      "Epoch [6/10], Batch [756/938], Loss: 0.5701\n",
      "Epoch [6/10], Batch [757/938], Loss: 0.5550\n",
      "Epoch [6/10], Batch [758/938], Loss: 0.6046\n",
      "Epoch [6/10], Batch [759/938], Loss: 0.5845\n",
      "Epoch [6/10], Batch [760/938], Loss: 0.5647\n",
      "Epoch [6/10], Batch [761/938], Loss: 0.5960\n",
      "Epoch [6/10], Batch [762/938], Loss: 0.5956\n",
      "Epoch [6/10], Batch [763/938], Loss: 0.5912\n",
      "Epoch [6/10], Batch [764/938], Loss: 0.5970\n",
      "Epoch [6/10], Batch [765/938], Loss: 0.6009\n",
      "Epoch [6/10], Batch [766/938], Loss: 0.5976\n",
      "Epoch [6/10], Batch [767/938], Loss: 0.5899\n",
      "Epoch [6/10], Batch [768/938], Loss: 0.6063\n",
      "Epoch [6/10], Batch [769/938], Loss: 0.6028\n",
      "Epoch [6/10], Batch [770/938], Loss: 0.5819\n",
      "Epoch [6/10], Batch [771/938], Loss: 0.6205\n",
      "Epoch [6/10], Batch [772/938], Loss: 0.5991\n",
      "Epoch [6/10], Batch [773/938], Loss: 0.5863\n",
      "Epoch [6/10], Batch [774/938], Loss: 0.5884\n",
      "Epoch [6/10], Batch [775/938], Loss: 0.6147\n",
      "Epoch [6/10], Batch [776/938], Loss: 0.5675\n",
      "Epoch [6/10], Batch [777/938], Loss: 0.5989\n",
      "Epoch [6/10], Batch [778/938], Loss: 0.6060\n",
      "Epoch [6/10], Batch [779/938], Loss: 0.6268\n",
      "Epoch [6/10], Batch [780/938], Loss: 0.5776\n",
      "Epoch [6/10], Batch [781/938], Loss: 0.6107\n",
      "Epoch [6/10], Batch [782/938], Loss: 0.6046\n",
      "Epoch [6/10], Batch [783/938], Loss: 0.6004\n",
      "Epoch [6/10], Batch [784/938], Loss: 0.6135\n",
      "Epoch [6/10], Batch [785/938], Loss: 0.5798\n",
      "Epoch [6/10], Batch [786/938], Loss: 0.5874\n",
      "Epoch [6/10], Batch [787/938], Loss: 0.5933\n",
      "Epoch [6/10], Batch [788/938], Loss: 0.5796\n",
      "Epoch [6/10], Batch [789/938], Loss: 0.5976\n",
      "Epoch [6/10], Batch [790/938], Loss: 0.5629\n",
      "Epoch [6/10], Batch [791/938], Loss: 0.5845\n",
      "Epoch [6/10], Batch [792/938], Loss: 0.5900\n",
      "Epoch [6/10], Batch [793/938], Loss: 0.5854\n",
      "Epoch [6/10], Batch [794/938], Loss: 0.6155\n",
      "Epoch [6/10], Batch [795/938], Loss: 0.5917\n",
      "Epoch [6/10], Batch [796/938], Loss: 0.5850\n",
      "Epoch [6/10], Batch [797/938], Loss: 0.6410\n",
      "Epoch [6/10], Batch [798/938], Loss: 0.6150\n",
      "Epoch [6/10], Batch [799/938], Loss: 0.6001\n",
      "Epoch [6/10], Batch [800/938], Loss: 0.6146\n",
      "Epoch [6/10], Batch [801/938], Loss: 0.5765\n",
      "Epoch [6/10], Batch [802/938], Loss: 0.5816\n",
      "Epoch [6/10], Batch [803/938], Loss: 0.6073\n",
      "Epoch [6/10], Batch [804/938], Loss: 0.5902\n",
      "Epoch [6/10], Batch [805/938], Loss: 0.6128\n",
      "Epoch [6/10], Batch [806/938], Loss: 0.5897\n",
      "Epoch [6/10], Batch [807/938], Loss: 0.5837\n",
      "Epoch [6/10], Batch [808/938], Loss: 0.5861\n",
      "Epoch [6/10], Batch [809/938], Loss: 0.5787\n",
      "Epoch [6/10], Batch [810/938], Loss: 0.5748\n",
      "Epoch [6/10], Batch [811/938], Loss: 0.5869\n",
      "Epoch [6/10], Batch [812/938], Loss: 0.5561\n",
      "Epoch [6/10], Batch [813/938], Loss: 0.6010\n",
      "Epoch [6/10], Batch [814/938], Loss: 0.6048\n",
      "Epoch [6/10], Batch [815/938], Loss: 0.5947\n",
      "Epoch [6/10], Batch [816/938], Loss: 0.6005\n",
      "Epoch [6/10], Batch [817/938], Loss: 0.6028\n",
      "Epoch [6/10], Batch [818/938], Loss: 0.5940\n",
      "Epoch [6/10], Batch [819/938], Loss: 0.5867\n",
      "Epoch [6/10], Batch [820/938], Loss: 0.6045\n",
      "Epoch [6/10], Batch [821/938], Loss: 0.6071\n",
      "Epoch [6/10], Batch [822/938], Loss: 0.5742\n",
      "Epoch [6/10], Batch [823/938], Loss: 0.6063\n",
      "Epoch [6/10], Batch [824/938], Loss: 0.5879\n",
      "Epoch [6/10], Batch [825/938], Loss: 0.5724\n",
      "Epoch [6/10], Batch [826/938], Loss: 0.5950\n",
      "Epoch [6/10], Batch [827/938], Loss: 0.5661\n",
      "Epoch [6/10], Batch [828/938], Loss: 0.5851\n",
      "Epoch [6/10], Batch [829/938], Loss: 0.5784\n",
      "Epoch [6/10], Batch [830/938], Loss: 0.5992\n",
      "Epoch [6/10], Batch [831/938], Loss: 0.5888\n",
      "Epoch [6/10], Batch [832/938], Loss: 0.5908\n",
      "Epoch [6/10], Batch [833/938], Loss: 0.5867\n",
      "Epoch [6/10], Batch [834/938], Loss: 0.6058\n",
      "Epoch [6/10], Batch [835/938], Loss: 0.5878\n",
      "Epoch [6/10], Batch [836/938], Loss: 0.6161\n",
      "Epoch [6/10], Batch [837/938], Loss: 0.6176\n",
      "Epoch [6/10], Batch [838/938], Loss: 0.5961\n",
      "Epoch [6/10], Batch [839/938], Loss: 0.5765\n",
      "Epoch [6/10], Batch [840/938], Loss: 0.6032\n",
      "Epoch [6/10], Batch [841/938], Loss: 0.5838\n",
      "Epoch [6/10], Batch [842/938], Loss: 0.5918\n",
      "Epoch [6/10], Batch [843/938], Loss: 0.5718\n",
      "Epoch [6/10], Batch [844/938], Loss: 0.6094\n",
      "Epoch [6/10], Batch [845/938], Loss: 0.5923\n",
      "Epoch [6/10], Batch [846/938], Loss: 0.5965\n",
      "Epoch [6/10], Batch [847/938], Loss: 0.5707\n",
      "Epoch [6/10], Batch [848/938], Loss: 0.5787\n",
      "Epoch [6/10], Batch [849/938], Loss: 0.6207\n",
      "Epoch [6/10], Batch [850/938], Loss: 0.5866\n",
      "Epoch [6/10], Batch [851/938], Loss: 0.6137\n",
      "Epoch [6/10], Batch [852/938], Loss: 0.5879\n",
      "Epoch [6/10], Batch [853/938], Loss: 0.5900\n",
      "Epoch [6/10], Batch [854/938], Loss: 0.6004\n",
      "Epoch [6/10], Batch [855/938], Loss: 0.6014\n",
      "Epoch [6/10], Batch [856/938], Loss: 0.5761\n",
      "Epoch [6/10], Batch [857/938], Loss: 0.6163\n",
      "Epoch [6/10], Batch [858/938], Loss: 0.5885\n",
      "Epoch [6/10], Batch [859/938], Loss: 0.5959\n",
      "Epoch [6/10], Batch [860/938], Loss: 0.6244\n",
      "Epoch [6/10], Batch [861/938], Loss: 0.6070\n",
      "Epoch [6/10], Batch [862/938], Loss: 0.5580\n",
      "Epoch [6/10], Batch [863/938], Loss: 0.5825\n",
      "Epoch [6/10], Batch [864/938], Loss: 0.6125\n",
      "Epoch [6/10], Batch [865/938], Loss: 0.6099\n",
      "Epoch [6/10], Batch [866/938], Loss: 0.5981\n",
      "Epoch [6/10], Batch [867/938], Loss: 0.6038\n",
      "Epoch [6/10], Batch [868/938], Loss: 0.5939\n",
      "Epoch [6/10], Batch [869/938], Loss: 0.5897\n",
      "Epoch [6/10], Batch [870/938], Loss: 0.5821\n",
      "Epoch [6/10], Batch [871/938], Loss: 0.5785\n",
      "Epoch [6/10], Batch [872/938], Loss: 0.5787\n",
      "Epoch [6/10], Batch [873/938], Loss: 0.5817\n",
      "Epoch [6/10], Batch [874/938], Loss: 0.6174\n",
      "Epoch [6/10], Batch [875/938], Loss: 0.5817\n",
      "Epoch [6/10], Batch [876/938], Loss: 0.5944\n",
      "Epoch [6/10], Batch [877/938], Loss: 0.5954\n",
      "Epoch [6/10], Batch [878/938], Loss: 0.5736\n",
      "Epoch [6/10], Batch [879/938], Loss: 0.5994\n",
      "Epoch [6/10], Batch [880/938], Loss: 0.6038\n",
      "Epoch [6/10], Batch [881/938], Loss: 0.5918\n",
      "Epoch [6/10], Batch [882/938], Loss: 0.6252\n",
      "Epoch [6/10], Batch [883/938], Loss: 0.6019\n",
      "Epoch [6/10], Batch [884/938], Loss: 0.5607\n",
      "Epoch [6/10], Batch [885/938], Loss: 0.5863\n",
      "Epoch [6/10], Batch [886/938], Loss: 0.6134\n",
      "Epoch [6/10], Batch [887/938], Loss: 0.5863\n",
      "Epoch [6/10], Batch [888/938], Loss: 0.5996\n",
      "Epoch [6/10], Batch [889/938], Loss: 0.6049\n",
      "Epoch [6/10], Batch [890/938], Loss: 0.5779\n",
      "Epoch [6/10], Batch [891/938], Loss: 0.5471\n",
      "Epoch [6/10], Batch [892/938], Loss: 0.6103\n",
      "Epoch [6/10], Batch [893/938], Loss: 0.6147\n",
      "Epoch [6/10], Batch [894/938], Loss: 0.5981\n",
      "Epoch [6/10], Batch [895/938], Loss: 0.5904\n",
      "Epoch [6/10], Batch [896/938], Loss: 0.5737\n",
      "Epoch [6/10], Batch [897/938], Loss: 0.6300\n",
      "Epoch [6/10], Batch [898/938], Loss: 0.6104\n",
      "Epoch [6/10], Batch [899/938], Loss: 0.5873\n",
      "Epoch [6/10], Batch [900/938], Loss: 0.5865\n",
      "Epoch [6/10], Batch [901/938], Loss: 0.6000\n",
      "Epoch [6/10], Batch [902/938], Loss: 0.5923\n",
      "Epoch [6/10], Batch [903/938], Loss: 0.5895\n",
      "Epoch [6/10], Batch [904/938], Loss: 0.5656\n",
      "Epoch [6/10], Batch [905/938], Loss: 0.6321\n",
      "Epoch [6/10], Batch [906/938], Loss: 0.5867\n",
      "Epoch [6/10], Batch [907/938], Loss: 0.5693\n",
      "Epoch [6/10], Batch [908/938], Loss: 0.5718\n",
      "Epoch [6/10], Batch [909/938], Loss: 0.5688\n",
      "Epoch [6/10], Batch [910/938], Loss: 0.5747\n",
      "Epoch [6/10], Batch [911/938], Loss: 0.6124\n",
      "Epoch [6/10], Batch [912/938], Loss: 0.5641\n",
      "Epoch [6/10], Batch [913/938], Loss: 0.5895\n",
      "Epoch [6/10], Batch [914/938], Loss: 0.5693\n",
      "Epoch [6/10], Batch [915/938], Loss: 0.5926\n",
      "Epoch [6/10], Batch [916/938], Loss: 0.6184\n",
      "Epoch [6/10], Batch [917/938], Loss: 0.5847\n",
      "Epoch [6/10], Batch [918/938], Loss: 0.5531\n",
      "Epoch [6/10], Batch [919/938], Loss: 0.6150\n",
      "Epoch [6/10], Batch [920/938], Loss: 0.6085\n",
      "Epoch [6/10], Batch [921/938], Loss: 0.6116\n",
      "Epoch [6/10], Batch [922/938], Loss: 0.6098\n",
      "Epoch [6/10], Batch [923/938], Loss: 0.5914\n",
      "Epoch [6/10], Batch [924/938], Loss: 0.6127\n",
      "Epoch [6/10], Batch [925/938], Loss: 0.5970\n",
      "Epoch [6/10], Batch [926/938], Loss: 0.6116\n",
      "Epoch [6/10], Batch [927/938], Loss: 0.5637\n",
      "Epoch [6/10], Batch [928/938], Loss: 0.5797\n",
      "Epoch [6/10], Batch [929/938], Loss: 0.6050\n",
      "Epoch [6/10], Batch [930/938], Loss: 0.5990\n",
      "Epoch [6/10], Batch [931/938], Loss: 0.6007\n",
      "Epoch [6/10], Batch [932/938], Loss: 0.5739\n",
      "Epoch [6/10], Batch [933/938], Loss: 0.5818\n",
      "Epoch [6/10], Batch [934/938], Loss: 0.6032\n",
      "Epoch [6/10], Batch [935/938], Loss: 0.5748\n",
      "Epoch [6/10], Batch [936/938], Loss: 0.5799\n",
      "Epoch [6/10], Batch [937/938], Loss: 0.5986\n",
      "Epoch [6/10], Batch [938/938], Loss: 0.5471\n",
      "Epoch [6/10], Loss: 0.5471\n",
      "Epoch [7/10], Batch [1/938], Loss: 0.5933\n",
      "Epoch [7/10], Batch [2/938], Loss: 0.6057\n",
      "Epoch [7/10], Batch [3/938], Loss: 0.5682\n",
      "Epoch [7/10], Batch [4/938], Loss: 0.6009\n",
      "Epoch [7/10], Batch [5/938], Loss: 0.5745\n",
      "Epoch [7/10], Batch [6/938], Loss: 0.5878\n",
      "Epoch [7/10], Batch [7/938], Loss: 0.5935\n",
      "Epoch [7/10], Batch [8/938], Loss: 0.5868\n",
      "Epoch [7/10], Batch [9/938], Loss: 0.5770\n",
      "Epoch [7/10], Batch [10/938], Loss: 0.5895\n",
      "Epoch [7/10], Batch [11/938], Loss: 0.5894\n",
      "Epoch [7/10], Batch [12/938], Loss: 0.5898\n",
      "Epoch [7/10], Batch [13/938], Loss: 0.6088\n",
      "Epoch [7/10], Batch [14/938], Loss: 0.5704\n",
      "Epoch [7/10], Batch [15/938], Loss: 0.5838\n",
      "Epoch [7/10], Batch [16/938], Loss: 0.5734\n",
      "Epoch [7/10], Batch [17/938], Loss: 0.5903\n",
      "Epoch [7/10], Batch [18/938], Loss: 0.5869\n",
      "Epoch [7/10], Batch [19/938], Loss: 0.5947\n",
      "Epoch [7/10], Batch [20/938], Loss: 0.5953\n",
      "Epoch [7/10], Batch [21/938], Loss: 0.6025\n",
      "Epoch [7/10], Batch [22/938], Loss: 0.6152\n",
      "Epoch [7/10], Batch [23/938], Loss: 0.5941\n",
      "Epoch [7/10], Batch [24/938], Loss: 0.5833\n",
      "Epoch [7/10], Batch [25/938], Loss: 0.5893\n",
      "Epoch [7/10], Batch [26/938], Loss: 0.5864\n",
      "Epoch [7/10], Batch [27/938], Loss: 0.5939\n",
      "Epoch [7/10], Batch [28/938], Loss: 0.5903\n",
      "Epoch [7/10], Batch [29/938], Loss: 0.5734\n",
      "Epoch [7/10], Batch [30/938], Loss: 0.6200\n",
      "Epoch [7/10], Batch [31/938], Loss: 0.5792\n",
      "Epoch [7/10], Batch [32/938], Loss: 0.5992\n",
      "Epoch [7/10], Batch [33/938], Loss: 0.5830\n",
      "Epoch [7/10], Batch [34/938], Loss: 0.5732\n",
      "Epoch [7/10], Batch [35/938], Loss: 0.5747\n",
      "Epoch [7/10], Batch [36/938], Loss: 0.5868\n",
      "Epoch [7/10], Batch [37/938], Loss: 0.6165\n",
      "Epoch [7/10], Batch [38/938], Loss: 0.5857\n",
      "Epoch [7/10], Batch [39/938], Loss: 0.6040\n",
      "Epoch [7/10], Batch [40/938], Loss: 0.5618\n",
      "Epoch [7/10], Batch [41/938], Loss: 0.6017\n",
      "Epoch [7/10], Batch [42/938], Loss: 0.5635\n",
      "Epoch [7/10], Batch [43/938], Loss: 0.5843\n",
      "Epoch [7/10], Batch [44/938], Loss: 0.6090\n",
      "Epoch [7/10], Batch [45/938], Loss: 0.5870\n",
      "Epoch [7/10], Batch [46/938], Loss: 0.6133\n",
      "Epoch [7/10], Batch [47/938], Loss: 0.5946\n",
      "Epoch [7/10], Batch [48/938], Loss: 0.5897\n",
      "Epoch [7/10], Batch [49/938], Loss: 0.5936\n",
      "Epoch [7/10], Batch [50/938], Loss: 0.5957\n",
      "Epoch [7/10], Batch [51/938], Loss: 0.5983\n",
      "Epoch [7/10], Batch [52/938], Loss: 0.5800\n",
      "Epoch [7/10], Batch [53/938], Loss: 0.6011\n",
      "Epoch [7/10], Batch [54/938], Loss: 0.5614\n",
      "Epoch [7/10], Batch [55/938], Loss: 0.5886\n",
      "Epoch [7/10], Batch [56/938], Loss: 0.5933\n",
      "Epoch [7/10], Batch [57/938], Loss: 0.5586\n",
      "Epoch [7/10], Batch [58/938], Loss: 0.5904\n",
      "Epoch [7/10], Batch [59/938], Loss: 0.5903\n",
      "Epoch [7/10], Batch [60/938], Loss: 0.5948\n",
      "Epoch [7/10], Batch [61/938], Loss: 0.6061\n",
      "Epoch [7/10], Batch [62/938], Loss: 0.6066\n",
      "Epoch [7/10], Batch [63/938], Loss: 0.6070\n",
      "Epoch [7/10], Batch [64/938], Loss: 0.5716\n",
      "Epoch [7/10], Batch [65/938], Loss: 0.5710\n",
      "Epoch [7/10], Batch [66/938], Loss: 0.6010\n",
      "Epoch [7/10], Batch [67/938], Loss: 0.5872\n",
      "Epoch [7/10], Batch [68/938], Loss: 0.5999\n",
      "Epoch [7/10], Batch [69/938], Loss: 0.5837\n",
      "Epoch [7/10], Batch [70/938], Loss: 0.5967\n",
      "Epoch [7/10], Batch [71/938], Loss: 0.5851\n",
      "Epoch [7/10], Batch [72/938], Loss: 0.5853\n",
      "Epoch [7/10], Batch [73/938], Loss: 0.5751\n",
      "Epoch [7/10], Batch [74/938], Loss: 0.5852\n",
      "Epoch [7/10], Batch [75/938], Loss: 0.6003\n",
      "Epoch [7/10], Batch [76/938], Loss: 0.5985\n",
      "Epoch [7/10], Batch [77/938], Loss: 0.5819\n",
      "Epoch [7/10], Batch [78/938], Loss: 0.6155\n",
      "Epoch [7/10], Batch [79/938], Loss: 0.5848\n",
      "Epoch [7/10], Batch [80/938], Loss: 0.5917\n",
      "Epoch [7/10], Batch [81/938], Loss: 0.5934\n",
      "Epoch [7/10], Batch [82/938], Loss: 0.5627\n",
      "Epoch [7/10], Batch [83/938], Loss: 0.6236\n",
      "Epoch [7/10], Batch [84/938], Loss: 0.5841\n",
      "Epoch [7/10], Batch [85/938], Loss: 0.5908\n",
      "Epoch [7/10], Batch [86/938], Loss: 0.5820\n",
      "Epoch [7/10], Batch [87/938], Loss: 0.5721\n",
      "Epoch [7/10], Batch [88/938], Loss: 0.5904\n",
      "Epoch [7/10], Batch [89/938], Loss: 0.5888\n",
      "Epoch [7/10], Batch [90/938], Loss: 0.6133\n",
      "Epoch [7/10], Batch [91/938], Loss: 0.5976\n",
      "Epoch [7/10], Batch [92/938], Loss: 0.5898\n",
      "Epoch [7/10], Batch [93/938], Loss: 0.5907\n",
      "Epoch [7/10], Batch [94/938], Loss: 0.5832\n",
      "Epoch [7/10], Batch [95/938], Loss: 0.5843\n",
      "Epoch [7/10], Batch [96/938], Loss: 0.5880\n",
      "Epoch [7/10], Batch [97/938], Loss: 0.6052\n",
      "Epoch [7/10], Batch [98/938], Loss: 0.6164\n",
      "Epoch [7/10], Batch [99/938], Loss: 0.5863\n",
      "Epoch [7/10], Batch [100/938], Loss: 0.5817\n",
      "Epoch [7/10], Batch [101/938], Loss: 0.5813\n",
      "Epoch [7/10], Batch [102/938], Loss: 0.5881\n",
      "Epoch [7/10], Batch [103/938], Loss: 0.5820\n",
      "Epoch [7/10], Batch [104/938], Loss: 0.6238\n",
      "Epoch [7/10], Batch [105/938], Loss: 0.5941\n",
      "Epoch [7/10], Batch [106/938], Loss: 0.6215\n",
      "Epoch [7/10], Batch [107/938], Loss: 0.5522\n",
      "Epoch [7/10], Batch [108/938], Loss: 0.6031\n",
      "Epoch [7/10], Batch [109/938], Loss: 0.5880\n",
      "Epoch [7/10], Batch [110/938], Loss: 0.5976\n",
      "Epoch [7/10], Batch [111/938], Loss: 0.5975\n",
      "Epoch [7/10], Batch [112/938], Loss: 0.5801\n",
      "Epoch [7/10], Batch [113/938], Loss: 0.6020\n",
      "Epoch [7/10], Batch [114/938], Loss: 0.5988\n",
      "Epoch [7/10], Batch [115/938], Loss: 0.5733\n",
      "Epoch [7/10], Batch [116/938], Loss: 0.6005\n",
      "Epoch [7/10], Batch [117/938], Loss: 0.5867\n",
      "Epoch [7/10], Batch [118/938], Loss: 0.5951\n",
      "Epoch [7/10], Batch [119/938], Loss: 0.5948\n",
      "Epoch [7/10], Batch [120/938], Loss: 0.6213\n",
      "Epoch [7/10], Batch [121/938], Loss: 0.5901\n",
      "Epoch [7/10], Batch [122/938], Loss: 0.5982\n",
      "Epoch [7/10], Batch [123/938], Loss: 0.5750\n",
      "Epoch [7/10], Batch [124/938], Loss: 0.5725\n",
      "Epoch [7/10], Batch [125/938], Loss: 0.5829\n",
      "Epoch [7/10], Batch [126/938], Loss: 0.6114\n",
      "Epoch [7/10], Batch [127/938], Loss: 0.6092\n",
      "Epoch [7/10], Batch [128/938], Loss: 0.5502\n",
      "Epoch [7/10], Batch [129/938], Loss: 0.5844\n",
      "Epoch [7/10], Batch [130/938], Loss: 0.5839\n",
      "Epoch [7/10], Batch [131/938], Loss: 0.5772\n",
      "Epoch [7/10], Batch [132/938], Loss: 0.5735\n",
      "Epoch [7/10], Batch [133/938], Loss: 0.5991\n",
      "Epoch [7/10], Batch [134/938], Loss: 0.6158\n",
      "Epoch [7/10], Batch [135/938], Loss: 0.5748\n",
      "Epoch [7/10], Batch [136/938], Loss: 0.5723\n",
      "Epoch [7/10], Batch [137/938], Loss: 0.5847\n",
      "Epoch [7/10], Batch [138/938], Loss: 0.5759\n",
      "Epoch [7/10], Batch [139/938], Loss: 0.5726\n",
      "Epoch [7/10], Batch [140/938], Loss: 0.5892\n",
      "Epoch [7/10], Batch [141/938], Loss: 0.6000\n",
      "Epoch [7/10], Batch [142/938], Loss: 0.5939\n",
      "Epoch [7/10], Batch [143/938], Loss: 0.5943\n",
      "Epoch [7/10], Batch [144/938], Loss: 0.6006\n",
      "Epoch [7/10], Batch [145/938], Loss: 0.5895\n",
      "Epoch [7/10], Batch [146/938], Loss: 0.5977\n",
      "Epoch [7/10], Batch [147/938], Loss: 0.6300\n",
      "Epoch [7/10], Batch [148/938], Loss: 0.6411\n",
      "Epoch [7/10], Batch [149/938], Loss: 0.5915\n",
      "Epoch [7/10], Batch [150/938], Loss: 0.5888\n",
      "Epoch [7/10], Batch [151/938], Loss: 0.5848\n",
      "Epoch [7/10], Batch [152/938], Loss: 0.5985\n",
      "Epoch [7/10], Batch [153/938], Loss: 0.5839\n",
      "Epoch [7/10], Batch [154/938], Loss: 0.6101\n",
      "Epoch [7/10], Batch [155/938], Loss: 0.6149\n",
      "Epoch [7/10], Batch [156/938], Loss: 0.5919\n",
      "Epoch [7/10], Batch [157/938], Loss: 0.5708\n",
      "Epoch [7/10], Batch [158/938], Loss: 0.5945\n",
      "Epoch [7/10], Batch [159/938], Loss: 0.6222\n",
      "Epoch [7/10], Batch [160/938], Loss: 0.5849\n",
      "Epoch [7/10], Batch [161/938], Loss: 0.6197\n",
      "Epoch [7/10], Batch [162/938], Loss: 0.5926\n",
      "Epoch [7/10], Batch [163/938], Loss: 0.5902\n",
      "Epoch [7/10], Batch [164/938], Loss: 0.5909\n",
      "Epoch [7/10], Batch [165/938], Loss: 0.5928\n",
      "Epoch [7/10], Batch [166/938], Loss: 0.5624\n",
      "Epoch [7/10], Batch [167/938], Loss: 0.5911\n",
      "Epoch [7/10], Batch [168/938], Loss: 0.5980\n",
      "Epoch [7/10], Batch [169/938], Loss: 0.5801\n",
      "Epoch [7/10], Batch [170/938], Loss: 0.5986\n",
      "Epoch [7/10], Batch [171/938], Loss: 0.5786\n",
      "Epoch [7/10], Batch [172/938], Loss: 0.5922\n",
      "Epoch [7/10], Batch [173/938], Loss: 0.5909\n",
      "Epoch [7/10], Batch [174/938], Loss: 0.6091\n",
      "Epoch [7/10], Batch [175/938], Loss: 0.5658\n",
      "Epoch [7/10], Batch [176/938], Loss: 0.5855\n",
      "Epoch [7/10], Batch [177/938], Loss: 0.5769\n",
      "Epoch [7/10], Batch [178/938], Loss: 0.5985\n",
      "Epoch [7/10], Batch [179/938], Loss: 0.6035\n",
      "Epoch [7/10], Batch [180/938], Loss: 0.6172\n",
      "Epoch [7/10], Batch [181/938], Loss: 0.5871\n",
      "Epoch [7/10], Batch [182/938], Loss: 0.5993\n",
      "Epoch [7/10], Batch [183/938], Loss: 0.5985\n",
      "Epoch [7/10], Batch [184/938], Loss: 0.5864\n",
      "Epoch [7/10], Batch [185/938], Loss: 0.5703\n",
      "Epoch [7/10], Batch [186/938], Loss: 0.5835\n",
      "Epoch [7/10], Batch [187/938], Loss: 0.5906\n",
      "Epoch [7/10], Batch [188/938], Loss: 0.5947\n",
      "Epoch [7/10], Batch [189/938], Loss: 0.5877\n",
      "Epoch [7/10], Batch [190/938], Loss: 0.5917\n",
      "Epoch [7/10], Batch [191/938], Loss: 0.5821\n",
      "Epoch [7/10], Batch [192/938], Loss: 0.5880\n",
      "Epoch [7/10], Batch [193/938], Loss: 0.5946\n",
      "Epoch [7/10], Batch [194/938], Loss: 0.5648\n",
      "Epoch [7/10], Batch [195/938], Loss: 0.6163\n",
      "Epoch [7/10], Batch [196/938], Loss: 0.5934\n",
      "Epoch [7/10], Batch [197/938], Loss: 0.6070\n",
      "Epoch [7/10], Batch [198/938], Loss: 0.5913\n",
      "Epoch [7/10], Batch [199/938], Loss: 0.5868\n",
      "Epoch [7/10], Batch [200/938], Loss: 0.6130\n",
      "Epoch [7/10], Batch [201/938], Loss: 0.6053\n",
      "Epoch [7/10], Batch [202/938], Loss: 0.6022\n",
      "Epoch [7/10], Batch [203/938], Loss: 0.5691\n",
      "Epoch [7/10], Batch [204/938], Loss: 0.5633\n",
      "Epoch [7/10], Batch [205/938], Loss: 0.6132\n",
      "Epoch [7/10], Batch [206/938], Loss: 0.6028\n",
      "Epoch [7/10], Batch [207/938], Loss: 0.5954\n",
      "Epoch [7/10], Batch [208/938], Loss: 0.5606\n",
      "Epoch [7/10], Batch [209/938], Loss: 0.5882\n",
      "Epoch [7/10], Batch [210/938], Loss: 0.6073\n",
      "Epoch [7/10], Batch [211/938], Loss: 0.5926\n",
      "Epoch [7/10], Batch [212/938], Loss: 0.5760\n",
      "Epoch [7/10], Batch [213/938], Loss: 0.5677\n",
      "Epoch [7/10], Batch [214/938], Loss: 0.5969\n",
      "Epoch [7/10], Batch [215/938], Loss: 0.6006\n",
      "Epoch [7/10], Batch [216/938], Loss: 0.6175\n",
      "Epoch [7/10], Batch [217/938], Loss: 0.5842\n",
      "Epoch [7/10], Batch [218/938], Loss: 0.6098\n",
      "Epoch [7/10], Batch [219/938], Loss: 0.6134\n",
      "Epoch [7/10], Batch [220/938], Loss: 0.6002\n",
      "Epoch [7/10], Batch [221/938], Loss: 0.6090\n",
      "Epoch [7/10], Batch [222/938], Loss: 0.6083\n",
      "Epoch [7/10], Batch [223/938], Loss: 0.5869\n",
      "Epoch [7/10], Batch [224/938], Loss: 0.6105\n",
      "Epoch [7/10], Batch [225/938], Loss: 0.5898\n",
      "Epoch [7/10], Batch [226/938], Loss: 0.5939\n",
      "Epoch [7/10], Batch [227/938], Loss: 0.6120\n",
      "Epoch [7/10], Batch [228/938], Loss: 0.5857\n",
      "Epoch [7/10], Batch [229/938], Loss: 0.5645\n",
      "Epoch [7/10], Batch [230/938], Loss: 0.6128\n",
      "Epoch [7/10], Batch [231/938], Loss: 0.5905\n",
      "Epoch [7/10], Batch [232/938], Loss: 0.5763\n",
      "Epoch [7/10], Batch [233/938], Loss: 0.5530\n",
      "Epoch [7/10], Batch [234/938], Loss: 0.5875\n",
      "Epoch [7/10], Batch [235/938], Loss: 0.5875\n",
      "Epoch [7/10], Batch [236/938], Loss: 0.5840\n",
      "Epoch [7/10], Batch [237/938], Loss: 0.5874\n",
      "Epoch [7/10], Batch [238/938], Loss: 0.5772\n",
      "Epoch [7/10], Batch [239/938], Loss: 0.5822\n",
      "Epoch [7/10], Batch [240/938], Loss: 0.5966\n",
      "Epoch [7/10], Batch [241/938], Loss: 0.5744\n",
      "Epoch [7/10], Batch [242/938], Loss: 0.5678\n",
      "Epoch [7/10], Batch [243/938], Loss: 0.6241\n",
      "Epoch [7/10], Batch [244/938], Loss: 0.6086\n",
      "Epoch [7/10], Batch [245/938], Loss: 0.6029\n",
      "Epoch [7/10], Batch [246/938], Loss: 0.5660\n",
      "Epoch [7/10], Batch [247/938], Loss: 0.5882\n",
      "Epoch [7/10], Batch [248/938], Loss: 0.6094\n",
      "Epoch [7/10], Batch [249/938], Loss: 0.6015\n",
      "Epoch [7/10], Batch [250/938], Loss: 0.6112\n",
      "Epoch [7/10], Batch [251/938], Loss: 0.5707\n",
      "Epoch [7/10], Batch [252/938], Loss: 0.5718\n",
      "Epoch [7/10], Batch [253/938], Loss: 0.5893\n",
      "Epoch [7/10], Batch [254/938], Loss: 0.5851\n",
      "Epoch [7/10], Batch [255/938], Loss: 0.5849\n",
      "Epoch [7/10], Batch [256/938], Loss: 0.5847\n",
      "Epoch [7/10], Batch [257/938], Loss: 0.5816\n",
      "Epoch [7/10], Batch [258/938], Loss: 0.5930\n",
      "Epoch [7/10], Batch [259/938], Loss: 0.5977\n",
      "Epoch [7/10], Batch [260/938], Loss: 0.5675\n",
      "Epoch [7/10], Batch [261/938], Loss: 0.6089\n",
      "Epoch [7/10], Batch [262/938], Loss: 0.5912\n",
      "Epoch [7/10], Batch [263/938], Loss: 0.5722\n",
      "Epoch [7/10], Batch [264/938], Loss: 0.5991\n",
      "Epoch [7/10], Batch [265/938], Loss: 0.5912\n",
      "Epoch [7/10], Batch [266/938], Loss: 0.5858\n",
      "Epoch [7/10], Batch [267/938], Loss: 0.6123\n",
      "Epoch [7/10], Batch [268/938], Loss: 0.5700\n",
      "Epoch [7/10], Batch [269/938], Loss: 0.6099\n",
      "Epoch [7/10], Batch [270/938], Loss: 0.6030\n",
      "Epoch [7/10], Batch [271/938], Loss: 0.5937\n",
      "Epoch [7/10], Batch [272/938], Loss: 0.5815\n",
      "Epoch [7/10], Batch [273/938], Loss: 0.6029\n",
      "Epoch [7/10], Batch [274/938], Loss: 0.5685\n",
      "Epoch [7/10], Batch [275/938], Loss: 0.6006\n",
      "Epoch [7/10], Batch [276/938], Loss: 0.5846\n",
      "Epoch [7/10], Batch [277/938], Loss: 0.6149\n",
      "Epoch [7/10], Batch [278/938], Loss: 0.5743\n",
      "Epoch [7/10], Batch [279/938], Loss: 0.6229\n",
      "Epoch [7/10], Batch [280/938], Loss: 0.6012\n",
      "Epoch [7/10], Batch [281/938], Loss: 0.5687\n",
      "Epoch [7/10], Batch [282/938], Loss: 0.5998\n",
      "Epoch [7/10], Batch [283/938], Loss: 0.6038\n",
      "Epoch [7/10], Batch [284/938], Loss: 0.6174\n",
      "Epoch [7/10], Batch [285/938], Loss: 0.6158\n",
      "Epoch [7/10], Batch [286/938], Loss: 0.6023\n",
      "Epoch [7/10], Batch [287/938], Loss: 0.5661\n",
      "Epoch [7/10], Batch [288/938], Loss: 0.5971\n",
      "Epoch [7/10], Batch [289/938], Loss: 0.5947\n",
      "Epoch [7/10], Batch [290/938], Loss: 0.5840\n",
      "Epoch [7/10], Batch [291/938], Loss: 0.5772\n",
      "Epoch [7/10], Batch [292/938], Loss: 0.5851\n",
      "Epoch [7/10], Batch [293/938], Loss: 0.5734\n",
      "Epoch [7/10], Batch [294/938], Loss: 0.5775\n",
      "Epoch [7/10], Batch [295/938], Loss: 0.6000\n",
      "Epoch [7/10], Batch [296/938], Loss: 0.6085\n",
      "Epoch [7/10], Batch [297/938], Loss: 0.5732\n",
      "Epoch [7/10], Batch [298/938], Loss: 0.5925\n",
      "Epoch [7/10], Batch [299/938], Loss: 0.5785\n",
      "Epoch [7/10], Batch [300/938], Loss: 0.5891\n",
      "Epoch [7/10], Batch [301/938], Loss: 0.5816\n",
      "Epoch [7/10], Batch [302/938], Loss: 0.5618\n",
      "Epoch [7/10], Batch [303/938], Loss: 0.5901\n",
      "Epoch [7/10], Batch [304/938], Loss: 0.5997\n",
      "Epoch [7/10], Batch [305/938], Loss: 0.5823\n",
      "Epoch [7/10], Batch [306/938], Loss: 0.5780\n",
      "Epoch [7/10], Batch [307/938], Loss: 0.5686\n",
      "Epoch [7/10], Batch [308/938], Loss: 0.5902\n",
      "Epoch [7/10], Batch [309/938], Loss: 0.5777\n",
      "Epoch [7/10], Batch [310/938], Loss: 0.5847\n",
      "Epoch [7/10], Batch [311/938], Loss: 0.5720\n",
      "Epoch [7/10], Batch [312/938], Loss: 0.5796\n",
      "Epoch [7/10], Batch [313/938], Loss: 0.5918\n",
      "Epoch [7/10], Batch [314/938], Loss: 0.5989\n",
      "Epoch [7/10], Batch [315/938], Loss: 0.5941\n",
      "Epoch [7/10], Batch [316/938], Loss: 0.6037\n",
      "Epoch [7/10], Batch [317/938], Loss: 0.5760\n",
      "Epoch [7/10], Batch [318/938], Loss: 0.6353\n",
      "Epoch [7/10], Batch [319/938], Loss: 0.5809\n",
      "Epoch [7/10], Batch [320/938], Loss: 0.6017\n",
      "Epoch [7/10], Batch [321/938], Loss: 0.5900\n",
      "Epoch [7/10], Batch [322/938], Loss: 0.5928\n",
      "Epoch [7/10], Batch [323/938], Loss: 0.5787\n",
      "Epoch [7/10], Batch [324/938], Loss: 0.5853\n",
      "Epoch [7/10], Batch [325/938], Loss: 0.5836\n",
      "Epoch [7/10], Batch [326/938], Loss: 0.5940\n",
      "Epoch [7/10], Batch [327/938], Loss: 0.5899\n",
      "Epoch [7/10], Batch [328/938], Loss: 0.5715\n",
      "Epoch [7/10], Batch [329/938], Loss: 0.6226\n",
      "Epoch [7/10], Batch [330/938], Loss: 0.5804\n",
      "Epoch [7/10], Batch [331/938], Loss: 0.5881\n",
      "Epoch [7/10], Batch [332/938], Loss: 0.5778\n",
      "Epoch [7/10], Batch [333/938], Loss: 0.5684\n",
      "Epoch [7/10], Batch [334/938], Loss: 0.6006\n",
      "Epoch [7/10], Batch [335/938], Loss: 0.6017\n",
      "Epoch [7/10], Batch [336/938], Loss: 0.6036\n",
      "Epoch [7/10], Batch [337/938], Loss: 0.6168\n",
      "Epoch [7/10], Batch [338/938], Loss: 0.5928\n",
      "Epoch [7/10], Batch [339/938], Loss: 0.5786\n",
      "Epoch [7/10], Batch [340/938], Loss: 0.5765\n",
      "Epoch [7/10], Batch [341/938], Loss: 0.6145\n",
      "Epoch [7/10], Batch [342/938], Loss: 0.5689\n",
      "Epoch [7/10], Batch [343/938], Loss: 0.5747\n",
      "Epoch [7/10], Batch [344/938], Loss: 0.5931\n",
      "Epoch [7/10], Batch [345/938], Loss: 0.6189\n",
      "Epoch [7/10], Batch [346/938], Loss: 0.5761\n",
      "Epoch [7/10], Batch [347/938], Loss: 0.5991\n",
      "Epoch [7/10], Batch [348/938], Loss: 0.6188\n",
      "Epoch [7/10], Batch [349/938], Loss: 0.5722\n",
      "Epoch [7/10], Batch [350/938], Loss: 0.5871\n",
      "Epoch [7/10], Batch [351/938], Loss: 0.5701\n",
      "Epoch [7/10], Batch [352/938], Loss: 0.6149\n",
      "Epoch [7/10], Batch [353/938], Loss: 0.6211\n",
      "Epoch [7/10], Batch [354/938], Loss: 0.6279\n",
      "Epoch [7/10], Batch [355/938], Loss: 0.5943\n",
      "Epoch [7/10], Batch [356/938], Loss: 0.5634\n",
      "Epoch [7/10], Batch [357/938], Loss: 0.5964\n",
      "Epoch [7/10], Batch [358/938], Loss: 0.6104\n",
      "Epoch [7/10], Batch [359/938], Loss: 0.5888\n",
      "Epoch [7/10], Batch [360/938], Loss: 0.5915\n",
      "Epoch [7/10], Batch [361/938], Loss: 0.5956\n",
      "Epoch [7/10], Batch [362/938], Loss: 0.5799\n",
      "Epoch [7/10], Batch [363/938], Loss: 0.5698\n",
      "Epoch [7/10], Batch [364/938], Loss: 0.6149\n",
      "Epoch [7/10], Batch [365/938], Loss: 0.6233\n",
      "Epoch [7/10], Batch [366/938], Loss: 0.5610\n",
      "Epoch [7/10], Batch [367/938], Loss: 0.6091\n",
      "Epoch [7/10], Batch [368/938], Loss: 0.6029\n",
      "Epoch [7/10], Batch [369/938], Loss: 0.5890\n",
      "Epoch [7/10], Batch [370/938], Loss: 0.6092\n",
      "Epoch [7/10], Batch [371/938], Loss: 0.5643\n",
      "Epoch [7/10], Batch [372/938], Loss: 0.6626\n",
      "Epoch [7/10], Batch [373/938], Loss: 0.5857\n",
      "Epoch [7/10], Batch [374/938], Loss: 0.5809\n",
      "Epoch [7/10], Batch [375/938], Loss: 0.6078\n",
      "Epoch [7/10], Batch [376/938], Loss: 0.5910\n",
      "Epoch [7/10], Batch [377/938], Loss: 0.5967\n",
      "Epoch [7/10], Batch [378/938], Loss: 0.6044\n",
      "Epoch [7/10], Batch [379/938], Loss: 0.5786\n",
      "Epoch [7/10], Batch [380/938], Loss: 0.5790\n",
      "Epoch [7/10], Batch [381/938], Loss: 0.5724\n",
      "Epoch [7/10], Batch [382/938], Loss: 0.6154\n",
      "Epoch [7/10], Batch [383/938], Loss: 0.5814\n",
      "Epoch [7/10], Batch [384/938], Loss: 0.5864\n",
      "Epoch [7/10], Batch [385/938], Loss: 0.5687\n",
      "Epoch [7/10], Batch [386/938], Loss: 0.5905\n",
      "Epoch [7/10], Batch [387/938], Loss: 0.5753\n",
      "Epoch [7/10], Batch [388/938], Loss: 0.5768\n",
      "Epoch [7/10], Batch [389/938], Loss: 0.5862\n",
      "Epoch [7/10], Batch [390/938], Loss: 0.5886\n",
      "Epoch [7/10], Batch [391/938], Loss: 0.5850\n",
      "Epoch [7/10], Batch [392/938], Loss: 0.5748\n",
      "Epoch [7/10], Batch [393/938], Loss: 0.5886\n",
      "Epoch [7/10], Batch [394/938], Loss: 0.6338\n",
      "Epoch [7/10], Batch [395/938], Loss: 0.5992\n",
      "Epoch [7/10], Batch [396/938], Loss: 0.5878\n",
      "Epoch [7/10], Batch [397/938], Loss: 0.6159\n",
      "Epoch [7/10], Batch [398/938], Loss: 0.5793\n",
      "Epoch [7/10], Batch [399/938], Loss: 0.5948\n",
      "Epoch [7/10], Batch [400/938], Loss: 0.6281\n",
      "Epoch [7/10], Batch [401/938], Loss: 0.5689\n",
      "Epoch [7/10], Batch [402/938], Loss: 0.6157\n",
      "Epoch [7/10], Batch [403/938], Loss: 0.5807\n",
      "Epoch [7/10], Batch [404/938], Loss: 0.5854\n",
      "Epoch [7/10], Batch [405/938], Loss: 0.5912\n",
      "Epoch [7/10], Batch [406/938], Loss: 0.5998\n",
      "Epoch [7/10], Batch [407/938], Loss: 0.5935\n",
      "Epoch [7/10], Batch [408/938], Loss: 0.5932\n",
      "Epoch [7/10], Batch [409/938], Loss: 0.5735\n",
      "Epoch [7/10], Batch [410/938], Loss: 0.6066\n",
      "Epoch [7/10], Batch [411/938], Loss: 0.6445\n",
      "Epoch [7/10], Batch [412/938], Loss: 0.5942\n",
      "Epoch [7/10], Batch [413/938], Loss: 0.5691\n",
      "Epoch [7/10], Batch [414/938], Loss: 0.5773\n",
      "Epoch [7/10], Batch [415/938], Loss: 0.6149\n",
      "Epoch [7/10], Batch [416/938], Loss: 0.6087\n",
      "Epoch [7/10], Batch [417/938], Loss: 0.6198\n",
      "Epoch [7/10], Batch [418/938], Loss: 0.5871\n",
      "Epoch [7/10], Batch [419/938], Loss: 0.5942\n",
      "Epoch [7/10], Batch [420/938], Loss: 0.5856\n",
      "Epoch [7/10], Batch [421/938], Loss: 0.5857\n",
      "Epoch [7/10], Batch [422/938], Loss: 0.5958\n",
      "Epoch [7/10], Batch [423/938], Loss: 0.5635\n",
      "Epoch [7/10], Batch [424/938], Loss: 0.5859\n",
      "Epoch [7/10], Batch [425/938], Loss: 0.5986\n",
      "Epoch [7/10], Batch [426/938], Loss: 0.6153\n",
      "Epoch [7/10], Batch [427/938], Loss: 0.6150\n",
      "Epoch [7/10], Batch [428/938], Loss: 0.5702\n",
      "Epoch [7/10], Batch [429/938], Loss: 0.5666\n",
      "Epoch [7/10], Batch [430/938], Loss: 0.6093\n",
      "Epoch [7/10], Batch [431/938], Loss: 0.5893\n",
      "Epoch [7/10], Batch [432/938], Loss: 0.6048\n",
      "Epoch [7/10], Batch [433/938], Loss: 0.6466\n",
      "Epoch [7/10], Batch [434/938], Loss: 0.5959\n",
      "Epoch [7/10], Batch [435/938], Loss: 0.5820\n",
      "Epoch [7/10], Batch [436/938], Loss: 0.5863\n",
      "Epoch [7/10], Batch [437/938], Loss: 0.6218\n",
      "Epoch [7/10], Batch [438/938], Loss: 0.5713\n",
      "Epoch [7/10], Batch [439/938], Loss: 0.5830\n",
      "Epoch [7/10], Batch [440/938], Loss: 0.5913\n",
      "Epoch [7/10], Batch [441/938], Loss: 0.5738\n",
      "Epoch [7/10], Batch [442/938], Loss: 0.5877\n",
      "Epoch [7/10], Batch [443/938], Loss: 0.5704\n",
      "Epoch [7/10], Batch [444/938], Loss: 0.6041\n",
      "Epoch [7/10], Batch [445/938], Loss: 0.6280\n",
      "Epoch [7/10], Batch [446/938], Loss: 0.6185\n",
      "Epoch [7/10], Batch [447/938], Loss: 0.6405\n",
      "Epoch [7/10], Batch [448/938], Loss: 0.5823\n",
      "Epoch [7/10], Batch [449/938], Loss: 0.5913\n",
      "Epoch [7/10], Batch [450/938], Loss: 0.5956\n",
      "Epoch [7/10], Batch [451/938], Loss: 0.5771\n",
      "Epoch [7/10], Batch [452/938], Loss: 0.5665\n",
      "Epoch [7/10], Batch [453/938], Loss: 0.5884\n",
      "Epoch [7/10], Batch [454/938], Loss: 0.5911\n",
      "Epoch [7/10], Batch [455/938], Loss: 0.6084\n",
      "Epoch [7/10], Batch [456/938], Loss: 0.5882\n",
      "Epoch [7/10], Batch [457/938], Loss: 0.6235\n",
      "Epoch [7/10], Batch [458/938], Loss: 0.6178\n",
      "Epoch [7/10], Batch [459/938], Loss: 0.5918\n",
      "Epoch [7/10], Batch [460/938], Loss: 0.6129\n",
      "Epoch [7/10], Batch [461/938], Loss: 0.5654\n",
      "Epoch [7/10], Batch [462/938], Loss: 0.5880\n",
      "Epoch [7/10], Batch [463/938], Loss: 0.6160\n",
      "Epoch [7/10], Batch [464/938], Loss: 0.5815\n",
      "Epoch [7/10], Batch [465/938], Loss: 0.5784\n",
      "Epoch [7/10], Batch [466/938], Loss: 0.6086\n",
      "Epoch [7/10], Batch [467/938], Loss: 0.6106\n",
      "Epoch [7/10], Batch [468/938], Loss: 0.5937\n",
      "Epoch [7/10], Batch [469/938], Loss: 0.5825\n",
      "Epoch [7/10], Batch [470/938], Loss: 0.5755\n",
      "Epoch [7/10], Batch [471/938], Loss: 0.5699\n",
      "Epoch [7/10], Batch [472/938], Loss: 0.6002\n",
      "Epoch [7/10], Batch [473/938], Loss: 0.5914\n",
      "Epoch [7/10], Batch [474/938], Loss: 0.5715\n",
      "Epoch [7/10], Batch [475/938], Loss: 0.5974\n",
      "Epoch [7/10], Batch [476/938], Loss: 0.5730\n",
      "Epoch [7/10], Batch [477/938], Loss: 0.5629\n",
      "Epoch [7/10], Batch [478/938], Loss: 0.6074\n",
      "Epoch [7/10], Batch [479/938], Loss: 0.6088\n",
      "Epoch [7/10], Batch [480/938], Loss: 0.5961\n",
      "Epoch [7/10], Batch [481/938], Loss: 0.5631\n",
      "Epoch [7/10], Batch [482/938], Loss: 0.5825\n",
      "Epoch [7/10], Batch [483/938], Loss: 0.6025\n",
      "Epoch [7/10], Batch [484/938], Loss: 0.5954\n",
      "Epoch [7/10], Batch [485/938], Loss: 0.6164\n",
      "Epoch [7/10], Batch [486/938], Loss: 0.5774\n",
      "Epoch [7/10], Batch [487/938], Loss: 0.5699\n",
      "Epoch [7/10], Batch [488/938], Loss: 0.5779\n",
      "Epoch [7/10], Batch [489/938], Loss: 0.5712\n",
      "Epoch [7/10], Batch [490/938], Loss: 0.5995\n",
      "Epoch [7/10], Batch [491/938], Loss: 0.5735\n",
      "Epoch [7/10], Batch [492/938], Loss: 0.5839\n",
      "Epoch [7/10], Batch [493/938], Loss: 0.5974\n",
      "Epoch [7/10], Batch [494/938], Loss: 0.6069\n",
      "Epoch [7/10], Batch [495/938], Loss: 0.6047\n",
      "Epoch [7/10], Batch [496/938], Loss: 0.5910\n",
      "Epoch [7/10], Batch [497/938], Loss: 0.5994\n",
      "Epoch [7/10], Batch [498/938], Loss: 0.5577\n",
      "Epoch [7/10], Batch [499/938], Loss: 0.5890\n",
      "Epoch [7/10], Batch [500/938], Loss: 0.5931\n",
      "Epoch [7/10], Batch [501/938], Loss: 0.5786\n",
      "Epoch [7/10], Batch [502/938], Loss: 0.6040\n",
      "Epoch [7/10], Batch [503/938], Loss: 0.6194\n",
      "Epoch [7/10], Batch [504/938], Loss: 0.5686\n",
      "Epoch [7/10], Batch [505/938], Loss: 0.6026\n",
      "Epoch [7/10], Batch [506/938], Loss: 0.5750\n",
      "Epoch [7/10], Batch [507/938], Loss: 0.5988\n",
      "Epoch [7/10], Batch [508/938], Loss: 0.6084\n",
      "Epoch [7/10], Batch [509/938], Loss: 0.5627\n",
      "Epoch [7/10], Batch [510/938], Loss: 0.5909\n",
      "Epoch [7/10], Batch [511/938], Loss: 0.6073\n",
      "Epoch [7/10], Batch [512/938], Loss: 0.5846\n",
      "Epoch [7/10], Batch [513/938], Loss: 0.6018\n",
      "Epoch [7/10], Batch [514/938], Loss: 0.5711\n",
      "Epoch [7/10], Batch [515/938], Loss: 0.5747\n",
      "Epoch [7/10], Batch [516/938], Loss: 0.5921\n",
      "Epoch [7/10], Batch [517/938], Loss: 0.5984\n",
      "Epoch [7/10], Batch [518/938], Loss: 0.5975\n",
      "Epoch [7/10], Batch [519/938], Loss: 0.5919\n",
      "Epoch [7/10], Batch [520/938], Loss: 0.6027\n",
      "Epoch [7/10], Batch [521/938], Loss: 0.5961\n",
      "Epoch [7/10], Batch [522/938], Loss: 0.5997\n",
      "Epoch [7/10], Batch [523/938], Loss: 0.5457\n",
      "Epoch [7/10], Batch [524/938], Loss: 0.5797\n",
      "Epoch [7/10], Batch [525/938], Loss: 0.5899\n",
      "Epoch [7/10], Batch [526/938], Loss: 0.5979\n",
      "Epoch [7/10], Batch [527/938], Loss: 0.5700\n",
      "Epoch [7/10], Batch [528/938], Loss: 0.5914\n",
      "Epoch [7/10], Batch [529/938], Loss: 0.5771\n",
      "Epoch [7/10], Batch [530/938], Loss: 0.5978\n",
      "Epoch [7/10], Batch [531/938], Loss: 0.5933\n",
      "Epoch [7/10], Batch [532/938], Loss: 0.6062\n",
      "Epoch [7/10], Batch [533/938], Loss: 0.5751\n",
      "Epoch [7/10], Batch [534/938], Loss: 0.6167\n",
      "Epoch [7/10], Batch [535/938], Loss: 0.5836\n",
      "Epoch [7/10], Batch [536/938], Loss: 0.5995\n",
      "Epoch [7/10], Batch [537/938], Loss: 0.5714\n",
      "Epoch [7/10], Batch [538/938], Loss: 0.5928\n",
      "Epoch [7/10], Batch [539/938], Loss: 0.6139\n",
      "Epoch [7/10], Batch [540/938], Loss: 0.6081\n",
      "Epoch [7/10], Batch [541/938], Loss: 0.5738\n",
      "Epoch [7/10], Batch [542/938], Loss: 0.5733\n",
      "Epoch [7/10], Batch [543/938], Loss: 0.5682\n",
      "Epoch [7/10], Batch [544/938], Loss: 0.5810\n",
      "Epoch [7/10], Batch [545/938], Loss: 0.5769\n",
      "Epoch [7/10], Batch [546/938], Loss: 0.5786\n",
      "Epoch [7/10], Batch [547/938], Loss: 0.5639\n",
      "Epoch [7/10], Batch [548/938], Loss: 0.6020\n",
      "Epoch [7/10], Batch [549/938], Loss: 0.5869\n",
      "Epoch [7/10], Batch [550/938], Loss: 0.5766\n",
      "Epoch [7/10], Batch [551/938], Loss: 0.5820\n",
      "Epoch [7/10], Batch [552/938], Loss: 0.6003\n",
      "Epoch [7/10], Batch [553/938], Loss: 0.5696\n",
      "Epoch [7/10], Batch [554/938], Loss: 0.6024\n",
      "Epoch [7/10], Batch [555/938], Loss: 0.6120\n",
      "Epoch [7/10], Batch [556/938], Loss: 0.5963\n",
      "Epoch [7/10], Batch [557/938], Loss: 0.5900\n",
      "Epoch [7/10], Batch [558/938], Loss: 0.5786\n",
      "Epoch [7/10], Batch [559/938], Loss: 0.5995\n",
      "Epoch [7/10], Batch [560/938], Loss: 0.5958\n",
      "Epoch [7/10], Batch [561/938], Loss: 0.5906\n",
      "Epoch [7/10], Batch [562/938], Loss: 0.6035\n",
      "Epoch [7/10], Batch [563/938], Loss: 0.5927\n",
      "Epoch [7/10], Batch [564/938], Loss: 0.5924\n",
      "Epoch [7/10], Batch [565/938], Loss: 0.5701\n",
      "Epoch [7/10], Batch [566/938], Loss: 0.5865\n",
      "Epoch [7/10], Batch [567/938], Loss: 0.5957\n",
      "Epoch [7/10], Batch [568/938], Loss: 0.6033\n",
      "Epoch [7/10], Batch [569/938], Loss: 0.5829\n",
      "Epoch [7/10], Batch [570/938], Loss: 0.5866\n",
      "Epoch [7/10], Batch [571/938], Loss: 0.5979\n",
      "Epoch [7/10], Batch [572/938], Loss: 0.5914\n",
      "Epoch [7/10], Batch [573/938], Loss: 0.5947\n",
      "Epoch [7/10], Batch [574/938], Loss: 0.5798\n",
      "Epoch [7/10], Batch [575/938], Loss: 0.6150\n",
      "Epoch [7/10], Batch [576/938], Loss: 0.5914\n",
      "Epoch [7/10], Batch [577/938], Loss: 0.6092\n",
      "Epoch [7/10], Batch [578/938], Loss: 0.6063\n",
      "Epoch [7/10], Batch [579/938], Loss: 0.5878\n",
      "Epoch [7/10], Batch [580/938], Loss: 0.5984\n",
      "Epoch [7/10], Batch [581/938], Loss: 0.5928\n",
      "Epoch [7/10], Batch [582/938], Loss: 0.5858\n",
      "Epoch [7/10], Batch [583/938], Loss: 0.5882\n",
      "Epoch [7/10], Batch [584/938], Loss: 0.5613\n",
      "Epoch [7/10], Batch [585/938], Loss: 0.6162\n",
      "Epoch [7/10], Batch [586/938], Loss: 0.5752\n",
      "Epoch [7/10], Batch [587/938], Loss: 0.5886\n",
      "Epoch [7/10], Batch [588/938], Loss: 0.5893\n",
      "Epoch [7/10], Batch [589/938], Loss: 0.5832\n",
      "Epoch [7/10], Batch [590/938], Loss: 0.5763\n",
      "Epoch [7/10], Batch [591/938], Loss: 0.6061\n",
      "Epoch [7/10], Batch [592/938], Loss: 0.5678\n",
      "Epoch [7/10], Batch [593/938], Loss: 0.6318\n",
      "Epoch [7/10], Batch [594/938], Loss: 0.5922\n",
      "Epoch [7/10], Batch [595/938], Loss: 0.5715\n",
      "Epoch [7/10], Batch [596/938], Loss: 0.5889\n",
      "Epoch [7/10], Batch [597/938], Loss: 0.5798\n",
      "Epoch [7/10], Batch [598/938], Loss: 0.5832\n",
      "Epoch [7/10], Batch [599/938], Loss: 0.5660\n",
      "Epoch [7/10], Batch [600/938], Loss: 0.5727\n",
      "Epoch [7/10], Batch [601/938], Loss: 0.6054\n",
      "Epoch [7/10], Batch [602/938], Loss: 0.5821\n",
      "Epoch [7/10], Batch [603/938], Loss: 0.5993\n",
      "Epoch [7/10], Batch [604/938], Loss: 0.5790\n",
      "Epoch [7/10], Batch [605/938], Loss: 0.5830\n",
      "Epoch [7/10], Batch [606/938], Loss: 0.5872\n",
      "Epoch [7/10], Batch [607/938], Loss: 0.5675\n",
      "Epoch [7/10], Batch [608/938], Loss: 0.5732\n",
      "Epoch [7/10], Batch [609/938], Loss: 0.6012\n",
      "Epoch [7/10], Batch [610/938], Loss: 0.5616\n",
      "Epoch [7/10], Batch [611/938], Loss: 0.5777\n",
      "Epoch [7/10], Batch [612/938], Loss: 0.5855\n",
      "Epoch [7/10], Batch [613/938], Loss: 0.5872\n",
      "Epoch [7/10], Batch [614/938], Loss: 0.5739\n",
      "Epoch [7/10], Batch [615/938], Loss: 0.5850\n",
      "Epoch [7/10], Batch [616/938], Loss: 0.5814\n",
      "Epoch [7/10], Batch [617/938], Loss: 0.5813\n",
      "Epoch [7/10], Batch [618/938], Loss: 0.5677\n",
      "Epoch [7/10], Batch [619/938], Loss: 0.5927\n",
      "Epoch [7/10], Batch [620/938], Loss: 0.6005\n",
      "Epoch [7/10], Batch [621/938], Loss: 0.5871\n",
      "Epoch [7/10], Batch [622/938], Loss: 0.5820\n",
      "Epoch [7/10], Batch [623/938], Loss: 0.5983\n",
      "Epoch [7/10], Batch [624/938], Loss: 0.6077\n",
      "Epoch [7/10], Batch [625/938], Loss: 0.5882\n",
      "Epoch [7/10], Batch [626/938], Loss: 0.5949\n",
      "Epoch [7/10], Batch [627/938], Loss: 0.5868\n",
      "Epoch [7/10], Batch [628/938], Loss: 0.6125\n",
      "Epoch [7/10], Batch [629/938], Loss: 0.5663\n",
      "Epoch [7/10], Batch [630/938], Loss: 0.6009\n",
      "Epoch [7/10], Batch [631/938], Loss: 0.6308\n",
      "Epoch [7/10], Batch [632/938], Loss: 0.5781\n",
      "Epoch [7/10], Batch [633/938], Loss: 0.5796\n",
      "Epoch [7/10], Batch [634/938], Loss: 0.5985\n",
      "Epoch [7/10], Batch [635/938], Loss: 0.5881\n",
      "Epoch [7/10], Batch [636/938], Loss: 0.5837\n",
      "Epoch [7/10], Batch [637/938], Loss: 0.5744\n",
      "Epoch [7/10], Batch [638/938], Loss: 0.5897\n",
      "Epoch [7/10], Batch [639/938], Loss: 0.5788\n",
      "Epoch [7/10], Batch [640/938], Loss: 0.6012\n",
      "Epoch [7/10], Batch [641/938], Loss: 0.5699\n",
      "Epoch [7/10], Batch [642/938], Loss: 0.5655\n",
      "Epoch [7/10], Batch [643/938], Loss: 0.5974\n",
      "Epoch [7/10], Batch [644/938], Loss: 0.5949\n",
      "Epoch [7/10], Batch [645/938], Loss: 0.5797\n",
      "Epoch [7/10], Batch [646/938], Loss: 0.6405\n",
      "Epoch [7/10], Batch [647/938], Loss: 0.5763\n",
      "Epoch [7/10], Batch [648/938], Loss: 0.5648\n",
      "Epoch [7/10], Batch [649/938], Loss: 0.5671\n",
      "Epoch [7/10], Batch [650/938], Loss: 0.5965\n",
      "Epoch [7/10], Batch [651/938], Loss: 0.5952\n",
      "Epoch [7/10], Batch [652/938], Loss: 0.5879\n",
      "Epoch [7/10], Batch [653/938], Loss: 0.5955\n",
      "Epoch [7/10], Batch [654/938], Loss: 0.5837\n",
      "Epoch [7/10], Batch [655/938], Loss: 0.5798\n",
      "Epoch [7/10], Batch [656/938], Loss: 0.6367\n",
      "Epoch [7/10], Batch [657/938], Loss: 0.5980\n",
      "Epoch [7/10], Batch [658/938], Loss: 0.6121\n",
      "Epoch [7/10], Batch [659/938], Loss: 0.5877\n",
      "Epoch [7/10], Batch [660/938], Loss: 0.5860\n",
      "Epoch [7/10], Batch [661/938], Loss: 0.5768\n",
      "Epoch [7/10], Batch [662/938], Loss: 0.5958\n",
      "Epoch [7/10], Batch [663/938], Loss: 0.5692\n",
      "Epoch [7/10], Batch [664/938], Loss: 0.5655\n",
      "Epoch [7/10], Batch [665/938], Loss: 0.5842\n",
      "Epoch [7/10], Batch [666/938], Loss: 0.5666\n",
      "Epoch [7/10], Batch [667/938], Loss: 0.6061\n",
      "Epoch [7/10], Batch [668/938], Loss: 0.5891\n",
      "Epoch [7/10], Batch [669/938], Loss: 0.5638\n",
      "Epoch [7/10], Batch [670/938], Loss: 0.5846\n",
      "Epoch [7/10], Batch [671/938], Loss: 0.5974\n",
      "Epoch [7/10], Batch [672/938], Loss: 0.5636\n",
      "Epoch [7/10], Batch [673/938], Loss: 0.5930\n",
      "Epoch [7/10], Batch [674/938], Loss: 0.5730\n",
      "Epoch [7/10], Batch [675/938], Loss: 0.5817\n",
      "Epoch [7/10], Batch [676/938], Loss: 0.5969\n",
      "Epoch [7/10], Batch [677/938], Loss: 0.5731\n",
      "Epoch [7/10], Batch [678/938], Loss: 0.6125\n",
      "Epoch [7/10], Batch [679/938], Loss: 0.5806\n",
      "Epoch [7/10], Batch [680/938], Loss: 0.5990\n",
      "Epoch [7/10], Batch [681/938], Loss: 0.5842\n",
      "Epoch [7/10], Batch [682/938], Loss: 0.5777\n",
      "Epoch [7/10], Batch [683/938], Loss: 0.6078\n",
      "Epoch [7/10], Batch [684/938], Loss: 0.6066\n",
      "Epoch [7/10], Batch [685/938], Loss: 0.6077\n",
      "Epoch [7/10], Batch [686/938], Loss: 0.5744\n",
      "Epoch [7/10], Batch [687/938], Loss: 0.5540\n",
      "Epoch [7/10], Batch [688/938], Loss: 0.5666\n",
      "Epoch [7/10], Batch [689/938], Loss: 0.5821\n",
      "Epoch [7/10], Batch [690/938], Loss: 0.5830\n",
      "Epoch [7/10], Batch [691/938], Loss: 0.6012\n",
      "Epoch [7/10], Batch [692/938], Loss: 0.5637\n",
      "Epoch [7/10], Batch [693/938], Loss: 0.6000\n",
      "Epoch [7/10], Batch [694/938], Loss: 0.5916\n",
      "Epoch [7/10], Batch [695/938], Loss: 0.5856\n",
      "Epoch [7/10], Batch [696/938], Loss: 0.5882\n",
      "Epoch [7/10], Batch [697/938], Loss: 0.5907\n",
      "Epoch [7/10], Batch [698/938], Loss: 0.5838\n",
      "Epoch [7/10], Batch [699/938], Loss: 0.5735\n",
      "Epoch [7/10], Batch [700/938], Loss: 0.5819\n",
      "Epoch [7/10], Batch [701/938], Loss: 0.5958\n",
      "Epoch [7/10], Batch [702/938], Loss: 0.5961\n",
      "Epoch [7/10], Batch [703/938], Loss: 0.6001\n",
      "Epoch [7/10], Batch [704/938], Loss: 0.5878\n",
      "Epoch [7/10], Batch [705/938], Loss: 0.5825\n",
      "Epoch [7/10], Batch [706/938], Loss: 0.6055\n",
      "Epoch [7/10], Batch [707/938], Loss: 0.5566\n",
      "Epoch [7/10], Batch [708/938], Loss: 0.5951\n",
      "Epoch [7/10], Batch [709/938], Loss: 0.6115\n",
      "Epoch [7/10], Batch [710/938], Loss: 0.5989\n",
      "Epoch [7/10], Batch [711/938], Loss: 0.5923\n",
      "Epoch [7/10], Batch [712/938], Loss: 0.5741\n",
      "Epoch [7/10], Batch [713/938], Loss: 0.5702\n",
      "Epoch [7/10], Batch [714/938], Loss: 0.5937\n",
      "Epoch [7/10], Batch [715/938], Loss: 0.5969\n",
      "Epoch [7/10], Batch [716/938], Loss: 0.5795\n",
      "Epoch [7/10], Batch [717/938], Loss: 0.6125\n",
      "Epoch [7/10], Batch [718/938], Loss: 0.5757\n",
      "Epoch [7/10], Batch [719/938], Loss: 0.5835\n",
      "Epoch [7/10], Batch [720/938], Loss: 0.5863\n",
      "Epoch [7/10], Batch [721/938], Loss: 0.6072\n",
      "Epoch [7/10], Batch [722/938], Loss: 0.5697\n",
      "Epoch [7/10], Batch [723/938], Loss: 0.5832\n",
      "Epoch [7/10], Batch [724/938], Loss: 0.5690\n",
      "Epoch [7/10], Batch [725/938], Loss: 0.5915\n",
      "Epoch [7/10], Batch [726/938], Loss: 0.5853\n",
      "Epoch [7/10], Batch [727/938], Loss: 0.5649\n",
      "Epoch [7/10], Batch [728/938], Loss: 0.5694\n",
      "Epoch [7/10], Batch [729/938], Loss: 0.6386\n",
      "Epoch [7/10], Batch [730/938], Loss: 0.5697\n",
      "Epoch [7/10], Batch [731/938], Loss: 0.5816\n",
      "Epoch [7/10], Batch [732/938], Loss: 0.5746\n",
      "Epoch [7/10], Batch [733/938], Loss: 0.5811\n",
      "Epoch [7/10], Batch [734/938], Loss: 0.5738\n",
      "Epoch [7/10], Batch [735/938], Loss: 0.5909\n",
      "Epoch [7/10], Batch [736/938], Loss: 0.5898\n",
      "Epoch [7/10], Batch [737/938], Loss: 0.5873\n",
      "Epoch [7/10], Batch [738/938], Loss: 0.5921\n",
      "Epoch [7/10], Batch [739/938], Loss: 0.6030\n",
      "Epoch [7/10], Batch [740/938], Loss: 0.5718\n",
      "Epoch [7/10], Batch [741/938], Loss: 0.5933\n",
      "Epoch [7/10], Batch [742/938], Loss: 0.6046\n",
      "Epoch [7/10], Batch [743/938], Loss: 0.6113\n",
      "Epoch [7/10], Batch [744/938], Loss: 0.6031\n",
      "Epoch [7/10], Batch [745/938], Loss: 0.6097\n",
      "Epoch [7/10], Batch [746/938], Loss: 0.5837\n",
      "Epoch [7/10], Batch [747/938], Loss: 0.5902\n",
      "Epoch [7/10], Batch [748/938], Loss: 0.5849\n",
      "Epoch [7/10], Batch [749/938], Loss: 0.5850\n",
      "Epoch [7/10], Batch [750/938], Loss: 0.5857\n",
      "Epoch [7/10], Batch [751/938], Loss: 0.5884\n",
      "Epoch [7/10], Batch [752/938], Loss: 0.5883\n",
      "Epoch [7/10], Batch [753/938], Loss: 0.6070\n",
      "Epoch [7/10], Batch [754/938], Loss: 0.5747\n",
      "Epoch [7/10], Batch [755/938], Loss: 0.5794\n",
      "Epoch [7/10], Batch [756/938], Loss: 0.6017\n",
      "Epoch [7/10], Batch [757/938], Loss: 0.5934\n",
      "Epoch [7/10], Batch [758/938], Loss: 0.5959\n",
      "Epoch [7/10], Batch [759/938], Loss: 0.5714\n",
      "Epoch [7/10], Batch [760/938], Loss: 0.5853\n",
      "Epoch [7/10], Batch [761/938], Loss: 0.5811\n",
      "Epoch [7/10], Batch [762/938], Loss: 0.5943\n",
      "Epoch [7/10], Batch [763/938], Loss: 0.6028\n",
      "Epoch [7/10], Batch [764/938], Loss: 0.5657\n",
      "Epoch [7/10], Batch [765/938], Loss: 0.5984\n",
      "Epoch [7/10], Batch [766/938], Loss: 0.5652\n",
      "Epoch [7/10], Batch [767/938], Loss: 0.5771\n",
      "Epoch [7/10], Batch [768/938], Loss: 0.5952\n",
      "Epoch [7/10], Batch [769/938], Loss: 0.5667\n",
      "Epoch [7/10], Batch [770/938], Loss: 0.5945\n",
      "Epoch [7/10], Batch [771/938], Loss: 0.6141\n",
      "Epoch [7/10], Batch [772/938], Loss: 0.6003\n",
      "Epoch [7/10], Batch [773/938], Loss: 0.5915\n",
      "Epoch [7/10], Batch [774/938], Loss: 0.5701\n",
      "Epoch [7/10], Batch [775/938], Loss: 0.5827\n",
      "Epoch [7/10], Batch [776/938], Loss: 0.6070\n",
      "Epoch [7/10], Batch [777/938], Loss: 0.5737\n",
      "Epoch [7/10], Batch [778/938], Loss: 0.5960\n",
      "Epoch [7/10], Batch [779/938], Loss: 0.5838\n",
      "Epoch [7/10], Batch [780/938], Loss: 0.5469\n",
      "Epoch [7/10], Batch [781/938], Loss: 0.6093\n",
      "Epoch [7/10], Batch [782/938], Loss: 0.5853\n",
      "Epoch [7/10], Batch [783/938], Loss: 0.5788\n",
      "Epoch [7/10], Batch [784/938], Loss: 0.6004\n",
      "Epoch [7/10], Batch [785/938], Loss: 0.5791\n",
      "Epoch [7/10], Batch [786/938], Loss: 0.5786\n",
      "Epoch [7/10], Batch [787/938], Loss: 0.5908\n",
      "Epoch [7/10], Batch [788/938], Loss: 0.6226\n",
      "Epoch [7/10], Batch [789/938], Loss: 0.5918\n",
      "Epoch [7/10], Batch [790/938], Loss: 0.5852\n",
      "Epoch [7/10], Batch [791/938], Loss: 0.5793\n",
      "Epoch [7/10], Batch [792/938], Loss: 0.6140\n",
      "Epoch [7/10], Batch [793/938], Loss: 0.5945\n",
      "Epoch [7/10], Batch [794/938], Loss: 0.5867\n",
      "Epoch [7/10], Batch [795/938], Loss: 0.6156\n",
      "Epoch [7/10], Batch [796/938], Loss: 0.6154\n",
      "Epoch [7/10], Batch [797/938], Loss: 0.5693\n",
      "Epoch [7/10], Batch [798/938], Loss: 0.5792\n",
      "Epoch [7/10], Batch [799/938], Loss: 0.5711\n",
      "Epoch [7/10], Batch [800/938], Loss: 0.6201\n",
      "Epoch [7/10], Batch [801/938], Loss: 0.5426\n",
      "Epoch [7/10], Batch [802/938], Loss: 0.6168\n",
      "Epoch [7/10], Batch [803/938], Loss: 0.5851\n",
      "Epoch [7/10], Batch [804/938], Loss: 0.5674\n",
      "Epoch [7/10], Batch [805/938], Loss: 0.5820\n",
      "Epoch [7/10], Batch [806/938], Loss: 0.5704\n",
      "Epoch [7/10], Batch [807/938], Loss: 0.6046\n",
      "Epoch [7/10], Batch [808/938], Loss: 0.5892\n",
      "Epoch [7/10], Batch [809/938], Loss: 0.5886\n",
      "Epoch [7/10], Batch [810/938], Loss: 0.5973\n",
      "Epoch [7/10], Batch [811/938], Loss: 0.6082\n",
      "Epoch [7/10], Batch [812/938], Loss: 0.5858\n",
      "Epoch [7/10], Batch [813/938], Loss: 0.5911\n",
      "Epoch [7/10], Batch [814/938], Loss: 0.5915\n",
      "Epoch [7/10], Batch [815/938], Loss: 0.5977\n",
      "Epoch [7/10], Batch [816/938], Loss: 0.5845\n",
      "Epoch [7/10], Batch [817/938], Loss: 0.5760\n",
      "Epoch [7/10], Batch [818/938], Loss: 0.5890\n",
      "Epoch [7/10], Batch [819/938], Loss: 0.6181\n",
      "Epoch [7/10], Batch [820/938], Loss: 0.5999\n",
      "Epoch [7/10], Batch [821/938], Loss: 0.5624\n",
      "Epoch [7/10], Batch [822/938], Loss: 0.5838\n",
      "Epoch [7/10], Batch [823/938], Loss: 0.5977\n",
      "Epoch [7/10], Batch [824/938], Loss: 0.6191\n",
      "Epoch [7/10], Batch [825/938], Loss: 0.5660\n",
      "Epoch [7/10], Batch [826/938], Loss: 0.5645\n",
      "Epoch [7/10], Batch [827/938], Loss: 0.5933\n",
      "Epoch [7/10], Batch [828/938], Loss: 0.5933\n",
      "Epoch [7/10], Batch [829/938], Loss: 0.6279\n",
      "Epoch [7/10], Batch [830/938], Loss: 0.6194\n",
      "Epoch [7/10], Batch [831/938], Loss: 0.5832\n",
      "Epoch [7/10], Batch [832/938], Loss: 0.5663\n",
      "Epoch [7/10], Batch [833/938], Loss: 0.6212\n",
      "Epoch [7/10], Batch [834/938], Loss: 0.5938\n",
      "Epoch [7/10], Batch [835/938], Loss: 0.6041\n",
      "Epoch [7/10], Batch [836/938], Loss: 0.5913\n",
      "Epoch [7/10], Batch [837/938], Loss: 0.6089\n",
      "Epoch [7/10], Batch [838/938], Loss: 0.5994\n",
      "Epoch [7/10], Batch [839/938], Loss: 0.5820\n",
      "Epoch [7/10], Batch [840/938], Loss: 0.5808\n",
      "Epoch [7/10], Batch [841/938], Loss: 0.6146\n",
      "Epoch [7/10], Batch [842/938], Loss: 0.5896\n",
      "Epoch [7/10], Batch [843/938], Loss: 0.5704\n",
      "Epoch [7/10], Batch [844/938], Loss: 0.5880\n",
      "Epoch [7/10], Batch [845/938], Loss: 0.6197\n",
      "Epoch [7/10], Batch [846/938], Loss: 0.6143\n",
      "Epoch [7/10], Batch [847/938], Loss: 0.5859\n",
      "Epoch [7/10], Batch [848/938], Loss: 0.6119\n",
      "Epoch [7/10], Batch [849/938], Loss: 0.5777\n",
      "Epoch [7/10], Batch [850/938], Loss: 0.5900\n",
      "Epoch [7/10], Batch [851/938], Loss: 0.5799\n",
      "Epoch [7/10], Batch [852/938], Loss: 0.6050\n",
      "Epoch [7/10], Batch [853/938], Loss: 0.5913\n",
      "Epoch [7/10], Batch [854/938], Loss: 0.6033\n",
      "Epoch [7/10], Batch [855/938], Loss: 0.6179\n",
      "Epoch [7/10], Batch [856/938], Loss: 0.5823\n",
      "Epoch [7/10], Batch [857/938], Loss: 0.5841\n",
      "Epoch [7/10], Batch [858/938], Loss: 0.5744\n",
      "Epoch [7/10], Batch [859/938], Loss: 0.5792\n",
      "Epoch [7/10], Batch [860/938], Loss: 0.5965\n",
      "Epoch [7/10], Batch [861/938], Loss: 0.5918\n",
      "Epoch [7/10], Batch [862/938], Loss: 0.5887\n",
      "Epoch [7/10], Batch [863/938], Loss: 0.5820\n",
      "Epoch [7/10], Batch [864/938], Loss: 0.5996\n",
      "Epoch [7/10], Batch [865/938], Loss: 0.5921\n",
      "Epoch [7/10], Batch [866/938], Loss: 0.5964\n",
      "Epoch [7/10], Batch [867/938], Loss: 0.5701\n",
      "Epoch [7/10], Batch [868/938], Loss: 0.5911\n",
      "Epoch [7/10], Batch [869/938], Loss: 0.6170\n",
      "Epoch [7/10], Batch [870/938], Loss: 0.5785\n",
      "Epoch [7/10], Batch [871/938], Loss: 0.5712\n",
      "Epoch [7/10], Batch [872/938], Loss: 0.5831\n",
      "Epoch [7/10], Batch [873/938], Loss: 0.5736\n",
      "Epoch [7/10], Batch [874/938], Loss: 0.5665\n",
      "Epoch [7/10], Batch [875/938], Loss: 0.5859\n",
      "Epoch [7/10], Batch [876/938], Loss: 0.6029\n",
      "Epoch [7/10], Batch [877/938], Loss: 0.5931\n",
      "Epoch [7/10], Batch [878/938], Loss: 0.6002\n",
      "Epoch [7/10], Batch [879/938], Loss: 0.6109\n",
      "Epoch [7/10], Batch [880/938], Loss: 0.5855\n",
      "Epoch [7/10], Batch [881/938], Loss: 0.6037\n",
      "Epoch [7/10], Batch [882/938], Loss: 0.5520\n",
      "Epoch [7/10], Batch [883/938], Loss: 0.5983\n",
      "Epoch [7/10], Batch [884/938], Loss: 0.5525\n",
      "Epoch [7/10], Batch [885/938], Loss: 0.5852\n",
      "Epoch [7/10], Batch [886/938], Loss: 0.6181\n",
      "Epoch [7/10], Batch [887/938], Loss: 0.6026\n",
      "Epoch [7/10], Batch [888/938], Loss: 0.6149\n",
      "Epoch [7/10], Batch [889/938], Loss: 0.5660\n",
      "Epoch [7/10], Batch [890/938], Loss: 0.6221\n",
      "Epoch [7/10], Batch [891/938], Loss: 0.5890\n",
      "Epoch [7/10], Batch [892/938], Loss: 0.5936\n",
      "Epoch [7/10], Batch [893/938], Loss: 0.6217\n",
      "Epoch [7/10], Batch [894/938], Loss: 0.5751\n",
      "Epoch [7/10], Batch [895/938], Loss: 0.6128\n",
      "Epoch [7/10], Batch [896/938], Loss: 0.5527\n",
      "Epoch [7/10], Batch [897/938], Loss: 0.5645\n",
      "Epoch [7/10], Batch [898/938], Loss: 0.5988\n",
      "Epoch [7/10], Batch [899/938], Loss: 0.6066\n",
      "Epoch [7/10], Batch [900/938], Loss: 0.5788\n",
      "Epoch [7/10], Batch [901/938], Loss: 0.5649\n",
      "Epoch [7/10], Batch [902/938], Loss: 0.6221\n",
      "Epoch [7/10], Batch [903/938], Loss: 0.6044\n",
      "Epoch [7/10], Batch [904/938], Loss: 0.5964\n",
      "Epoch [7/10], Batch [905/938], Loss: 0.5723\n",
      "Epoch [7/10], Batch [906/938], Loss: 0.5710\n",
      "Epoch [7/10], Batch [907/938], Loss: 0.5812\n",
      "Epoch [7/10], Batch [908/938], Loss: 0.5930\n",
      "Epoch [7/10], Batch [909/938], Loss: 0.6066\n",
      "Epoch [7/10], Batch [910/938], Loss: 0.6202\n",
      "Epoch [7/10], Batch [911/938], Loss: 0.5862\n",
      "Epoch [7/10], Batch [912/938], Loss: 0.6051\n",
      "Epoch [7/10], Batch [913/938], Loss: 0.5850\n",
      "Epoch [7/10], Batch [914/938], Loss: 0.5733\n",
      "Epoch [7/10], Batch [915/938], Loss: 0.5913\n",
      "Epoch [7/10], Batch [916/938], Loss: 0.6034\n",
      "Epoch [7/10], Batch [917/938], Loss: 0.5736\n",
      "Epoch [7/10], Batch [918/938], Loss: 0.5747\n",
      "Epoch [7/10], Batch [919/938], Loss: 0.5905\n",
      "Epoch [7/10], Batch [920/938], Loss: 0.6289\n",
      "Epoch [7/10], Batch [921/938], Loss: 0.6058\n",
      "Epoch [7/10], Batch [922/938], Loss: 0.6003\n",
      "Epoch [7/10], Batch [923/938], Loss: 0.6152\n",
      "Epoch [7/10], Batch [924/938], Loss: 0.5635\n",
      "Epoch [7/10], Batch [925/938], Loss: 0.6092\n",
      "Epoch [7/10], Batch [926/938], Loss: 0.6000\n",
      "Epoch [7/10], Batch [927/938], Loss: 0.6194\n",
      "Epoch [7/10], Batch [928/938], Loss: 0.5861\n",
      "Epoch [7/10], Batch [929/938], Loss: 0.5863\n",
      "Epoch [7/10], Batch [930/938], Loss: 0.5970\n",
      "Epoch [7/10], Batch [931/938], Loss: 0.5841\n",
      "Epoch [7/10], Batch [932/938], Loss: 0.5465\n",
      "Epoch [7/10], Batch [933/938], Loss: 0.5822\n",
      "Epoch [7/10], Batch [934/938], Loss: 0.6051\n",
      "Epoch [7/10], Batch [935/938], Loss: 0.5641\n",
      "Epoch [7/10], Batch [936/938], Loss: 0.6118\n",
      "Epoch [7/10], Batch [937/938], Loss: 0.5785\n",
      "Epoch [7/10], Batch [938/938], Loss: 0.6011\n",
      "Epoch [7/10], Loss: 0.6011\n",
      "Epoch [8/10], Batch [1/938], Loss: 0.5637\n",
      "Epoch [8/10], Batch [2/938], Loss: 0.6088\n",
      "Epoch [8/10], Batch [3/938], Loss: 0.5930\n",
      "Epoch [8/10], Batch [4/938], Loss: 0.5784\n",
      "Epoch [8/10], Batch [5/938], Loss: 0.6038\n",
      "Epoch [8/10], Batch [6/938], Loss: 0.6051\n",
      "Epoch [8/10], Batch [7/938], Loss: 0.5854\n",
      "Epoch [8/10], Batch [8/938], Loss: 0.5646\n",
      "Epoch [8/10], Batch [9/938], Loss: 0.5767\n",
      "Epoch [8/10], Batch [10/938], Loss: 0.5876\n",
      "Epoch [8/10], Batch [11/938], Loss: 0.5837\n",
      "Epoch [8/10], Batch [12/938], Loss: 0.5769\n",
      "Epoch [8/10], Batch [13/938], Loss: 0.5998\n",
      "Epoch [8/10], Batch [14/938], Loss: 0.5853\n",
      "Epoch [8/10], Batch [15/938], Loss: 0.5642\n",
      "Epoch [8/10], Batch [16/938], Loss: 0.5847\n",
      "Epoch [8/10], Batch [17/938], Loss: 0.5887\n",
      "Epoch [8/10], Batch [18/938], Loss: 0.5815\n",
      "Epoch [8/10], Batch [19/938], Loss: 0.6107\n",
      "Epoch [8/10], Batch [20/938], Loss: 0.5652\n",
      "Epoch [8/10], Batch [21/938], Loss: 0.5797\n",
      "Epoch [8/10], Batch [22/938], Loss: 0.5966\n",
      "Epoch [8/10], Batch [23/938], Loss: 0.5796\n",
      "Epoch [8/10], Batch [24/938], Loss: 0.5817\n",
      "Epoch [8/10], Batch [25/938], Loss: 0.5799\n",
      "Epoch [8/10], Batch [26/938], Loss: 0.5800\n",
      "Epoch [8/10], Batch [27/938], Loss: 0.5817\n",
      "Epoch [8/10], Batch [28/938], Loss: 0.6041\n",
      "Epoch [8/10], Batch [29/938], Loss: 0.6177\n",
      "Epoch [8/10], Batch [30/938], Loss: 0.5885\n",
      "Epoch [8/10], Batch [31/938], Loss: 0.5709\n",
      "Epoch [8/10], Batch [32/938], Loss: 0.5884\n",
      "Epoch [8/10], Batch [33/938], Loss: 0.5903\n",
      "Epoch [8/10], Batch [34/938], Loss: 0.6200\n",
      "Epoch [8/10], Batch [35/938], Loss: 0.5937\n",
      "Epoch [8/10], Batch [36/938], Loss: 0.6083\n",
      "Epoch [8/10], Batch [37/938], Loss: 0.5829\n",
      "Epoch [8/10], Batch [38/938], Loss: 0.5952\n",
      "Epoch [8/10], Batch [39/938], Loss: 0.6094\n",
      "Epoch [8/10], Batch [40/938], Loss: 0.5991\n",
      "Epoch [8/10], Batch [41/938], Loss: 0.6038\n",
      "Epoch [8/10], Batch [42/938], Loss: 0.6318\n",
      "Epoch [8/10], Batch [43/938], Loss: 0.5853\n",
      "Epoch [8/10], Batch [44/938], Loss: 0.5874\n",
      "Epoch [8/10], Batch [45/938], Loss: 0.5964\n",
      "Epoch [8/10], Batch [46/938], Loss: 0.5592\n",
      "Epoch [8/10], Batch [47/938], Loss: 0.5916\n",
      "Epoch [8/10], Batch [48/938], Loss: 0.5729\n",
      "Epoch [8/10], Batch [49/938], Loss: 0.5730\n",
      "Epoch [8/10], Batch [50/938], Loss: 0.5879\n",
      "Epoch [8/10], Batch [51/938], Loss: 0.6202\n",
      "Epoch [8/10], Batch [52/938], Loss: 0.5639\n",
      "Epoch [8/10], Batch [53/938], Loss: 0.5743\n",
      "Epoch [8/10], Batch [54/938], Loss: 0.6120\n",
      "Epoch [8/10], Batch [55/938], Loss: 0.5850\n",
      "Epoch [8/10], Batch [56/938], Loss: 0.6009\n",
      "Epoch [8/10], Batch [57/938], Loss: 0.5956\n",
      "Epoch [8/10], Batch [58/938], Loss: 0.5916\n",
      "Epoch [8/10], Batch [59/938], Loss: 0.5985\n",
      "Epoch [8/10], Batch [60/938], Loss: 0.5724\n",
      "Epoch [8/10], Batch [61/938], Loss: 0.6011\n",
      "Epoch [8/10], Batch [62/938], Loss: 0.6219\n",
      "Epoch [8/10], Batch [63/938], Loss: 0.6041\n",
      "Epoch [8/10], Batch [64/938], Loss: 0.6229\n",
      "Epoch [8/10], Batch [65/938], Loss: 0.5983\n",
      "Epoch [8/10], Batch [66/938], Loss: 0.5822\n",
      "Epoch [8/10], Batch [67/938], Loss: 0.5799\n",
      "Epoch [8/10], Batch [68/938], Loss: 0.6042\n",
      "Epoch [8/10], Batch [69/938], Loss: 0.5930\n",
      "Epoch [8/10], Batch [70/938], Loss: 0.6220\n",
      "Epoch [8/10], Batch [71/938], Loss: 0.5713\n",
      "Epoch [8/10], Batch [72/938], Loss: 0.5941\n",
      "Epoch [8/10], Batch [73/938], Loss: 0.5967\n",
      "Epoch [8/10], Batch [74/938], Loss: 0.6212\n",
      "Epoch [8/10], Batch [75/938], Loss: 0.5892\n",
      "Epoch [8/10], Batch [76/938], Loss: 0.5939\n",
      "Epoch [8/10], Batch [77/938], Loss: 0.5960\n",
      "Epoch [8/10], Batch [78/938], Loss: 0.5970\n",
      "Epoch [8/10], Batch [79/938], Loss: 0.5763\n",
      "Epoch [8/10], Batch [80/938], Loss: 0.5793\n",
      "Epoch [8/10], Batch [81/938], Loss: 0.5891\n",
      "Epoch [8/10], Batch [82/938], Loss: 0.5831\n",
      "Epoch [8/10], Batch [83/938], Loss: 0.5688\n",
      "Epoch [8/10], Batch [84/938], Loss: 0.5894\n",
      "Epoch [8/10], Batch [85/938], Loss: 0.6115\n",
      "Epoch [8/10], Batch [86/938], Loss: 0.5814\n",
      "Epoch [8/10], Batch [87/938], Loss: 0.5850\n",
      "Epoch [8/10], Batch [88/938], Loss: 0.5693\n",
      "Epoch [8/10], Batch [89/938], Loss: 0.6026\n",
      "Epoch [8/10], Batch [90/938], Loss: 0.5765\n",
      "Epoch [8/10], Batch [91/938], Loss: 0.5937\n",
      "Epoch [8/10], Batch [92/938], Loss: 0.5932\n",
      "Epoch [8/10], Batch [93/938], Loss: 0.5797\n",
      "Epoch [8/10], Batch [94/938], Loss: 0.5936\n",
      "Epoch [8/10], Batch [95/938], Loss: 0.5878\n",
      "Epoch [8/10], Batch [96/938], Loss: 0.5858\n",
      "Epoch [8/10], Batch [97/938], Loss: 0.5749\n",
      "Epoch [8/10], Batch [98/938], Loss: 0.5554\n",
      "Epoch [8/10], Batch [99/938], Loss: 0.5974\n",
      "Epoch [8/10], Batch [100/938], Loss: 0.5941\n",
      "Epoch [8/10], Batch [101/938], Loss: 0.6162\n",
      "Epoch [8/10], Batch [102/938], Loss: 0.5627\n",
      "Epoch [8/10], Batch [103/938], Loss: 0.5879\n",
      "Epoch [8/10], Batch [104/938], Loss: 0.5925\n",
      "Epoch [8/10], Batch [105/938], Loss: 0.5759\n",
      "Epoch [8/10], Batch [106/938], Loss: 0.5939\n",
      "Epoch [8/10], Batch [107/938], Loss: 0.5980\n",
      "Epoch [8/10], Batch [108/938], Loss: 0.5876\n",
      "Epoch [8/10], Batch [109/938], Loss: 0.5880\n",
      "Epoch [8/10], Batch [110/938], Loss: 0.5829\n",
      "Epoch [8/10], Batch [111/938], Loss: 0.6062\n",
      "Epoch [8/10], Batch [112/938], Loss: 0.6099\n",
      "Epoch [8/10], Batch [113/938], Loss: 0.5792\n",
      "Epoch [8/10], Batch [114/938], Loss: 0.5987\n",
      "Epoch [8/10], Batch [115/938], Loss: 0.5617\n",
      "Epoch [8/10], Batch [116/938], Loss: 0.6143\n",
      "Epoch [8/10], Batch [117/938], Loss: 0.5985\n",
      "Epoch [8/10], Batch [118/938], Loss: 0.5630\n",
      "Epoch [8/10], Batch [119/938], Loss: 0.5769\n",
      "Epoch [8/10], Batch [120/938], Loss: 0.5807\n",
      "Epoch [8/10], Batch [121/938], Loss: 0.5986\n",
      "Epoch [8/10], Batch [122/938], Loss: 0.5727\n",
      "Epoch [8/10], Batch [123/938], Loss: 0.6016\n",
      "Epoch [8/10], Batch [124/938], Loss: 0.5835\n",
      "Epoch [8/10], Batch [125/938], Loss: 0.5855\n",
      "Epoch [8/10], Batch [126/938], Loss: 0.6003\n",
      "Epoch [8/10], Batch [127/938], Loss: 0.6169\n",
      "Epoch [8/10], Batch [128/938], Loss: 0.5787\n",
      "Epoch [8/10], Batch [129/938], Loss: 0.5800\n",
      "Epoch [8/10], Batch [130/938], Loss: 0.6000\n",
      "Epoch [8/10], Batch [131/938], Loss: 0.5904\n",
      "Epoch [8/10], Batch [132/938], Loss: 0.5858\n",
      "Epoch [8/10], Batch [133/938], Loss: 0.6085\n",
      "Epoch [8/10], Batch [134/938], Loss: 0.5817\n",
      "Epoch [8/10], Batch [135/938], Loss: 0.5912\n",
      "Epoch [8/10], Batch [136/938], Loss: 0.5924\n",
      "Epoch [8/10], Batch [137/938], Loss: 0.5661\n",
      "Epoch [8/10], Batch [138/938], Loss: 0.5643\n",
      "Epoch [8/10], Batch [139/938], Loss: 0.5891\n",
      "Epoch [8/10], Batch [140/938], Loss: 0.5785\n",
      "Epoch [8/10], Batch [141/938], Loss: 0.5959\n",
      "Epoch [8/10], Batch [142/938], Loss: 0.5739\n",
      "Epoch [8/10], Batch [143/938], Loss: 0.5724\n",
      "Epoch [8/10], Batch [144/938], Loss: 0.5829\n",
      "Epoch [8/10], Batch [145/938], Loss: 0.5558\n",
      "Epoch [8/10], Batch [146/938], Loss: 0.6063\n",
      "Epoch [8/10], Batch [147/938], Loss: 0.5716\n",
      "Epoch [8/10], Batch [148/938], Loss: 0.5827\n",
      "Epoch [8/10], Batch [149/938], Loss: 0.6093\n",
      "Epoch [8/10], Batch [150/938], Loss: 0.5799\n",
      "Epoch [8/10], Batch [151/938], Loss: 0.5843\n",
      "Epoch [8/10], Batch [152/938], Loss: 0.5949\n",
      "Epoch [8/10], Batch [153/938], Loss: 0.5742\n",
      "Epoch [8/10], Batch [154/938], Loss: 0.6056\n",
      "Epoch [8/10], Batch [155/938], Loss: 0.5811\n",
      "Epoch [8/10], Batch [156/938], Loss: 0.5854\n",
      "Epoch [8/10], Batch [157/938], Loss: 0.5786\n",
      "Epoch [8/10], Batch [158/938], Loss: 0.5821\n",
      "Epoch [8/10], Batch [159/938], Loss: 0.5974\n",
      "Epoch [8/10], Batch [160/938], Loss: 0.6041\n",
      "Epoch [8/10], Batch [161/938], Loss: 0.5893\n",
      "Epoch [8/10], Batch [162/938], Loss: 0.5757\n",
      "Epoch [8/10], Batch [163/938], Loss: 0.5813\n",
      "Epoch [8/10], Batch [164/938], Loss: 0.5619\n",
      "Epoch [8/10], Batch [165/938], Loss: 0.5656\n",
      "Epoch [8/10], Batch [166/938], Loss: 0.5727\n",
      "Epoch [8/10], Batch [167/938], Loss: 0.5718\n",
      "Epoch [8/10], Batch [168/938], Loss: 0.5927\n",
      "Epoch [8/10], Batch [169/938], Loss: 0.5778\n",
      "Epoch [8/10], Batch [170/938], Loss: 0.5758\n",
      "Epoch [8/10], Batch [171/938], Loss: 0.5755\n",
      "Epoch [8/10], Batch [172/938], Loss: 0.5792\n",
      "Epoch [8/10], Batch [173/938], Loss: 0.5518\n",
      "Epoch [8/10], Batch [174/938], Loss: 0.5856\n",
      "Epoch [8/10], Batch [175/938], Loss: 0.5662\n",
      "Epoch [8/10], Batch [176/938], Loss: 0.5812\n",
      "Epoch [8/10], Batch [177/938], Loss: 0.5885\n",
      "Epoch [8/10], Batch [178/938], Loss: 0.6233\n",
      "Epoch [8/10], Batch [179/938], Loss: 0.5824\n",
      "Epoch [8/10], Batch [180/938], Loss: 0.5706\n",
      "Epoch [8/10], Batch [181/938], Loss: 0.6064\n",
      "Epoch [8/10], Batch [182/938], Loss: 0.5894\n",
      "Epoch [8/10], Batch [183/938], Loss: 0.5799\n",
      "Epoch [8/10], Batch [184/938], Loss: 0.5863\n",
      "Epoch [8/10], Batch [185/938], Loss: 0.5957\n",
      "Epoch [8/10], Batch [186/938], Loss: 0.6003\n",
      "Epoch [8/10], Batch [187/938], Loss: 0.5980\n",
      "Epoch [8/10], Batch [188/938], Loss: 0.5575\n",
      "Epoch [8/10], Batch [189/938], Loss: 0.5902\n",
      "Epoch [8/10], Batch [190/938], Loss: 0.5837\n",
      "Epoch [8/10], Batch [191/938], Loss: 0.6146\n",
      "Epoch [8/10], Batch [192/938], Loss: 0.5871\n",
      "Epoch [8/10], Batch [193/938], Loss: 0.6158\n",
      "Epoch [8/10], Batch [194/938], Loss: 0.5957\n",
      "Epoch [8/10], Batch [195/938], Loss: 0.5909\n",
      "Epoch [8/10], Batch [196/938], Loss: 0.6116\n",
      "Epoch [8/10], Batch [197/938], Loss: 0.5772\n",
      "Epoch [8/10], Batch [198/938], Loss: 0.5474\n",
      "Epoch [8/10], Batch [199/938], Loss: 0.5822\n",
      "Epoch [8/10], Batch [200/938], Loss: 0.5710\n",
      "Epoch [8/10], Batch [201/938], Loss: 0.5644\n",
      "Epoch [8/10], Batch [202/938], Loss: 0.5854\n",
      "Epoch [8/10], Batch [203/938], Loss: 0.5762\n",
      "Epoch [8/10], Batch [204/938], Loss: 0.5770\n",
      "Epoch [8/10], Batch [205/938], Loss: 0.5793\n",
      "Epoch [8/10], Batch [206/938], Loss: 0.6080\n",
      "Epoch [8/10], Batch [207/938], Loss: 0.5785\n",
      "Epoch [8/10], Batch [208/938], Loss: 0.5807\n",
      "Epoch [8/10], Batch [209/938], Loss: 0.5865\n",
      "Epoch [8/10], Batch [210/938], Loss: 0.5925\n",
      "Epoch [8/10], Batch [211/938], Loss: 0.6083\n",
      "Epoch [8/10], Batch [212/938], Loss: 0.5679\n",
      "Epoch [8/10], Batch [213/938], Loss: 0.6050\n",
      "Epoch [8/10], Batch [214/938], Loss: 0.5803\n",
      "Epoch [8/10], Batch [215/938], Loss: 0.5996\n",
      "Epoch [8/10], Batch [216/938], Loss: 0.5882\n",
      "Epoch [8/10], Batch [217/938], Loss: 0.5980\n",
      "Epoch [8/10], Batch [218/938], Loss: 0.5675\n",
      "Epoch [8/10], Batch [219/938], Loss: 0.6124\n",
      "Epoch [8/10], Batch [220/938], Loss: 0.5955\n",
      "Epoch [8/10], Batch [221/938], Loss: 0.6045\n",
      "Epoch [8/10], Batch [222/938], Loss: 0.5919\n",
      "Epoch [8/10], Batch [223/938], Loss: 0.5810\n",
      "Epoch [8/10], Batch [224/938], Loss: 0.5967\n",
      "Epoch [8/10], Batch [225/938], Loss: 0.5730\n",
      "Epoch [8/10], Batch [226/938], Loss: 0.5615\n",
      "Epoch [8/10], Batch [227/938], Loss: 0.5716\n",
      "Epoch [8/10], Batch [228/938], Loss: 0.5975\n",
      "Epoch [8/10], Batch [229/938], Loss: 0.6178\n",
      "Epoch [8/10], Batch [230/938], Loss: 0.5858\n",
      "Epoch [8/10], Batch [231/938], Loss: 0.5798\n",
      "Epoch [8/10], Batch [232/938], Loss: 0.5937\n",
      "Epoch [8/10], Batch [233/938], Loss: 0.5931\n",
      "Epoch [8/10], Batch [234/938], Loss: 0.6099\n",
      "Epoch [8/10], Batch [235/938], Loss: 0.5754\n",
      "Epoch [8/10], Batch [236/938], Loss: 0.5509\n",
      "Epoch [8/10], Batch [237/938], Loss: 0.5745\n",
      "Epoch [8/10], Batch [238/938], Loss: 0.5969\n",
      "Epoch [8/10], Batch [239/938], Loss: 0.5775\n",
      "Epoch [8/10], Batch [240/938], Loss: 0.5822\n",
      "Epoch [8/10], Batch [241/938], Loss: 0.5929\n",
      "Epoch [8/10], Batch [242/938], Loss: 0.6055\n",
      "Epoch [8/10], Batch [243/938], Loss: 0.5842\n",
      "Epoch [8/10], Batch [244/938], Loss: 0.5783\n",
      "Epoch [8/10], Batch [245/938], Loss: 0.5786\n",
      "Epoch [8/10], Batch [246/938], Loss: 0.5977\n",
      "Epoch [8/10], Batch [247/938], Loss: 0.5607\n",
      "Epoch [8/10], Batch [248/938], Loss: 0.6068\n",
      "Epoch [8/10], Batch [249/938], Loss: 0.5934\n",
      "Epoch [8/10], Batch [250/938], Loss: 0.5841\n",
      "Epoch [8/10], Batch [251/938], Loss: 0.5988\n",
      "Epoch [8/10], Batch [252/938], Loss: 0.5651\n",
      "Epoch [8/10], Batch [253/938], Loss: 0.5984\n",
      "Epoch [8/10], Batch [254/938], Loss: 0.6016\n",
      "Epoch [8/10], Batch [255/938], Loss: 0.5577\n",
      "Epoch [8/10], Batch [256/938], Loss: 0.5853\n",
      "Epoch [8/10], Batch [257/938], Loss: 0.6072\n",
      "Epoch [8/10], Batch [258/938], Loss: 0.5897\n",
      "Epoch [8/10], Batch [259/938], Loss: 0.5887\n",
      "Epoch [8/10], Batch [260/938], Loss: 0.6102\n",
      "Epoch [8/10], Batch [261/938], Loss: 0.5912\n",
      "Epoch [8/10], Batch [262/938], Loss: 0.5876\n",
      "Epoch [8/10], Batch [263/938], Loss: 0.5932\n",
      "Epoch [8/10], Batch [264/938], Loss: 0.5842\n",
      "Epoch [8/10], Batch [265/938], Loss: 0.5717\n",
      "Epoch [8/10], Batch [266/938], Loss: 0.6013\n",
      "Epoch [8/10], Batch [267/938], Loss: 0.5828\n",
      "Epoch [8/10], Batch [268/938], Loss: 0.5792\n",
      "Epoch [8/10], Batch [269/938], Loss: 0.5782\n",
      "Epoch [8/10], Batch [270/938], Loss: 0.5892\n",
      "Epoch [8/10], Batch [271/938], Loss: 0.5798\n",
      "Epoch [8/10], Batch [272/938], Loss: 0.5812\n",
      "Epoch [8/10], Batch [273/938], Loss: 0.5832\n",
      "Epoch [8/10], Batch [274/938], Loss: 0.5981\n",
      "Epoch [8/10], Batch [275/938], Loss: 0.6181\n",
      "Epoch [8/10], Batch [276/938], Loss: 0.5997\n",
      "Epoch [8/10], Batch [277/938], Loss: 0.5532\n",
      "Epoch [8/10], Batch [278/938], Loss: 0.5667\n",
      "Epoch [8/10], Batch [279/938], Loss: 0.5854\n",
      "Epoch [8/10], Batch [280/938], Loss: 0.5832\n",
      "Epoch [8/10], Batch [281/938], Loss: 0.6308\n",
      "Epoch [8/10], Batch [282/938], Loss: 0.6109\n",
      "Epoch [8/10], Batch [283/938], Loss: 0.5824\n",
      "Epoch [8/10], Batch [284/938], Loss: 0.5830\n",
      "Epoch [8/10], Batch [285/938], Loss: 0.5940\n",
      "Epoch [8/10], Batch [286/938], Loss: 0.5863\n",
      "Epoch [8/10], Batch [287/938], Loss: 0.5926\n",
      "Epoch [8/10], Batch [288/938], Loss: 0.5719\n",
      "Epoch [8/10], Batch [289/938], Loss: 0.5817\n",
      "Epoch [8/10], Batch [290/938], Loss: 0.5926\n",
      "Epoch [8/10], Batch [291/938], Loss: 0.5820\n",
      "Epoch [8/10], Batch [292/938], Loss: 0.5910\n",
      "Epoch [8/10], Batch [293/938], Loss: 0.5979\n",
      "Epoch [8/10], Batch [294/938], Loss: 0.5709\n",
      "Epoch [8/10], Batch [295/938], Loss: 0.6113\n",
      "Epoch [8/10], Batch [296/938], Loss: 0.5765\n",
      "Epoch [8/10], Batch [297/938], Loss: 0.6041\n",
      "Epoch [8/10], Batch [298/938], Loss: 0.5802\n",
      "Epoch [8/10], Batch [299/938], Loss: 0.5791\n",
      "Epoch [8/10], Batch [300/938], Loss: 0.6129\n",
      "Epoch [8/10], Batch [301/938], Loss: 0.5870\n",
      "Epoch [8/10], Batch [302/938], Loss: 0.6040\n",
      "Epoch [8/10], Batch [303/938], Loss: 0.5842\n",
      "Epoch [8/10], Batch [304/938], Loss: 0.5933\n",
      "Epoch [8/10], Batch [305/938], Loss: 0.5961\n",
      "Epoch [8/10], Batch [306/938], Loss: 0.5911\n",
      "Epoch [8/10], Batch [307/938], Loss: 0.5824\n",
      "Epoch [8/10], Batch [308/938], Loss: 0.5777\n",
      "Epoch [8/10], Batch [309/938], Loss: 0.5904\n",
      "Epoch [8/10], Batch [310/938], Loss: 0.5793\n",
      "Epoch [8/10], Batch [311/938], Loss: 0.6086\n",
      "Epoch [8/10], Batch [312/938], Loss: 0.5627\n",
      "Epoch [8/10], Batch [313/938], Loss: 0.5615\n",
      "Epoch [8/10], Batch [314/938], Loss: 0.5806\n",
      "Epoch [8/10], Batch [315/938], Loss: 0.5763\n",
      "Epoch [8/10], Batch [316/938], Loss: 0.5754\n",
      "Epoch [8/10], Batch [317/938], Loss: 0.5749\n",
      "Epoch [8/10], Batch [318/938], Loss: 0.6075\n",
      "Epoch [8/10], Batch [319/938], Loss: 0.5969\n",
      "Epoch [8/10], Batch [320/938], Loss: 0.6020\n",
      "Epoch [8/10], Batch [321/938], Loss: 0.6185\n",
      "Epoch [8/10], Batch [322/938], Loss: 0.6060\n",
      "Epoch [8/10], Batch [323/938], Loss: 0.6271\n",
      "Epoch [8/10], Batch [324/938], Loss: 0.6382\n",
      "Epoch [8/10], Batch [325/938], Loss: 0.6002\n",
      "Epoch [8/10], Batch [326/938], Loss: 0.6127\n",
      "Epoch [8/10], Batch [327/938], Loss: 0.6008\n",
      "Epoch [8/10], Batch [328/938], Loss: 0.5710\n",
      "Epoch [8/10], Batch [329/938], Loss: 0.6055\n",
      "Epoch [8/10], Batch [330/938], Loss: 0.6049\n",
      "Epoch [8/10], Batch [331/938], Loss: 0.5675\n",
      "Epoch [8/10], Batch [332/938], Loss: 0.5851\n",
      "Epoch [8/10], Batch [333/938], Loss: 0.5733\n",
      "Epoch [8/10], Batch [334/938], Loss: 0.5931\n",
      "Epoch [8/10], Batch [335/938], Loss: 0.5776\n",
      "Epoch [8/10], Batch [336/938], Loss: 0.5772\n",
      "Epoch [8/10], Batch [337/938], Loss: 0.6110\n",
      "Epoch [8/10], Batch [338/938], Loss: 0.5767\n",
      "Epoch [8/10], Batch [339/938], Loss: 0.5840\n",
      "Epoch [8/10], Batch [340/938], Loss: 0.5846\n",
      "Epoch [8/10], Batch [341/938], Loss: 0.5645\n",
      "Epoch [8/10], Batch [342/938], Loss: 0.5976\n",
      "Epoch [8/10], Batch [343/938], Loss: 0.5950\n",
      "Epoch [8/10], Batch [344/938], Loss: 0.5661\n",
      "Epoch [8/10], Batch [345/938], Loss: 0.6023\n",
      "Epoch [8/10], Batch [346/938], Loss: 0.5865\n",
      "Epoch [8/10], Batch [347/938], Loss: 0.5854\n",
      "Epoch [8/10], Batch [348/938], Loss: 0.6334\n",
      "Epoch [8/10], Batch [349/938], Loss: 0.5740\n",
      "Epoch [8/10], Batch [350/938], Loss: 0.5889\n",
      "Epoch [8/10], Batch [351/938], Loss: 0.6015\n",
      "Epoch [8/10], Batch [352/938], Loss: 0.5894\n",
      "Epoch [8/10], Batch [353/938], Loss: 0.5810\n",
      "Epoch [8/10], Batch [354/938], Loss: 0.5841\n",
      "Epoch [8/10], Batch [355/938], Loss: 0.5894\n",
      "Epoch [8/10], Batch [356/938], Loss: 0.5727\n",
      "Epoch [8/10], Batch [357/938], Loss: 0.6024\n",
      "Epoch [8/10], Batch [358/938], Loss: 0.5890\n",
      "Epoch [8/10], Batch [359/938], Loss: 0.5779\n",
      "Epoch [8/10], Batch [360/938], Loss: 0.6077\n",
      "Epoch [8/10], Batch [361/938], Loss: 0.6087\n",
      "Epoch [8/10], Batch [362/938], Loss: 0.5773\n",
      "Epoch [8/10], Batch [363/938], Loss: 0.5880\n",
      "Epoch [8/10], Batch [364/938], Loss: 0.6067\n",
      "Epoch [8/10], Batch [365/938], Loss: 0.5973\n",
      "Epoch [8/10], Batch [366/938], Loss: 0.5601\n",
      "Epoch [8/10], Batch [367/938], Loss: 0.5915\n",
      "Epoch [8/10], Batch [368/938], Loss: 0.6040\n",
      "Epoch [8/10], Batch [369/938], Loss: 0.5658\n",
      "Epoch [8/10], Batch [370/938], Loss: 0.5834\n",
      "Epoch [8/10], Batch [371/938], Loss: 0.6007\n",
      "Epoch [8/10], Batch [372/938], Loss: 0.5876\n",
      "Epoch [8/10], Batch [373/938], Loss: 0.5786\n",
      "Epoch [8/10], Batch [374/938], Loss: 0.6101\n",
      "Epoch [8/10], Batch [375/938], Loss: 0.5759\n",
      "Epoch [8/10], Batch [376/938], Loss: 0.5886\n",
      "Epoch [8/10], Batch [377/938], Loss: 0.5906\n",
      "Epoch [8/10], Batch [378/938], Loss: 0.6281\n",
      "Epoch [8/10], Batch [379/938], Loss: 0.5746\n",
      "Epoch [8/10], Batch [380/938], Loss: 0.5913\n",
      "Epoch [8/10], Batch [381/938], Loss: 0.5723\n",
      "Epoch [8/10], Batch [382/938], Loss: 0.5716\n",
      "Epoch [8/10], Batch [383/938], Loss: 0.6044\n",
      "Epoch [8/10], Batch [384/938], Loss: 0.5948\n",
      "Epoch [8/10], Batch [385/938], Loss: 0.6048\n",
      "Epoch [8/10], Batch [386/938], Loss: 0.6007\n",
      "Epoch [8/10], Batch [387/938], Loss: 0.5755\n",
      "Epoch [8/10], Batch [388/938], Loss: 0.5992\n",
      "Epoch [8/10], Batch [389/938], Loss: 0.6029\n",
      "Epoch [8/10], Batch [390/938], Loss: 0.6036\n",
      "Epoch [8/10], Batch [391/938], Loss: 0.5915\n",
      "Epoch [8/10], Batch [392/938], Loss: 0.6023\n",
      "Epoch [8/10], Batch [393/938], Loss: 0.5872\n",
      "Epoch [8/10], Batch [394/938], Loss: 0.5806\n",
      "Epoch [8/10], Batch [395/938], Loss: 0.5682\n",
      "Epoch [8/10], Batch [396/938], Loss: 0.6011\n",
      "Epoch [8/10], Batch [397/938], Loss: 0.5753\n",
      "Epoch [8/10], Batch [398/938], Loss: 0.5909\n",
      "Epoch [8/10], Batch [399/938], Loss: 0.6116\n",
      "Epoch [8/10], Batch [400/938], Loss: 0.5926\n",
      "Epoch [8/10], Batch [401/938], Loss: 0.5666\n",
      "Epoch [8/10], Batch [402/938], Loss: 0.5901\n",
      "Epoch [8/10], Batch [403/938], Loss: 0.6176\n",
      "Epoch [8/10], Batch [404/938], Loss: 0.6161\n",
      "Epoch [8/10], Batch [405/938], Loss: 0.5686\n",
      "Epoch [8/10], Batch [406/938], Loss: 0.6033\n",
      "Epoch [8/10], Batch [407/938], Loss: 0.6269\n",
      "Epoch [8/10], Batch [408/938], Loss: 0.6113\n",
      "Epoch [8/10], Batch [409/938], Loss: 0.5686\n",
      "Epoch [8/10], Batch [410/938], Loss: 0.5722\n",
      "Epoch [8/10], Batch [411/938], Loss: 0.5869\n",
      "Epoch [8/10], Batch [412/938], Loss: 0.5600\n",
      "Epoch [8/10], Batch [413/938], Loss: 0.5910\n",
      "Epoch [8/10], Batch [414/938], Loss: 0.5617\n",
      "Epoch [8/10], Batch [415/938], Loss: 0.6012\n",
      "Epoch [8/10], Batch [416/938], Loss: 0.6123\n",
      "Epoch [8/10], Batch [417/938], Loss: 0.5760\n",
      "Epoch [8/10], Batch [418/938], Loss: 0.5990\n",
      "Epoch [8/10], Batch [419/938], Loss: 0.5898\n",
      "Epoch [8/10], Batch [420/938], Loss: 0.5975\n",
      "Epoch [8/10], Batch [421/938], Loss: 0.5965\n",
      "Epoch [8/10], Batch [422/938], Loss: 0.5671\n",
      "Epoch [8/10], Batch [423/938], Loss: 0.5956\n",
      "Epoch [8/10], Batch [424/938], Loss: 0.6049\n",
      "Epoch [8/10], Batch [425/938], Loss: 0.5718\n",
      "Epoch [8/10], Batch [426/938], Loss: 0.6082\n",
      "Epoch [8/10], Batch [427/938], Loss: 0.6195\n",
      "Epoch [8/10], Batch [428/938], Loss: 0.5942\n",
      "Epoch [8/10], Batch [429/938], Loss: 0.5755\n",
      "Epoch [8/10], Batch [430/938], Loss: 0.5861\n",
      "Epoch [8/10], Batch [431/938], Loss: 0.6008\n",
      "Epoch [8/10], Batch [432/938], Loss: 0.5881\n",
      "Epoch [8/10], Batch [433/938], Loss: 0.5973\n",
      "Epoch [8/10], Batch [434/938], Loss: 0.5815\n",
      "Epoch [8/10], Batch [435/938], Loss: 0.5948\n",
      "Epoch [8/10], Batch [436/938], Loss: 0.6010\n",
      "Epoch [8/10], Batch [437/938], Loss: 0.6005\n",
      "Epoch [8/10], Batch [438/938], Loss: 0.6024\n",
      "Epoch [8/10], Batch [439/938], Loss: 0.6052\n",
      "Epoch [8/10], Batch [440/938], Loss: 0.6009\n",
      "Epoch [8/10], Batch [441/938], Loss: 0.5990\n",
      "Epoch [8/10], Batch [442/938], Loss: 0.6113\n",
      "Epoch [8/10], Batch [443/938], Loss: 0.5885\n",
      "Epoch [8/10], Batch [444/938], Loss: 0.5910\n",
      "Epoch [8/10], Batch [445/938], Loss: 0.6155\n",
      "Epoch [8/10], Batch [446/938], Loss: 0.5858\n",
      "Epoch [8/10], Batch [447/938], Loss: 0.5999\n",
      "Epoch [8/10], Batch [448/938], Loss: 0.6099\n",
      "Epoch [8/10], Batch [449/938], Loss: 0.5714\n",
      "Epoch [8/10], Batch [450/938], Loss: 0.5654\n",
      "Epoch [8/10], Batch [451/938], Loss: 0.6010\n",
      "Epoch [8/10], Batch [452/938], Loss: 0.5910\n",
      "Epoch [8/10], Batch [453/938], Loss: 0.5765\n",
      "Epoch [8/10], Batch [454/938], Loss: 0.5634\n",
      "Epoch [8/10], Batch [455/938], Loss: 0.5668\n",
      "Epoch [8/10], Batch [456/938], Loss: 0.5804\n",
      "Epoch [8/10], Batch [457/938], Loss: 0.6078\n",
      "Epoch [8/10], Batch [458/938], Loss: 0.5813\n",
      "Epoch [8/10], Batch [459/938], Loss: 0.5943\n",
      "Epoch [8/10], Batch [460/938], Loss: 0.5661\n",
      "Epoch [8/10], Batch [461/938], Loss: 0.5925\n",
      "Epoch [8/10], Batch [462/938], Loss: 0.5791\n",
      "Epoch [8/10], Batch [463/938], Loss: 0.6031\n",
      "Epoch [8/10], Batch [464/938], Loss: 0.5912\n",
      "Epoch [8/10], Batch [465/938], Loss: 0.6105\n",
      "Epoch [8/10], Batch [466/938], Loss: 0.5656\n",
      "Epoch [8/10], Batch [467/938], Loss: 0.5719\n",
      "Epoch [8/10], Batch [468/938], Loss: 0.5810\n",
      "Epoch [8/10], Batch [469/938], Loss: 0.5886\n",
      "Epoch [8/10], Batch [470/938], Loss: 0.5937\n",
      "Epoch [8/10], Batch [471/938], Loss: 0.5760\n",
      "Epoch [8/10], Batch [472/938], Loss: 0.6075\n",
      "Epoch [8/10], Batch [473/938], Loss: 0.5861\n",
      "Epoch [8/10], Batch [474/938], Loss: 0.5833\n",
      "Epoch [8/10], Batch [475/938], Loss: 0.5886\n",
      "Epoch [8/10], Batch [476/938], Loss: 0.5844\n",
      "Epoch [8/10], Batch [477/938], Loss: 0.6077\n",
      "Epoch [8/10], Batch [478/938], Loss: 0.5721\n",
      "Epoch [8/10], Batch [479/938], Loss: 0.5920\n",
      "Epoch [8/10], Batch [480/938], Loss: 0.5823\n",
      "Epoch [8/10], Batch [481/938], Loss: 0.6124\n",
      "Epoch [8/10], Batch [482/938], Loss: 0.5738\n",
      "Epoch [8/10], Batch [483/938], Loss: 0.5596\n",
      "Epoch [8/10], Batch [484/938], Loss: 0.6019\n",
      "Epoch [8/10], Batch [485/938], Loss: 0.5882\n",
      "Epoch [8/10], Batch [486/938], Loss: 0.5732\n",
      "Epoch [8/10], Batch [487/938], Loss: 0.5948\n",
      "Epoch [8/10], Batch [488/938], Loss: 0.5782\n",
      "Epoch [8/10], Batch [489/938], Loss: 0.5907\n",
      "Epoch [8/10], Batch [490/938], Loss: 0.5969\n",
      "Epoch [8/10], Batch [491/938], Loss: 0.5735\n",
      "Epoch [8/10], Batch [492/938], Loss: 0.5753\n",
      "Epoch [8/10], Batch [493/938], Loss: 0.5801\n",
      "Epoch [8/10], Batch [494/938], Loss: 0.5528\n",
      "Epoch [8/10], Batch [495/938], Loss: 0.5666\n",
      "Epoch [8/10], Batch [496/938], Loss: 0.6003\n",
      "Epoch [8/10], Batch [497/938], Loss: 0.5835\n",
      "Epoch [8/10], Batch [498/938], Loss: 0.5944\n",
      "Epoch [8/10], Batch [499/938], Loss: 0.5832\n",
      "Epoch [8/10], Batch [500/938], Loss: 0.6112\n",
      "Epoch [8/10], Batch [501/938], Loss: 0.5784\n",
      "Epoch [8/10], Batch [502/938], Loss: 0.5861\n",
      "Epoch [8/10], Batch [503/938], Loss: 0.5506\n",
      "Epoch [8/10], Batch [504/938], Loss: 0.6073\n",
      "Epoch [8/10], Batch [505/938], Loss: 0.5998\n",
      "Epoch [8/10], Batch [506/938], Loss: 0.6119\n",
      "Epoch [8/10], Batch [507/938], Loss: 0.5873\n",
      "Epoch [8/10], Batch [508/938], Loss: 0.5827\n",
      "Epoch [8/10], Batch [509/938], Loss: 0.5799\n",
      "Epoch [8/10], Batch [510/938], Loss: 0.5859\n",
      "Epoch [8/10], Batch [511/938], Loss: 0.6106\n",
      "Epoch [8/10], Batch [512/938], Loss: 0.5906\n",
      "Epoch [8/10], Batch [513/938], Loss: 0.5833\n",
      "Epoch [8/10], Batch [514/938], Loss: 0.5725\n",
      "Epoch [8/10], Batch [515/938], Loss: 0.5925\n",
      "Epoch [8/10], Batch [516/938], Loss: 0.5843\n",
      "Epoch [8/10], Batch [517/938], Loss: 0.5725\n",
      "Epoch [8/10], Batch [518/938], Loss: 0.5877\n",
      "Epoch [8/10], Batch [519/938], Loss: 0.5453\n",
      "Epoch [8/10], Batch [520/938], Loss: 0.5653\n",
      "Epoch [8/10], Batch [521/938], Loss: 0.5674\n",
      "Epoch [8/10], Batch [522/938], Loss: 0.5450\n",
      "Epoch [8/10], Batch [523/938], Loss: 0.5927\n",
      "Epoch [8/10], Batch [524/938], Loss: 0.5604\n",
      "Epoch [8/10], Batch [525/938], Loss: 0.5619\n",
      "Epoch [8/10], Batch [526/938], Loss: 0.6182\n",
      "Epoch [8/10], Batch [527/938], Loss: 0.5912\n",
      "Epoch [8/10], Batch [528/938], Loss: 0.5749\n",
      "Epoch [8/10], Batch [529/938], Loss: 0.5636\n",
      "Epoch [8/10], Batch [530/938], Loss: 0.5833\n",
      "Epoch [8/10], Batch [531/938], Loss: 0.6112\n",
      "Epoch [8/10], Batch [532/938], Loss: 0.5893\n",
      "Epoch [8/10], Batch [533/938], Loss: 0.5846\n",
      "Epoch [8/10], Batch [534/938], Loss: 0.6045\n",
      "Epoch [8/10], Batch [535/938], Loss: 0.5703\n",
      "Epoch [8/10], Batch [536/938], Loss: 0.6076\n",
      "Epoch [8/10], Batch [537/938], Loss: 0.6045\n",
      "Epoch [8/10], Batch [538/938], Loss: 0.5815\n",
      "Epoch [8/10], Batch [539/938], Loss: 0.5757\n",
      "Epoch [8/10], Batch [540/938], Loss: 0.6000\n",
      "Epoch [8/10], Batch [541/938], Loss: 0.5760\n",
      "Epoch [8/10], Batch [542/938], Loss: 0.5534\n",
      "Epoch [8/10], Batch [543/938], Loss: 0.5676\n",
      "Epoch [8/10], Batch [544/938], Loss: 0.5964\n",
      "Epoch [8/10], Batch [545/938], Loss: 0.5587\n",
      "Epoch [8/10], Batch [546/938], Loss: 0.5963\n",
      "Epoch [8/10], Batch [547/938], Loss: 0.5806\n",
      "Epoch [8/10], Batch [548/938], Loss: 0.6240\n",
      "Epoch [8/10], Batch [549/938], Loss: 0.6253\n",
      "Epoch [8/10], Batch [550/938], Loss: 0.5583\n",
      "Epoch [8/10], Batch [551/938], Loss: 0.5726\n",
      "Epoch [8/10], Batch [552/938], Loss: 0.5949\n",
      "Epoch [8/10], Batch [553/938], Loss: 0.5917\n",
      "Epoch [8/10], Batch [554/938], Loss: 0.5784\n",
      "Epoch [8/10], Batch [555/938], Loss: 0.5824\n",
      "Epoch [8/10], Batch [556/938], Loss: 0.5791\n",
      "Epoch [8/10], Batch [557/938], Loss: 0.5734\n",
      "Epoch [8/10], Batch [558/938], Loss: 0.5647\n",
      "Epoch [8/10], Batch [559/938], Loss: 0.5957\n",
      "Epoch [8/10], Batch [560/938], Loss: 0.5948\n",
      "Epoch [8/10], Batch [561/938], Loss: 0.5984\n",
      "Epoch [8/10], Batch [562/938], Loss: 0.6161\n",
      "Epoch [8/10], Batch [563/938], Loss: 0.6114\n",
      "Epoch [8/10], Batch [564/938], Loss: 0.6233\n",
      "Epoch [8/10], Batch [565/938], Loss: 0.5801\n",
      "Epoch [8/10], Batch [566/938], Loss: 0.5808\n",
      "Epoch [8/10], Batch [567/938], Loss: 0.5963\n",
      "Epoch [8/10], Batch [568/938], Loss: 0.5752\n",
      "Epoch [8/10], Batch [569/938], Loss: 0.5890\n",
      "Epoch [8/10], Batch [570/938], Loss: 0.5896\n",
      "Epoch [8/10], Batch [571/938], Loss: 0.6097\n",
      "Epoch [8/10], Batch [572/938], Loss: 0.5996\n",
      "Epoch [8/10], Batch [573/938], Loss: 0.6065\n",
      "Epoch [8/10], Batch [574/938], Loss: 0.5807\n",
      "Epoch [8/10], Batch [575/938], Loss: 0.6024\n",
      "Epoch [8/10], Batch [576/938], Loss: 0.5725\n",
      "Epoch [8/10], Batch [577/938], Loss: 0.6058\n",
      "Epoch [8/10], Batch [578/938], Loss: 0.5826\n",
      "Epoch [8/10], Batch [579/938], Loss: 0.5836\n",
      "Epoch [8/10], Batch [580/938], Loss: 0.5757\n",
      "Epoch [8/10], Batch [581/938], Loss: 0.5827\n",
      "Epoch [8/10], Batch [582/938], Loss: 0.6118\n",
      "Epoch [8/10], Batch [583/938], Loss: 0.5910\n",
      "Epoch [8/10], Batch [584/938], Loss: 0.5964\n",
      "Epoch [8/10], Batch [585/938], Loss: 0.5723\n",
      "Epoch [8/10], Batch [586/938], Loss: 0.5773\n",
      "Epoch [8/10], Batch [587/938], Loss: 0.6172\n",
      "Epoch [8/10], Batch [588/938], Loss: 0.5990\n",
      "Epoch [8/10], Batch [589/938], Loss: 0.5845\n",
      "Epoch [8/10], Batch [590/938], Loss: 0.5883\n",
      "Epoch [8/10], Batch [591/938], Loss: 0.6037\n",
      "Epoch [8/10], Batch [592/938], Loss: 0.5672\n",
      "Epoch [8/10], Batch [593/938], Loss: 0.5914\n",
      "Epoch [8/10], Batch [594/938], Loss: 0.5934\n",
      "Epoch [8/10], Batch [595/938], Loss: 0.5849\n",
      "Epoch [8/10], Batch [596/938], Loss: 0.6083\n",
      "Epoch [8/10], Batch [597/938], Loss: 0.5785\n",
      "Epoch [8/10], Batch [598/938], Loss: 0.6015\n",
      "Epoch [8/10], Batch [599/938], Loss: 0.5756\n",
      "Epoch [8/10], Batch [600/938], Loss: 0.5759\n",
      "Epoch [8/10], Batch [601/938], Loss: 0.5662\n",
      "Epoch [8/10], Batch [602/938], Loss: 0.5977\n",
      "Epoch [8/10], Batch [603/938], Loss: 0.5874\n",
      "Epoch [8/10], Batch [604/938], Loss: 0.6153\n",
      "Epoch [8/10], Batch [605/938], Loss: 0.5710\n",
      "Epoch [8/10], Batch [606/938], Loss: 0.5689\n",
      "Epoch [8/10], Batch [607/938], Loss: 0.5755\n",
      "Epoch [8/10], Batch [608/938], Loss: 0.6078\n",
      "Epoch [8/10], Batch [609/938], Loss: 0.5858\n",
      "Epoch [8/10], Batch [610/938], Loss: 0.5852\n",
      "Epoch [8/10], Batch [611/938], Loss: 0.5761\n",
      "Epoch [8/10], Batch [612/938], Loss: 0.6181\n",
      "Epoch [8/10], Batch [613/938], Loss: 0.5569\n",
      "Epoch [8/10], Batch [614/938], Loss: 0.6014\n",
      "Epoch [8/10], Batch [615/938], Loss: 0.5971\n",
      "Epoch [8/10], Batch [616/938], Loss: 0.5858\n",
      "Epoch [8/10], Batch [617/938], Loss: 0.5435\n",
      "Epoch [8/10], Batch [618/938], Loss: 0.6014\n",
      "Epoch [8/10], Batch [619/938], Loss: 0.6098\n",
      "Epoch [8/10], Batch [620/938], Loss: 0.5851\n",
      "Epoch [8/10], Batch [621/938], Loss: 0.6093\n",
      "Epoch [8/10], Batch [622/938], Loss: 0.5973\n",
      "Epoch [8/10], Batch [623/938], Loss: 0.6015\n",
      "Epoch [8/10], Batch [624/938], Loss: 0.5514\n",
      "Epoch [8/10], Batch [625/938], Loss: 0.5755\n",
      "Epoch [8/10], Batch [626/938], Loss: 0.5770\n",
      "Epoch [8/10], Batch [627/938], Loss: 0.5793\n",
      "Epoch [8/10], Batch [628/938], Loss: 0.5810\n",
      "Epoch [8/10], Batch [629/938], Loss: 0.6017\n",
      "Epoch [8/10], Batch [630/938], Loss: 0.5840\n",
      "Epoch [8/10], Batch [631/938], Loss: 0.5800\n",
      "Epoch [8/10], Batch [632/938], Loss: 0.5813\n",
      "Epoch [8/10], Batch [633/938], Loss: 0.5938\n",
      "Epoch [8/10], Batch [634/938], Loss: 0.5839\n",
      "Epoch [8/10], Batch [635/938], Loss: 0.5710\n",
      "Epoch [8/10], Batch [636/938], Loss: 0.5906\n",
      "Epoch [8/10], Batch [637/938], Loss: 0.5709\n",
      "Epoch [8/10], Batch [638/938], Loss: 0.5846\n",
      "Epoch [8/10], Batch [639/938], Loss: 0.5797\n",
      "Epoch [8/10], Batch [640/938], Loss: 0.5579\n",
      "Epoch [8/10], Batch [641/938], Loss: 0.5897\n",
      "Epoch [8/10], Batch [642/938], Loss: 0.5700\n",
      "Epoch [8/10], Batch [643/938], Loss: 0.6132\n",
      "Epoch [8/10], Batch [644/938], Loss: 0.6029\n",
      "Epoch [8/10], Batch [645/938], Loss: 0.5864\n",
      "Epoch [8/10], Batch [646/938], Loss: 0.6019\n",
      "Epoch [8/10], Batch [647/938], Loss: 0.5892\n",
      "Epoch [8/10], Batch [648/938], Loss: 0.5629\n",
      "Epoch [8/10], Batch [649/938], Loss: 0.5920\n",
      "Epoch [8/10], Batch [650/938], Loss: 0.5644\n",
      "Epoch [8/10], Batch [651/938], Loss: 0.5785\n",
      "Epoch [8/10], Batch [652/938], Loss: 0.5754\n",
      "Epoch [8/10], Batch [653/938], Loss: 0.5934\n",
      "Epoch [8/10], Batch [654/938], Loss: 0.5958\n",
      "Epoch [8/10], Batch [655/938], Loss: 0.6166\n",
      "Epoch [8/10], Batch [656/938], Loss: 0.6023\n",
      "Epoch [8/10], Batch [657/938], Loss: 0.5647\n",
      "Epoch [8/10], Batch [658/938], Loss: 0.6110\n",
      "Epoch [8/10], Batch [659/938], Loss: 0.5804\n",
      "Epoch [8/10], Batch [660/938], Loss: 0.5927\n",
      "Epoch [8/10], Batch [661/938], Loss: 0.5710\n",
      "Epoch [8/10], Batch [662/938], Loss: 0.5868\n",
      "Epoch [8/10], Batch [663/938], Loss: 0.5957\n",
      "Epoch [8/10], Batch [664/938], Loss: 0.5783\n",
      "Epoch [8/10], Batch [665/938], Loss: 0.6140\n",
      "Epoch [8/10], Batch [666/938], Loss: 0.5623\n",
      "Epoch [8/10], Batch [667/938], Loss: 0.5829\n",
      "Epoch [8/10], Batch [668/938], Loss: 0.5799\n",
      "Epoch [8/10], Batch [669/938], Loss: 0.5887\n",
      "Epoch [8/10], Batch [670/938], Loss: 0.6077\n",
      "Epoch [8/10], Batch [671/938], Loss: 0.6224\n",
      "Epoch [8/10], Batch [672/938], Loss: 0.5863\n",
      "Epoch [8/10], Batch [673/938], Loss: 0.6055\n",
      "Epoch [8/10], Batch [674/938], Loss: 0.5693\n",
      "Epoch [8/10], Batch [675/938], Loss: 0.5879\n",
      "Epoch [8/10], Batch [676/938], Loss: 0.5636\n",
      "Epoch [8/10], Batch [677/938], Loss: 0.6012\n",
      "Epoch [8/10], Batch [678/938], Loss: 0.5642\n",
      "Epoch [8/10], Batch [679/938], Loss: 0.6040\n",
      "Epoch [8/10], Batch [680/938], Loss: 0.5760\n",
      "Epoch [8/10], Batch [681/938], Loss: 0.5984\n",
      "Epoch [8/10], Batch [682/938], Loss: 0.5668\n",
      "Epoch [8/10], Batch [683/938], Loss: 0.5973\n",
      "Epoch [8/10], Batch [684/938], Loss: 0.5763\n",
      "Epoch [8/10], Batch [685/938], Loss: 0.5798\n",
      "Epoch [8/10], Batch [686/938], Loss: 0.6118\n",
      "Epoch [8/10], Batch [687/938], Loss: 0.5851\n",
      "Epoch [8/10], Batch [688/938], Loss: 0.5635\n",
      "Epoch [8/10], Batch [689/938], Loss: 0.5725\n",
      "Epoch [8/10], Batch [690/938], Loss: 0.6241\n",
      "Epoch [8/10], Batch [691/938], Loss: 0.5772\n",
      "Epoch [8/10], Batch [692/938], Loss: 0.6090\n",
      "Epoch [8/10], Batch [693/938], Loss: 0.5723\n",
      "Epoch [8/10], Batch [694/938], Loss: 0.5735\n",
      "Epoch [8/10], Batch [695/938], Loss: 0.6157\n",
      "Epoch [8/10], Batch [696/938], Loss: 0.5947\n",
      "Epoch [8/10], Batch [697/938], Loss: 0.6226\n",
      "Epoch [8/10], Batch [698/938], Loss: 0.5825\n",
      "Epoch [8/10], Batch [699/938], Loss: 0.5703\n",
      "Epoch [8/10], Batch [700/938], Loss: 0.6017\n",
      "Epoch [8/10], Batch [701/938], Loss: 0.5714\n",
      "Epoch [8/10], Batch [702/938], Loss: 0.5961\n",
      "Epoch [8/10], Batch [703/938], Loss: 0.5988\n",
      "Epoch [8/10], Batch [704/938], Loss: 0.6172\n",
      "Epoch [8/10], Batch [705/938], Loss: 0.5690\n",
      "Epoch [8/10], Batch [706/938], Loss: 0.6094\n",
      "Epoch [8/10], Batch [707/938], Loss: 0.5775\n",
      "Epoch [8/10], Batch [708/938], Loss: 0.5843\n",
      "Epoch [8/10], Batch [709/938], Loss: 0.5927\n",
      "Epoch [8/10], Batch [710/938], Loss: 0.5563\n",
      "Epoch [8/10], Batch [711/938], Loss: 0.5725\n",
      "Epoch [8/10], Batch [712/938], Loss: 0.5764\n",
      "Epoch [8/10], Batch [713/938], Loss: 0.5747\n",
      "Epoch [8/10], Batch [714/938], Loss: 0.6194\n",
      "Epoch [8/10], Batch [715/938], Loss: 0.5556\n",
      "Epoch [8/10], Batch [716/938], Loss: 0.5959\n",
      "Epoch [8/10], Batch [717/938], Loss: 0.5744\n",
      "Epoch [8/10], Batch [718/938], Loss: 0.5782\n",
      "Epoch [8/10], Batch [719/938], Loss: 0.5777\n",
      "Epoch [8/10], Batch [720/938], Loss: 0.5695\n",
      "Epoch [8/10], Batch [721/938], Loss: 0.5995\n",
      "Epoch [8/10], Batch [722/938], Loss: 0.5914\n",
      "Epoch [8/10], Batch [723/938], Loss: 0.6108\n",
      "Epoch [8/10], Batch [724/938], Loss: 0.5746\n",
      "Epoch [8/10], Batch [725/938], Loss: 0.5708\n",
      "Epoch [8/10], Batch [726/938], Loss: 0.6006\n",
      "Epoch [8/10], Batch [727/938], Loss: 0.5993\n",
      "Epoch [8/10], Batch [728/938], Loss: 0.5914\n",
      "Epoch [8/10], Batch [729/938], Loss: 0.6108\n",
      "Epoch [8/10], Batch [730/938], Loss: 0.5517\n",
      "Epoch [8/10], Batch [731/938], Loss: 0.5874\n",
      "Epoch [8/10], Batch [732/938], Loss: 0.5913\n",
      "Epoch [8/10], Batch [733/938], Loss: 0.5663\n",
      "Epoch [8/10], Batch [734/938], Loss: 0.5887\n",
      "Epoch [8/10], Batch [735/938], Loss: 0.5872\n",
      "Epoch [8/10], Batch [736/938], Loss: 0.6057\n",
      "Epoch [8/10], Batch [737/938], Loss: 0.6252\n",
      "Epoch [8/10], Batch [738/938], Loss: 0.5655\n",
      "Epoch [8/10], Batch [739/938], Loss: 0.6041\n",
      "Epoch [8/10], Batch [740/938], Loss: 0.5766\n",
      "Epoch [8/10], Batch [741/938], Loss: 0.5986\n",
      "Epoch [8/10], Batch [742/938], Loss: 0.5940\n",
      "Epoch [8/10], Batch [743/938], Loss: 0.5750\n",
      "Epoch [8/10], Batch [744/938], Loss: 0.5704\n",
      "Epoch [8/10], Batch [745/938], Loss: 0.6103\n",
      "Epoch [8/10], Batch [746/938], Loss: 0.5738\n",
      "Epoch [8/10], Batch [747/938], Loss: 0.5967\n",
      "Epoch [8/10], Batch [748/938], Loss: 0.5777\n",
      "Epoch [8/10], Batch [749/938], Loss: 0.5603\n",
      "Epoch [8/10], Batch [750/938], Loss: 0.5951\n",
      "Epoch [8/10], Batch [751/938], Loss: 0.5785\n",
      "Epoch [8/10], Batch [752/938], Loss: 0.5938\n",
      "Epoch [8/10], Batch [753/938], Loss: 0.5868\n",
      "Epoch [8/10], Batch [754/938], Loss: 0.5823\n",
      "Epoch [8/10], Batch [755/938], Loss: 0.5841\n",
      "Epoch [8/10], Batch [756/938], Loss: 0.5856\n",
      "Epoch [8/10], Batch [757/938], Loss: 0.5754\n",
      "Epoch [8/10], Batch [758/938], Loss: 0.5788\n",
      "Epoch [8/10], Batch [759/938], Loss: 0.5728\n",
      "Epoch [8/10], Batch [760/938], Loss: 0.6039\n",
      "Epoch [8/10], Batch [761/938], Loss: 0.5578\n",
      "Epoch [8/10], Batch [762/938], Loss: 0.5857\n",
      "Epoch [8/10], Batch [763/938], Loss: 0.5564\n",
      "Epoch [8/10], Batch [764/938], Loss: 0.5757\n",
      "Epoch [8/10], Batch [765/938], Loss: 0.5909\n",
      "Epoch [8/10], Batch [766/938], Loss: 0.5655\n",
      "Epoch [8/10], Batch [767/938], Loss: 0.5841\n",
      "Epoch [8/10], Batch [768/938], Loss: 0.5822\n",
      "Epoch [8/10], Batch [769/938], Loss: 0.5887\n",
      "Epoch [8/10], Batch [770/938], Loss: 0.6013\n",
      "Epoch [8/10], Batch [771/938], Loss: 0.5736\n",
      "Epoch [8/10], Batch [772/938], Loss: 0.5689\n",
      "Epoch [8/10], Batch [773/938], Loss: 0.6057\n",
      "Epoch [8/10], Batch [774/938], Loss: 0.5921\n",
      "Epoch [8/10], Batch [775/938], Loss: 0.5801\n",
      "Epoch [8/10], Batch [776/938], Loss: 0.5906\n",
      "Epoch [8/10], Batch [777/938], Loss: 0.6053\n",
      "Epoch [8/10], Batch [778/938], Loss: 0.5779\n",
      "Epoch [8/10], Batch [779/938], Loss: 0.5896\n",
      "Epoch [8/10], Batch [780/938], Loss: 0.5569\n",
      "Epoch [8/10], Batch [781/938], Loss: 0.5962\n",
      "Epoch [8/10], Batch [782/938], Loss: 0.5912\n",
      "Epoch [8/10], Batch [783/938], Loss: 0.5837\n",
      "Epoch [8/10], Batch [784/938], Loss: 0.5972\n",
      "Epoch [8/10], Batch [785/938], Loss: 0.5974\n",
      "Epoch [8/10], Batch [786/938], Loss: 0.5876\n",
      "Epoch [8/10], Batch [787/938], Loss: 0.6184\n",
      "Epoch [8/10], Batch [788/938], Loss: 0.6142\n",
      "Epoch [8/10], Batch [789/938], Loss: 0.6006\n",
      "Epoch [8/10], Batch [790/938], Loss: 0.5699\n",
      "Epoch [8/10], Batch [791/938], Loss: 0.5622\n",
      "Epoch [8/10], Batch [792/938], Loss: 0.5766\n",
      "Epoch [8/10], Batch [793/938], Loss: 0.5879\n",
      "Epoch [8/10], Batch [794/938], Loss: 0.6116\n",
      "Epoch [8/10], Batch [795/938], Loss: 0.5918\n",
      "Epoch [8/10], Batch [796/938], Loss: 0.5879\n",
      "Epoch [8/10], Batch [797/938], Loss: 0.5956\n",
      "Epoch [8/10], Batch [798/938], Loss: 0.6208\n",
      "Epoch [8/10], Batch [799/938], Loss: 0.6018\n",
      "Epoch [8/10], Batch [800/938], Loss: 0.5736\n",
      "Epoch [8/10], Batch [801/938], Loss: 0.5836\n",
      "Epoch [8/10], Batch [802/938], Loss: 0.5543\n",
      "Epoch [8/10], Batch [803/938], Loss: 0.5718\n",
      "Epoch [8/10], Batch [804/938], Loss: 0.5871\n",
      "Epoch [8/10], Batch [805/938], Loss: 0.5833\n",
      "Epoch [8/10], Batch [806/938], Loss: 0.5919\n",
      "Epoch [8/10], Batch [807/938], Loss: 0.5803\n",
      "Epoch [8/10], Batch [808/938], Loss: 0.6274\n",
      "Epoch [8/10], Batch [809/938], Loss: 0.6116\n",
      "Epoch [8/10], Batch [810/938], Loss: 0.5962\n",
      "Epoch [8/10], Batch [811/938], Loss: 0.5915\n",
      "Epoch [8/10], Batch [812/938], Loss: 0.5767\n",
      "Epoch [8/10], Batch [813/938], Loss: 0.5887\n",
      "Epoch [8/10], Batch [814/938], Loss: 0.6023\n",
      "Epoch [8/10], Batch [815/938], Loss: 0.5712\n",
      "Epoch [8/10], Batch [816/938], Loss: 0.6228\n",
      "Epoch [8/10], Batch [817/938], Loss: 0.5710\n",
      "Epoch [8/10], Batch [818/938], Loss: 0.5995\n",
      "Epoch [8/10], Batch [819/938], Loss: 0.5837\n",
      "Epoch [8/10], Batch [820/938], Loss: 0.5828\n",
      "Epoch [8/10], Batch [821/938], Loss: 0.5899\n",
      "Epoch [8/10], Batch [822/938], Loss: 0.5989\n",
      "Epoch [8/10], Batch [823/938], Loss: 0.5803\n",
      "Epoch [8/10], Batch [824/938], Loss: 0.6082\n",
      "Epoch [8/10], Batch [825/938], Loss: 0.5905\n",
      "Epoch [8/10], Batch [826/938], Loss: 0.5803\n",
      "Epoch [8/10], Batch [827/938], Loss: 0.5836\n",
      "Epoch [8/10], Batch [828/938], Loss: 0.5663\n",
      "Epoch [8/10], Batch [829/938], Loss: 0.5823\n",
      "Epoch [8/10], Batch [830/938], Loss: 0.5866\n",
      "Epoch [8/10], Batch [831/938], Loss: 0.6008\n",
      "Epoch [8/10], Batch [832/938], Loss: 0.5689\n",
      "Epoch [8/10], Batch [833/938], Loss: 0.6134\n",
      "Epoch [8/10], Batch [834/938], Loss: 0.5736\n",
      "Epoch [8/10], Batch [835/938], Loss: 0.5667\n",
      "Epoch [8/10], Batch [836/938], Loss: 0.5900\n",
      "Epoch [8/10], Batch [837/938], Loss: 0.5857\n",
      "Epoch [8/10], Batch [838/938], Loss: 0.5782\n",
      "Epoch [8/10], Batch [839/938], Loss: 0.5836\n",
      "Epoch [8/10], Batch [840/938], Loss: 0.6193\n",
      "Epoch [8/10], Batch [841/938], Loss: 0.6049\n",
      "Epoch [8/10], Batch [842/938], Loss: 0.5943\n",
      "Epoch [8/10], Batch [843/938], Loss: 0.5856\n",
      "Epoch [8/10], Batch [844/938], Loss: 0.5741\n",
      "Epoch [8/10], Batch [845/938], Loss: 0.5641\n",
      "Epoch [8/10], Batch [846/938], Loss: 0.5632\n",
      "Epoch [8/10], Batch [847/938], Loss: 0.6046\n",
      "Epoch [8/10], Batch [848/938], Loss: 0.5763\n",
      "Epoch [8/10], Batch [849/938], Loss: 0.5947\n",
      "Epoch [8/10], Batch [850/938], Loss: 0.5696\n",
      "Epoch [8/10], Batch [851/938], Loss: 0.6037\n",
      "Epoch [8/10], Batch [852/938], Loss: 0.5883\n",
      "Epoch [8/10], Batch [853/938], Loss: 0.5693\n",
      "Epoch [8/10], Batch [854/938], Loss: 0.5913\n",
      "Epoch [8/10], Batch [855/938], Loss: 0.5794\n",
      "Epoch [8/10], Batch [856/938], Loss: 0.5915\n",
      "Epoch [8/10], Batch [857/938], Loss: 0.5925\n",
      "Epoch [8/10], Batch [858/938], Loss: 0.6175\n",
      "Epoch [8/10], Batch [859/938], Loss: 0.5794\n",
      "Epoch [8/10], Batch [860/938], Loss: 0.5954\n",
      "Epoch [8/10], Batch [861/938], Loss: 0.5882\n",
      "Epoch [8/10], Batch [862/938], Loss: 0.5881\n",
      "Epoch [8/10], Batch [863/938], Loss: 0.5811\n",
      "Epoch [8/10], Batch [864/938], Loss: 0.5957\n",
      "Epoch [8/10], Batch [865/938], Loss: 0.5680\n",
      "Epoch [8/10], Batch [866/938], Loss: 0.5747\n",
      "Epoch [8/10], Batch [867/938], Loss: 0.5955\n",
      "Epoch [8/10], Batch [868/938], Loss: 0.5972\n",
      "Epoch [8/10], Batch [869/938], Loss: 0.5895\n",
      "Epoch [8/10], Batch [870/938], Loss: 0.5744\n",
      "Epoch [8/10], Batch [871/938], Loss: 0.5955\n",
      "Epoch [8/10], Batch [872/938], Loss: 0.5962\n",
      "Epoch [8/10], Batch [873/938], Loss: 0.5685\n",
      "Epoch [8/10], Batch [874/938], Loss: 0.5756\n",
      "Epoch [8/10], Batch [875/938], Loss: 0.6040\n",
      "Epoch [8/10], Batch [876/938], Loss: 0.5724\n",
      "Epoch [8/10], Batch [877/938], Loss: 0.5439\n",
      "Epoch [8/10], Batch [878/938], Loss: 0.5953\n",
      "Epoch [8/10], Batch [879/938], Loss: 0.5745\n",
      "Epoch [8/10], Batch [880/938], Loss: 0.5946\n",
      "Epoch [8/10], Batch [881/938], Loss: 0.5924\n",
      "Epoch [8/10], Batch [882/938], Loss: 0.6229\n",
      "Epoch [8/10], Batch [883/938], Loss: 0.5846\n",
      "Epoch [8/10], Batch [884/938], Loss: 0.5753\n",
      "Epoch [8/10], Batch [885/938], Loss: 0.6000\n",
      "Epoch [8/10], Batch [886/938], Loss: 0.5583\n",
      "Epoch [8/10], Batch [887/938], Loss: 0.6204\n",
      "Epoch [8/10], Batch [888/938], Loss: 0.5770\n",
      "Epoch [8/10], Batch [889/938], Loss: 0.6191\n",
      "Epoch [8/10], Batch [890/938], Loss: 0.5727\n",
      "Epoch [8/10], Batch [891/938], Loss: 0.5853\n",
      "Epoch [8/10], Batch [892/938], Loss: 0.5899\n",
      "Epoch [8/10], Batch [893/938], Loss: 0.6028\n",
      "Epoch [8/10], Batch [894/938], Loss: 0.5759\n",
      "Epoch [8/10], Batch [895/938], Loss: 0.5765\n",
      "Epoch [8/10], Batch [896/938], Loss: 0.6128\n",
      "Epoch [8/10], Batch [897/938], Loss: 0.5447\n",
      "Epoch [8/10], Batch [898/938], Loss: 0.5640\n",
      "Epoch [8/10], Batch [899/938], Loss: 0.5764\n",
      "Epoch [8/10], Batch [900/938], Loss: 0.6143\n",
      "Epoch [8/10], Batch [901/938], Loss: 0.5848\n",
      "Epoch [8/10], Batch [902/938], Loss: 0.5837\n",
      "Epoch [8/10], Batch [903/938], Loss: 0.5731\n",
      "Epoch [8/10], Batch [904/938], Loss: 0.5591\n",
      "Epoch [8/10], Batch [905/938], Loss: 0.6068\n",
      "Epoch [8/10], Batch [906/938], Loss: 0.5944\n",
      "Epoch [8/10], Batch [907/938], Loss: 0.6112\n",
      "Epoch [8/10], Batch [908/938], Loss: 0.5975\n",
      "Epoch [8/10], Batch [909/938], Loss: 0.5757\n",
      "Epoch [8/10], Batch [910/938], Loss: 0.5965\n",
      "Epoch [8/10], Batch [911/938], Loss: 0.5777\n",
      "Epoch [8/10], Batch [912/938], Loss: 0.5595\n",
      "Epoch [8/10], Batch [913/938], Loss: 0.5641\n",
      "Epoch [8/10], Batch [914/938], Loss: 0.5906\n",
      "Epoch [8/10], Batch [915/938], Loss: 0.5809\n",
      "Epoch [8/10], Batch [916/938], Loss: 0.6068\n",
      "Epoch [8/10], Batch [917/938], Loss: 0.5758\n",
      "Epoch [8/10], Batch [918/938], Loss: 0.5956\n",
      "Epoch [8/10], Batch [919/938], Loss: 0.5695\n",
      "Epoch [8/10], Batch [920/938], Loss: 0.5825\n",
      "Epoch [8/10], Batch [921/938], Loss: 0.5759\n",
      "Epoch [8/10], Batch [922/938], Loss: 0.5853\n",
      "Epoch [8/10], Batch [923/938], Loss: 0.5774\n",
      "Epoch [8/10], Batch [924/938], Loss: 0.5888\n",
      "Epoch [8/10], Batch [925/938], Loss: 0.5721\n",
      "Epoch [8/10], Batch [926/938], Loss: 0.5724\n",
      "Epoch [8/10], Batch [927/938], Loss: 0.6090\n",
      "Epoch [8/10], Batch [928/938], Loss: 0.6038\n",
      "Epoch [8/10], Batch [929/938], Loss: 0.5882\n",
      "Epoch [8/10], Batch [930/938], Loss: 0.5849\n",
      "Epoch [8/10], Batch [931/938], Loss: 0.5973\n",
      "Epoch [8/10], Batch [932/938], Loss: 0.6033\n",
      "Epoch [8/10], Batch [933/938], Loss: 0.5699\n",
      "Epoch [8/10], Batch [934/938], Loss: 0.5557\n",
      "Epoch [8/10], Batch [935/938], Loss: 0.5744\n",
      "Epoch [8/10], Batch [936/938], Loss: 0.5926\n",
      "Epoch [8/10], Batch [937/938], Loss: 0.5469\n",
      "Epoch [8/10], Batch [938/938], Loss: 0.5695\n",
      "Epoch [8/10], Loss: 0.5695\n",
      "Epoch [9/10], Batch [1/938], Loss: 0.5849\n",
      "Epoch [9/10], Batch [2/938], Loss: 0.5671\n",
      "Epoch [9/10], Batch [3/938], Loss: 0.5694\n",
      "Epoch [9/10], Batch [4/938], Loss: 0.5898\n",
      "Epoch [9/10], Batch [5/938], Loss: 0.5872\n",
      "Epoch [9/10], Batch [6/938], Loss: 0.5784\n",
      "Epoch [9/10], Batch [7/938], Loss: 0.5982\n",
      "Epoch [9/10], Batch [8/938], Loss: 0.5855\n",
      "Epoch [9/10], Batch [9/938], Loss: 0.5675\n",
      "Epoch [9/10], Batch [10/938], Loss: 0.5933\n",
      "Epoch [9/10], Batch [11/938], Loss: 0.5833\n",
      "Epoch [9/10], Batch [12/938], Loss: 0.6165\n",
      "Epoch [9/10], Batch [13/938], Loss: 0.5601\n",
      "Epoch [9/10], Batch [14/938], Loss: 0.5828\n",
      "Epoch [9/10], Batch [15/938], Loss: 0.5667\n",
      "Epoch [9/10], Batch [16/938], Loss: 0.5563\n",
      "Epoch [9/10], Batch [17/938], Loss: 0.5860\n",
      "Epoch [9/10], Batch [18/938], Loss: 0.5701\n",
      "Epoch [9/10], Batch [19/938], Loss: 0.5704\n",
      "Epoch [9/10], Batch [20/938], Loss: 0.5799\n",
      "Epoch [9/10], Batch [21/938], Loss: 0.5937\n",
      "Epoch [9/10], Batch [22/938], Loss: 0.5831\n",
      "Epoch [9/10], Batch [23/938], Loss: 0.5765\n",
      "Epoch [9/10], Batch [24/938], Loss: 0.5787\n",
      "Epoch [9/10], Batch [25/938], Loss: 0.5784\n",
      "Epoch [9/10], Batch [26/938], Loss: 0.5724\n",
      "Epoch [9/10], Batch [27/938], Loss: 0.6052\n",
      "Epoch [9/10], Batch [28/938], Loss: 0.5751\n",
      "Epoch [9/10], Batch [29/938], Loss: 0.5869\n",
      "Epoch [9/10], Batch [30/938], Loss: 0.5748\n",
      "Epoch [9/10], Batch [31/938], Loss: 0.5898\n",
      "Epoch [9/10], Batch [32/938], Loss: 0.5921\n",
      "Epoch [9/10], Batch [33/938], Loss: 0.5608\n",
      "Epoch [9/10], Batch [34/938], Loss: 0.5617\n",
      "Epoch [9/10], Batch [35/938], Loss: 0.6052\n",
      "Epoch [9/10], Batch [36/938], Loss: 0.5777\n",
      "Epoch [9/10], Batch [37/938], Loss: 0.5821\n",
      "Epoch [9/10], Batch [38/938], Loss: 0.5637\n",
      "Epoch [9/10], Batch [39/938], Loss: 0.5862\n",
      "Epoch [9/10], Batch [40/938], Loss: 0.5769\n",
      "Epoch [9/10], Batch [41/938], Loss: 0.5848\n",
      "Epoch [9/10], Batch [42/938], Loss: 0.5697\n",
      "Epoch [9/10], Batch [43/938], Loss: 0.5923\n",
      "Epoch [9/10], Batch [44/938], Loss: 0.5774\n",
      "Epoch [9/10], Batch [45/938], Loss: 0.5956\n",
      "Epoch [9/10], Batch [46/938], Loss: 0.6124\n",
      "Epoch [9/10], Batch [47/938], Loss: 0.5761\n",
      "Epoch [9/10], Batch [48/938], Loss: 0.5872\n",
      "Epoch [9/10], Batch [49/938], Loss: 0.5890\n",
      "Epoch [9/10], Batch [50/938], Loss: 0.6010\n",
      "Epoch [9/10], Batch [51/938], Loss: 0.5646\n",
      "Epoch [9/10], Batch [52/938], Loss: 0.5923\n",
      "Epoch [9/10], Batch [53/938], Loss: 0.5941\n",
      "Epoch [9/10], Batch [54/938], Loss: 0.6074\n",
      "Epoch [9/10], Batch [55/938], Loss: 0.5964\n",
      "Epoch [9/10], Batch [56/938], Loss: 0.5896\n",
      "Epoch [9/10], Batch [57/938], Loss: 0.5739\n",
      "Epoch [9/10], Batch [58/938], Loss: 0.5908\n",
      "Epoch [9/10], Batch [59/938], Loss: 0.6290\n",
      "Epoch [9/10], Batch [60/938], Loss: 0.5772\n",
      "Epoch [9/10], Batch [61/938], Loss: 0.5808\n",
      "Epoch [9/10], Batch [62/938], Loss: 0.5986\n",
      "Epoch [9/10], Batch [63/938], Loss: 0.5688\n",
      "Epoch [9/10], Batch [64/938], Loss: 0.5846\n",
      "Epoch [9/10], Batch [65/938], Loss: 0.5749\n",
      "Epoch [9/10], Batch [66/938], Loss: 0.5756\n",
      "Epoch [9/10], Batch [67/938], Loss: 0.5939\n",
      "Epoch [9/10], Batch [68/938], Loss: 0.5968\n",
      "Epoch [9/10], Batch [69/938], Loss: 0.6063\n",
      "Epoch [9/10], Batch [70/938], Loss: 0.5788\n",
      "Epoch [9/10], Batch [71/938], Loss: 0.5816\n",
      "Epoch [9/10], Batch [72/938], Loss: 0.5883\n",
      "Epoch [9/10], Batch [73/938], Loss: 0.5950\n",
      "Epoch [9/10], Batch [74/938], Loss: 0.5722\n",
      "Epoch [9/10], Batch [75/938], Loss: 0.5709\n",
      "Epoch [9/10], Batch [76/938], Loss: 0.5819\n",
      "Epoch [9/10], Batch [77/938], Loss: 0.5510\n",
      "Epoch [9/10], Batch [78/938], Loss: 0.5746\n",
      "Epoch [9/10], Batch [79/938], Loss: 0.5729\n",
      "Epoch [9/10], Batch [80/938], Loss: 0.5899\n",
      "Epoch [9/10], Batch [81/938], Loss: 0.5971\n",
      "Epoch [9/10], Batch [82/938], Loss: 0.5805\n",
      "Epoch [9/10], Batch [83/938], Loss: 0.5935\n",
      "Epoch [9/10], Batch [84/938], Loss: 0.6021\n",
      "Epoch [9/10], Batch [85/938], Loss: 0.5963\n",
      "Epoch [9/10], Batch [86/938], Loss: 0.5887\n",
      "Epoch [9/10], Batch [87/938], Loss: 0.5755\n",
      "Epoch [9/10], Batch [88/938], Loss: 0.5717\n",
      "Epoch [9/10], Batch [89/938], Loss: 0.6135\n",
      "Epoch [9/10], Batch [90/938], Loss: 0.5878\n",
      "Epoch [9/10], Batch [91/938], Loss: 0.5840\n",
      "Epoch [9/10], Batch [92/938], Loss: 0.5570\n",
      "Epoch [9/10], Batch [93/938], Loss: 0.5987\n",
      "Epoch [9/10], Batch [94/938], Loss: 0.5737\n",
      "Epoch [9/10], Batch [95/938], Loss: 0.5676\n",
      "Epoch [9/10], Batch [96/938], Loss: 0.5711\n",
      "Epoch [9/10], Batch [97/938], Loss: 0.5966\n",
      "Epoch [9/10], Batch [98/938], Loss: 0.5791\n",
      "Epoch [9/10], Batch [99/938], Loss: 0.5655\n",
      "Epoch [9/10], Batch [100/938], Loss: 0.5749\n",
      "Epoch [9/10], Batch [101/938], Loss: 0.5407\n",
      "Epoch [9/10], Batch [102/938], Loss: 0.5689\n",
      "Epoch [9/10], Batch [103/938], Loss: 0.5602\n",
      "Epoch [9/10], Batch [104/938], Loss: 0.6162\n",
      "Epoch [9/10], Batch [105/938], Loss: 0.5849\n",
      "Epoch [9/10], Batch [106/938], Loss: 0.5892\n",
      "Epoch [9/10], Batch [107/938], Loss: 0.5890\n",
      "Epoch [9/10], Batch [108/938], Loss: 0.5507\n",
      "Epoch [9/10], Batch [109/938], Loss: 0.5735\n",
      "Epoch [9/10], Batch [110/938], Loss: 0.5806\n",
      "Epoch [9/10], Batch [111/938], Loss: 0.5971\n",
      "Epoch [9/10], Batch [112/938], Loss: 0.6022\n",
      "Epoch [9/10], Batch [113/938], Loss: 0.5663\n",
      "Epoch [9/10], Batch [114/938], Loss: 0.5851\n",
      "Epoch [9/10], Batch [115/938], Loss: 0.6106\n",
      "Epoch [9/10], Batch [116/938], Loss: 0.5730\n",
      "Epoch [9/10], Batch [117/938], Loss: 0.5464\n",
      "Epoch [9/10], Batch [118/938], Loss: 0.6139\n",
      "Epoch [9/10], Batch [119/938], Loss: 0.5851\n",
      "Epoch [9/10], Batch [120/938], Loss: 0.6232\n",
      "Epoch [9/10], Batch [121/938], Loss: 0.5891\n",
      "Epoch [9/10], Batch [122/938], Loss: 0.5795\n",
      "Epoch [9/10], Batch [123/938], Loss: 0.5930\n",
      "Epoch [9/10], Batch [124/938], Loss: 0.5941\n",
      "Epoch [9/10], Batch [125/938], Loss: 0.5678\n",
      "Epoch [9/10], Batch [126/938], Loss: 0.5547\n",
      "Epoch [9/10], Batch [127/938], Loss: 0.5990\n",
      "Epoch [9/10], Batch [128/938], Loss: 0.5842\n",
      "Epoch [9/10], Batch [129/938], Loss: 0.5517\n",
      "Epoch [9/10], Batch [130/938], Loss: 0.5948\n",
      "Epoch [9/10], Batch [131/938], Loss: 0.5944\n",
      "Epoch [9/10], Batch [132/938], Loss: 0.5859\n",
      "Epoch [9/10], Batch [133/938], Loss: 0.5892\n",
      "Epoch [9/10], Batch [134/938], Loss: 0.5885\n",
      "Epoch [9/10], Batch [135/938], Loss: 0.5659\n",
      "Epoch [9/10], Batch [136/938], Loss: 0.5766\n",
      "Epoch [9/10], Batch [137/938], Loss: 0.5585\n",
      "Epoch [9/10], Batch [138/938], Loss: 0.5686\n",
      "Epoch [9/10], Batch [139/938], Loss: 0.5879\n",
      "Epoch [9/10], Batch [140/938], Loss: 0.5566\n",
      "Epoch [9/10], Batch [141/938], Loss: 0.5870\n",
      "Epoch [9/10], Batch [142/938], Loss: 0.5789\n",
      "Epoch [9/10], Batch [143/938], Loss: 0.5874\n",
      "Epoch [9/10], Batch [144/938], Loss: 0.5749\n",
      "Epoch [9/10], Batch [145/938], Loss: 0.5762\n",
      "Epoch [9/10], Batch [146/938], Loss: 0.5738\n",
      "Epoch [9/10], Batch [147/938], Loss: 0.5657\n",
      "Epoch [9/10], Batch [148/938], Loss: 0.5814\n",
      "Epoch [9/10], Batch [149/938], Loss: 0.5518\n",
      "Epoch [9/10], Batch [150/938], Loss: 0.6129\n",
      "Epoch [9/10], Batch [151/938], Loss: 0.6009\n",
      "Epoch [9/10], Batch [152/938], Loss: 0.5801\n",
      "Epoch [9/10], Batch [153/938], Loss: 0.5743\n",
      "Epoch [9/10], Batch [154/938], Loss: 0.6044\n",
      "Epoch [9/10], Batch [155/938], Loss: 0.5851\n",
      "Epoch [9/10], Batch [156/938], Loss: 0.5994\n",
      "Epoch [9/10], Batch [157/938], Loss: 0.5743\n",
      "Epoch [9/10], Batch [158/938], Loss: 0.5917\n",
      "Epoch [9/10], Batch [159/938], Loss: 0.6017\n",
      "Epoch [9/10], Batch [160/938], Loss: 0.5972\n",
      "Epoch [9/10], Batch [161/938], Loss: 0.6203\n",
      "Epoch [9/10], Batch [162/938], Loss: 0.5659\n",
      "Epoch [9/10], Batch [163/938], Loss: 0.6236\n",
      "Epoch [9/10], Batch [164/938], Loss: 0.5687\n",
      "Epoch [9/10], Batch [165/938], Loss: 0.6010\n",
      "Epoch [9/10], Batch [166/938], Loss: 0.6126\n",
      "Epoch [9/10], Batch [167/938], Loss: 0.6070\n",
      "Epoch [9/10], Batch [168/938], Loss: 0.5957\n",
      "Epoch [9/10], Batch [169/938], Loss: 0.6066\n",
      "Epoch [9/10], Batch [170/938], Loss: 0.6058\n",
      "Epoch [9/10], Batch [171/938], Loss: 0.5750\n",
      "Epoch [9/10], Batch [172/938], Loss: 0.5723\n",
      "Epoch [9/10], Batch [173/938], Loss: 0.5814\n",
      "Epoch [9/10], Batch [174/938], Loss: 0.5512\n",
      "Epoch [9/10], Batch [175/938], Loss: 0.5831\n",
      "Epoch [9/10], Batch [176/938], Loss: 0.5807\n",
      "Epoch [9/10], Batch [177/938], Loss: 0.6335\n",
      "Epoch [9/10], Batch [178/938], Loss: 0.5711\n",
      "Epoch [9/10], Batch [179/938], Loss: 0.5649\n",
      "Epoch [9/10], Batch [180/938], Loss: 0.5891\n",
      "Epoch [9/10], Batch [181/938], Loss: 0.5982\n",
      "Epoch [9/10], Batch [182/938], Loss: 0.5641\n",
      "Epoch [9/10], Batch [183/938], Loss: 0.5635\n",
      "Epoch [9/10], Batch [184/938], Loss: 0.5785\n",
      "Epoch [9/10], Batch [185/938], Loss: 0.5874\n",
      "Epoch [9/10], Batch [186/938], Loss: 0.5815\n",
      "Epoch [9/10], Batch [187/938], Loss: 0.5560\n",
      "Epoch [9/10], Batch [188/938], Loss: 0.5805\n",
      "Epoch [9/10], Batch [189/938], Loss: 0.5797\n",
      "Epoch [9/10], Batch [190/938], Loss: 0.5935\n",
      "Epoch [9/10], Batch [191/938], Loss: 0.5945\n",
      "Epoch [9/10], Batch [192/938], Loss: 0.5736\n",
      "Epoch [9/10], Batch [193/938], Loss: 0.5875\n",
      "Epoch [9/10], Batch [194/938], Loss: 0.5586\n",
      "Epoch [9/10], Batch [195/938], Loss: 0.5794\n",
      "Epoch [9/10], Batch [196/938], Loss: 0.5920\n",
      "Epoch [9/10], Batch [197/938], Loss: 0.5824\n",
      "Epoch [9/10], Batch [198/938], Loss: 0.5923\n",
      "Epoch [9/10], Batch [199/938], Loss: 0.6083\n",
      "Epoch [9/10], Batch [200/938], Loss: 0.5861\n",
      "Epoch [9/10], Batch [201/938], Loss: 0.5782\n",
      "Epoch [9/10], Batch [202/938], Loss: 0.5911\n",
      "Epoch [9/10], Batch [203/938], Loss: 0.5666\n",
      "Epoch [9/10], Batch [204/938], Loss: 0.5657\n",
      "Epoch [9/10], Batch [205/938], Loss: 0.5942\n",
      "Epoch [9/10], Batch [206/938], Loss: 0.5813\n",
      "Epoch [9/10], Batch [207/938], Loss: 0.5895\n",
      "Epoch [9/10], Batch [208/938], Loss: 0.5607\n",
      "Epoch [9/10], Batch [209/938], Loss: 0.5821\n",
      "Epoch [9/10], Batch [210/938], Loss: 0.5674\n",
      "Epoch [9/10], Batch [211/938], Loss: 0.6120\n",
      "Epoch [9/10], Batch [212/938], Loss: 0.5593\n",
      "Epoch [9/10], Batch [213/938], Loss: 0.5804\n",
      "Epoch [9/10], Batch [214/938], Loss: 0.5838\n",
      "Epoch [9/10], Batch [215/938], Loss: 0.5856\n",
      "Epoch [9/10], Batch [216/938], Loss: 0.5799\n",
      "Epoch [9/10], Batch [217/938], Loss: 0.5906\n",
      "Epoch [9/10], Batch [218/938], Loss: 0.6075\n",
      "Epoch [9/10], Batch [219/938], Loss: 0.5709\n",
      "Epoch [9/10], Batch [220/938], Loss: 0.5915\n",
      "Epoch [9/10], Batch [221/938], Loss: 0.5964\n",
      "Epoch [9/10], Batch [222/938], Loss: 0.6056\n",
      "Epoch [9/10], Batch [223/938], Loss: 0.5879\n",
      "Epoch [9/10], Batch [224/938], Loss: 0.5805\n",
      "Epoch [9/10], Batch [225/938], Loss: 0.5996\n",
      "Epoch [9/10], Batch [226/938], Loss: 0.5652\n",
      "Epoch [9/10], Batch [227/938], Loss: 0.5940\n",
      "Epoch [9/10], Batch [228/938], Loss: 0.5726\n",
      "Epoch [9/10], Batch [229/938], Loss: 0.5945\n",
      "Epoch [9/10], Batch [230/938], Loss: 0.5758\n",
      "Epoch [9/10], Batch [231/938], Loss: 0.6091\n",
      "Epoch [9/10], Batch [232/938], Loss: 0.5933\n",
      "Epoch [9/10], Batch [233/938], Loss: 0.5514\n",
      "Epoch [9/10], Batch [234/938], Loss: 0.5944\n",
      "Epoch [9/10], Batch [235/938], Loss: 0.5953\n",
      "Epoch [9/10], Batch [236/938], Loss: 0.5902\n",
      "Epoch [9/10], Batch [237/938], Loss: 0.5923\n",
      "Epoch [9/10], Batch [238/938], Loss: 0.5952\n",
      "Epoch [9/10], Batch [239/938], Loss: 0.5836\n",
      "Epoch [9/10], Batch [240/938], Loss: 0.5890\n",
      "Epoch [9/10], Batch [241/938], Loss: 0.5807\n",
      "Epoch [9/10], Batch [242/938], Loss: 0.5982\n",
      "Epoch [9/10], Batch [243/938], Loss: 0.5850\n",
      "Epoch [9/10], Batch [244/938], Loss: 0.5993\n",
      "Epoch [9/10], Batch [245/938], Loss: 0.5981\n",
      "Epoch [9/10], Batch [246/938], Loss: 0.5858\n",
      "Epoch [9/10], Batch [247/938], Loss: 0.5924\n",
      "Epoch [9/10], Batch [248/938], Loss: 0.5893\n",
      "Epoch [9/10], Batch [249/938], Loss: 0.5972\n",
      "Epoch [9/10], Batch [250/938], Loss: 0.5724\n",
      "Epoch [9/10], Batch [251/938], Loss: 0.6072\n",
      "Epoch [9/10], Batch [252/938], Loss: 0.5762\n",
      "Epoch [9/10], Batch [253/938], Loss: 0.5739\n",
      "Epoch [9/10], Batch [254/938], Loss: 0.5833\n",
      "Epoch [9/10], Batch [255/938], Loss: 0.5977\n",
      "Epoch [9/10], Batch [256/938], Loss: 0.5818\n",
      "Epoch [9/10], Batch [257/938], Loss: 0.5667\n",
      "Epoch [9/10], Batch [258/938], Loss: 0.5713\n",
      "Epoch [9/10], Batch [259/938], Loss: 0.5581\n",
      "Epoch [9/10], Batch [260/938], Loss: 0.5905\n",
      "Epoch [9/10], Batch [261/938], Loss: 0.5819\n",
      "Epoch [9/10], Batch [262/938], Loss: 0.5843\n",
      "Epoch [9/10], Batch [263/938], Loss: 0.6006\n",
      "Epoch [9/10], Batch [264/938], Loss: 0.5715\n",
      "Epoch [9/10], Batch [265/938], Loss: 0.5796\n",
      "Epoch [9/10], Batch [266/938], Loss: 0.5779\n",
      "Epoch [9/10], Batch [267/938], Loss: 0.6016\n",
      "Epoch [9/10], Batch [268/938], Loss: 0.6143\n",
      "Epoch [9/10], Batch [269/938], Loss: 0.5982\n",
      "Epoch [9/10], Batch [270/938], Loss: 0.5766\n",
      "Epoch [9/10], Batch [271/938], Loss: 0.5707\n",
      "Epoch [9/10], Batch [272/938], Loss: 0.5677\n",
      "Epoch [9/10], Batch [273/938], Loss: 0.5766\n",
      "Epoch [9/10], Batch [274/938], Loss: 0.5782\n",
      "Epoch [9/10], Batch [275/938], Loss: 0.5791\n",
      "Epoch [9/10], Batch [276/938], Loss: 0.5963\n",
      "Epoch [9/10], Batch [277/938], Loss: 0.5858\n",
      "Epoch [9/10], Batch [278/938], Loss: 0.5577\n",
      "Epoch [9/10], Batch [279/938], Loss: 0.5800\n",
      "Epoch [9/10], Batch [280/938], Loss: 0.5814\n",
      "Epoch [9/10], Batch [281/938], Loss: 0.5933\n",
      "Epoch [9/10], Batch [282/938], Loss: 0.5702\n",
      "Epoch [9/10], Batch [283/938], Loss: 0.6174\n",
      "Epoch [9/10], Batch [284/938], Loss: 0.5985\n",
      "Epoch [9/10], Batch [285/938], Loss: 0.5945\n",
      "Epoch [9/10], Batch [286/938], Loss: 0.5991\n",
      "Epoch [9/10], Batch [287/938], Loss: 0.5659\n",
      "Epoch [9/10], Batch [288/938], Loss: 0.5821\n",
      "Epoch [9/10], Batch [289/938], Loss: 0.5828\n",
      "Epoch [9/10], Batch [290/938], Loss: 0.5866\n",
      "Epoch [9/10], Batch [291/938], Loss: 0.6087\n",
      "Epoch [9/10], Batch [292/938], Loss: 0.5678\n",
      "Epoch [9/10], Batch [293/938], Loss: 0.5933\n",
      "Epoch [9/10], Batch [294/938], Loss: 0.5918\n",
      "Epoch [9/10], Batch [295/938], Loss: 0.5474\n",
      "Epoch [9/10], Batch [296/938], Loss: 0.6238\n",
      "Epoch [9/10], Batch [297/938], Loss: 0.5848\n",
      "Epoch [9/10], Batch [298/938], Loss: 0.5720\n",
      "Epoch [9/10], Batch [299/938], Loss: 0.5825\n",
      "Epoch [9/10], Batch [300/938], Loss: 0.5734\n",
      "Epoch [9/10], Batch [301/938], Loss: 0.5934\n",
      "Epoch [9/10], Batch [302/938], Loss: 0.6276\n",
      "Epoch [9/10], Batch [303/938], Loss: 0.5717\n",
      "Epoch [9/10], Batch [304/938], Loss: 0.5790\n",
      "Epoch [9/10], Batch [305/938], Loss: 0.5832\n",
      "Epoch [9/10], Batch [306/938], Loss: 0.5811\n",
      "Epoch [9/10], Batch [307/938], Loss: 0.5847\n",
      "Epoch [9/10], Batch [308/938], Loss: 0.5921\n",
      "Epoch [9/10], Batch [309/938], Loss: 0.5852\n",
      "Epoch [9/10], Batch [310/938], Loss: 0.5526\n",
      "Epoch [9/10], Batch [311/938], Loss: 0.5536\n",
      "Epoch [9/10], Batch [312/938], Loss: 0.5626\n",
      "Epoch [9/10], Batch [313/938], Loss: 0.5777\n",
      "Epoch [9/10], Batch [314/938], Loss: 0.6036\n",
      "Epoch [9/10], Batch [315/938], Loss: 0.5727\n",
      "Epoch [9/10], Batch [316/938], Loss: 0.6069\n",
      "Epoch [9/10], Batch [317/938], Loss: 0.5663\n",
      "Epoch [9/10], Batch [318/938], Loss: 0.5885\n",
      "Epoch [9/10], Batch [319/938], Loss: 0.5943\n",
      "Epoch [9/10], Batch [320/938], Loss: 0.5669\n",
      "Epoch [9/10], Batch [321/938], Loss: 0.6016\n",
      "Epoch [9/10], Batch [322/938], Loss: 0.5860\n",
      "Epoch [9/10], Batch [323/938], Loss: 0.5519\n",
      "Epoch [9/10], Batch [324/938], Loss: 0.5860\n",
      "Epoch [9/10], Batch [325/938], Loss: 0.5860\n",
      "Epoch [9/10], Batch [326/938], Loss: 0.5649\n",
      "Epoch [9/10], Batch [327/938], Loss: 0.5571\n",
      "Epoch [9/10], Batch [328/938], Loss: 0.5802\n",
      "Epoch [9/10], Batch [329/938], Loss: 0.5785\n",
      "Epoch [9/10], Batch [330/938], Loss: 0.5976\n",
      "Epoch [9/10], Batch [331/938], Loss: 0.5684\n",
      "Epoch [9/10], Batch [332/938], Loss: 0.6043\n",
      "Epoch [9/10], Batch [333/938], Loss: 0.5659\n",
      "Epoch [9/10], Batch [334/938], Loss: 0.5994\n",
      "Epoch [9/10], Batch [335/938], Loss: 0.5770\n",
      "Epoch [9/10], Batch [336/938], Loss: 0.5940\n",
      "Epoch [9/10], Batch [337/938], Loss: 0.5734\n",
      "Epoch [9/10], Batch [338/938], Loss: 0.5908\n",
      "Epoch [9/10], Batch [339/938], Loss: 0.5919\n",
      "Epoch [9/10], Batch [340/938], Loss: 0.5705\n",
      "Epoch [9/10], Batch [341/938], Loss: 0.5760\n",
      "Epoch [9/10], Batch [342/938], Loss: 0.5900\n",
      "Epoch [9/10], Batch [343/938], Loss: 0.6036\n",
      "Epoch [9/10], Batch [344/938], Loss: 0.6167\n",
      "Epoch [9/10], Batch [345/938], Loss: 0.5658\n",
      "Epoch [9/10], Batch [346/938], Loss: 0.5836\n",
      "Epoch [9/10], Batch [347/938], Loss: 0.5968\n",
      "Epoch [9/10], Batch [348/938], Loss: 0.5831\n",
      "Epoch [9/10], Batch [349/938], Loss: 0.5934\n",
      "Epoch [9/10], Batch [350/938], Loss: 0.5888\n",
      "Epoch [9/10], Batch [351/938], Loss: 0.5872\n",
      "Epoch [9/10], Batch [352/938], Loss: 0.5567\n",
      "Epoch [9/10], Batch [353/938], Loss: 0.6071\n",
      "Epoch [9/10], Batch [354/938], Loss: 0.5872\n",
      "Epoch [9/10], Batch [355/938], Loss: 0.6168\n",
      "Epoch [9/10], Batch [356/938], Loss: 0.5775\n",
      "Epoch [9/10], Batch [357/938], Loss: 0.5676\n",
      "Epoch [9/10], Batch [358/938], Loss: 0.5984\n",
      "Epoch [9/10], Batch [359/938], Loss: 0.5962\n",
      "Epoch [9/10], Batch [360/938], Loss: 0.5822\n",
      "Epoch [9/10], Batch [361/938], Loss: 0.5937\n",
      "Epoch [9/10], Batch [362/938], Loss: 0.5461\n",
      "Epoch [9/10], Batch [363/938], Loss: 0.6002\n",
      "Epoch [9/10], Batch [364/938], Loss: 0.5778\n",
      "Epoch [9/10], Batch [365/938], Loss: 0.6120\n",
      "Epoch [9/10], Batch [366/938], Loss: 0.6017\n",
      "Epoch [9/10], Batch [367/938], Loss: 0.5939\n",
      "Epoch [9/10], Batch [368/938], Loss: 0.5726\n",
      "Epoch [9/10], Batch [369/938], Loss: 0.5819\n",
      "Epoch [9/10], Batch [370/938], Loss: 0.5665\n",
      "Epoch [9/10], Batch [371/938], Loss: 0.5797\n",
      "Epoch [9/10], Batch [372/938], Loss: 0.6039\n",
      "Epoch [9/10], Batch [373/938], Loss: 0.6080\n",
      "Epoch [9/10], Batch [374/938], Loss: 0.5892\n",
      "Epoch [9/10], Batch [375/938], Loss: 0.6104\n",
      "Epoch [9/10], Batch [376/938], Loss: 0.5782\n",
      "Epoch [9/10], Batch [377/938], Loss: 0.5897\n",
      "Epoch [9/10], Batch [378/938], Loss: 0.6017\n",
      "Epoch [9/10], Batch [379/938], Loss: 0.6036\n",
      "Epoch [9/10], Batch [380/938], Loss: 0.5870\n",
      "Epoch [9/10], Batch [381/938], Loss: 0.5923\n",
      "Epoch [9/10], Batch [382/938], Loss: 0.5982\n",
      "Epoch [9/10], Batch [383/938], Loss: 0.6016\n",
      "Epoch [9/10], Batch [384/938], Loss: 0.5793\n",
      "Epoch [9/10], Batch [385/938], Loss: 0.5799\n",
      "Epoch [9/10], Batch [386/938], Loss: 0.5867\n",
      "Epoch [9/10], Batch [387/938], Loss: 0.5693\n",
      "Epoch [9/10], Batch [388/938], Loss: 0.5871\n",
      "Epoch [9/10], Batch [389/938], Loss: 0.5634\n",
      "Epoch [9/10], Batch [390/938], Loss: 0.5834\n",
      "Epoch [9/10], Batch [391/938], Loss: 0.5917\n",
      "Epoch [9/10], Batch [392/938], Loss: 0.5522\n",
      "Epoch [9/10], Batch [393/938], Loss: 0.6148\n",
      "Epoch [9/10], Batch [394/938], Loss: 0.5900\n",
      "Epoch [9/10], Batch [395/938], Loss: 0.6000\n",
      "Epoch [9/10], Batch [396/938], Loss: 0.5897\n",
      "Epoch [9/10], Batch [397/938], Loss: 0.5965\n",
      "Epoch [9/10], Batch [398/938], Loss: 0.6153\n",
      "Epoch [9/10], Batch [399/938], Loss: 0.5906\n",
      "Epoch [9/10], Batch [400/938], Loss: 0.5752\n",
      "Epoch [9/10], Batch [401/938], Loss: 0.5898\n",
      "Epoch [9/10], Batch [402/938], Loss: 0.5839\n",
      "Epoch [9/10], Batch [403/938], Loss: 0.5838\n",
      "Epoch [9/10], Batch [404/938], Loss: 0.5828\n",
      "Epoch [9/10], Batch [405/938], Loss: 0.5748\n",
      "Epoch [9/10], Batch [406/938], Loss: 0.5722\n",
      "Epoch [9/10], Batch [407/938], Loss: 0.5813\n",
      "Epoch [9/10], Batch [408/938], Loss: 0.6112\n",
      "Epoch [9/10], Batch [409/938], Loss: 0.6004\n",
      "Epoch [9/10], Batch [410/938], Loss: 0.6033\n",
      "Epoch [9/10], Batch [411/938], Loss: 0.5886\n",
      "Epoch [9/10], Batch [412/938], Loss: 0.5741\n",
      "Epoch [9/10], Batch [413/938], Loss: 0.5589\n",
      "Epoch [9/10], Batch [414/938], Loss: 0.6177\n",
      "Epoch [9/10], Batch [415/938], Loss: 0.5818\n",
      "Epoch [9/10], Batch [416/938], Loss: 0.6006\n",
      "Epoch [9/10], Batch [417/938], Loss: 0.5779\n",
      "Epoch [9/10], Batch [418/938], Loss: 0.5808\n",
      "Epoch [9/10], Batch [419/938], Loss: 0.5863\n",
      "Epoch [9/10], Batch [420/938], Loss: 0.5937\n",
      "Epoch [9/10], Batch [421/938], Loss: 0.5789\n",
      "Epoch [9/10], Batch [422/938], Loss: 0.5885\n",
      "Epoch [9/10], Batch [423/938], Loss: 0.5887\n",
      "Epoch [9/10], Batch [424/938], Loss: 0.6164\n",
      "Epoch [9/10], Batch [425/938], Loss: 0.5738\n",
      "Epoch [9/10], Batch [426/938], Loss: 0.5932\n",
      "Epoch [9/10], Batch [427/938], Loss: 0.5836\n",
      "Epoch [9/10], Batch [428/938], Loss: 0.5956\n",
      "Epoch [9/10], Batch [429/938], Loss: 0.5742\n",
      "Epoch [9/10], Batch [430/938], Loss: 0.5818\n",
      "Epoch [9/10], Batch [431/938], Loss: 0.5895\n",
      "Epoch [9/10], Batch [432/938], Loss: 0.5854\n",
      "Epoch [9/10], Batch [433/938], Loss: 0.6131\n",
      "Epoch [9/10], Batch [434/938], Loss: 0.5612\n",
      "Epoch [9/10], Batch [435/938], Loss: 0.5985\n",
      "Epoch [9/10], Batch [436/938], Loss: 0.6114\n",
      "Epoch [9/10], Batch [437/938], Loss: 0.5714\n",
      "Epoch [9/10], Batch [438/938], Loss: 0.5699\n",
      "Epoch [9/10], Batch [439/938], Loss: 0.5952\n",
      "Epoch [9/10], Batch [440/938], Loss: 0.5911\n",
      "Epoch [9/10], Batch [441/938], Loss: 0.5990\n",
      "Epoch [9/10], Batch [442/938], Loss: 0.5968\n",
      "Epoch [9/10], Batch [443/938], Loss: 0.5792\n",
      "Epoch [9/10], Batch [444/938], Loss: 0.5920\n",
      "Epoch [9/10], Batch [445/938], Loss: 0.6056\n",
      "Epoch [9/10], Batch [446/938], Loss: 0.6131\n",
      "Epoch [9/10], Batch [447/938], Loss: 0.5710\n",
      "Epoch [9/10], Batch [448/938], Loss: 0.5803\n",
      "Epoch [9/10], Batch [449/938], Loss: 0.6065\n",
      "Epoch [9/10], Batch [450/938], Loss: 0.5582\n",
      "Epoch [9/10], Batch [451/938], Loss: 0.5651\n",
      "Epoch [9/10], Batch [452/938], Loss: 0.5777\n",
      "Epoch [9/10], Batch [453/938], Loss: 0.6033\n",
      "Epoch [9/10], Batch [454/938], Loss: 0.5701\n",
      "Epoch [9/10], Batch [455/938], Loss: 0.6068\n",
      "Epoch [9/10], Batch [456/938], Loss: 0.5921\n",
      "Epoch [9/10], Batch [457/938], Loss: 0.5724\n",
      "Epoch [9/10], Batch [458/938], Loss: 0.5948\n",
      "Epoch [9/10], Batch [459/938], Loss: 0.5610\n",
      "Epoch [9/10], Batch [460/938], Loss: 0.5742\n",
      "Epoch [9/10], Batch [461/938], Loss: 0.5725\n",
      "Epoch [9/10], Batch [462/938], Loss: 0.5926\n",
      "Epoch [9/10], Batch [463/938], Loss: 0.5890\n",
      "Epoch [9/10], Batch [464/938], Loss: 0.5655\n",
      "Epoch [9/10], Batch [465/938], Loss: 0.5569\n",
      "Epoch [9/10], Batch [466/938], Loss: 0.6084\n",
      "Epoch [9/10], Batch [467/938], Loss: 0.5742\n",
      "Epoch [9/10], Batch [468/938], Loss: 0.6011\n",
      "Epoch [9/10], Batch [469/938], Loss: 0.5680\n",
      "Epoch [9/10], Batch [470/938], Loss: 0.5932\n",
      "Epoch [9/10], Batch [471/938], Loss: 0.5842\n",
      "Epoch [9/10], Batch [472/938], Loss: 0.5953\n",
      "Epoch [9/10], Batch [473/938], Loss: 0.5633\n",
      "Epoch [9/10], Batch [474/938], Loss: 0.5955\n",
      "Epoch [9/10], Batch [475/938], Loss: 0.5854\n",
      "Epoch [9/10], Batch [476/938], Loss: 0.5528\n",
      "Epoch [9/10], Batch [477/938], Loss: 0.5890\n",
      "Epoch [9/10], Batch [478/938], Loss: 0.6081\n",
      "Epoch [9/10], Batch [479/938], Loss: 0.5942\n",
      "Epoch [9/10], Batch [480/938], Loss: 0.5899\n",
      "Epoch [9/10], Batch [481/938], Loss: 0.5841\n",
      "Epoch [9/10], Batch [482/938], Loss: 0.6064\n",
      "Epoch [9/10], Batch [483/938], Loss: 0.5664\n",
      "Epoch [9/10], Batch [484/938], Loss: 0.5907\n",
      "Epoch [9/10], Batch [485/938], Loss: 0.5496\n",
      "Epoch [9/10], Batch [486/938], Loss: 0.5733\n",
      "Epoch [9/10], Batch [487/938], Loss: 0.5790\n",
      "Epoch [9/10], Batch [488/938], Loss: 0.6051\n",
      "Epoch [9/10], Batch [489/938], Loss: 0.5739\n",
      "Epoch [9/10], Batch [490/938], Loss: 0.5797\n",
      "Epoch [9/10], Batch [491/938], Loss: 0.5815\n",
      "Epoch [9/10], Batch [492/938], Loss: 0.5700\n",
      "Epoch [9/10], Batch [493/938], Loss: 0.6118\n",
      "Epoch [9/10], Batch [494/938], Loss: 0.5976\n",
      "Epoch [9/10], Batch [495/938], Loss: 0.5736\n",
      "Epoch [9/10], Batch [496/938], Loss: 0.5559\n",
      "Epoch [9/10], Batch [497/938], Loss: 0.5760\n",
      "Epoch [9/10], Batch [498/938], Loss: 0.5860\n",
      "Epoch [9/10], Batch [499/938], Loss: 0.5970\n",
      "Epoch [9/10], Batch [500/938], Loss: 0.5830\n",
      "Epoch [9/10], Batch [501/938], Loss: 0.5876\n",
      "Epoch [9/10], Batch [502/938], Loss: 0.5907\n",
      "Epoch [9/10], Batch [503/938], Loss: 0.5692\n",
      "Epoch [9/10], Batch [504/938], Loss: 0.5927\n",
      "Epoch [9/10], Batch [505/938], Loss: 0.5583\n",
      "Epoch [9/10], Batch [506/938], Loss: 0.5958\n",
      "Epoch [9/10], Batch [507/938], Loss: 0.5990\n",
      "Epoch [9/10], Batch [508/938], Loss: 0.5785\n",
      "Epoch [9/10], Batch [509/938], Loss: 0.6146\n",
      "Epoch [9/10], Batch [510/938], Loss: 0.5736\n",
      "Epoch [9/10], Batch [511/938], Loss: 0.5931\n",
      "Epoch [9/10], Batch [512/938], Loss: 0.5688\n",
      "Epoch [9/10], Batch [513/938], Loss: 0.6056\n",
      "Epoch [9/10], Batch [514/938], Loss: 0.5836\n",
      "Epoch [9/10], Batch [515/938], Loss: 0.6043\n",
      "Epoch [9/10], Batch [516/938], Loss: 0.5964\n",
      "Epoch [9/10], Batch [517/938], Loss: 0.6088\n",
      "Epoch [9/10], Batch [518/938], Loss: 0.5900\n",
      "Epoch [9/10], Batch [519/938], Loss: 0.5677\n",
      "Epoch [9/10], Batch [520/938], Loss: 0.6027\n",
      "Epoch [9/10], Batch [521/938], Loss: 0.5818\n",
      "Epoch [9/10], Batch [522/938], Loss: 0.6023\n",
      "Epoch [9/10], Batch [523/938], Loss: 0.5903\n",
      "Epoch [9/10], Batch [524/938], Loss: 0.5966\n",
      "Epoch [9/10], Batch [525/938], Loss: 0.5855\n",
      "Epoch [9/10], Batch [526/938], Loss: 0.5701\n",
      "Epoch [9/10], Batch [527/938], Loss: 0.5871\n",
      "Epoch [9/10], Batch [528/938], Loss: 0.5879\n",
      "Epoch [9/10], Batch [529/938], Loss: 0.6205\n",
      "Epoch [9/10], Batch [530/938], Loss: 0.5865\n",
      "Epoch [9/10], Batch [531/938], Loss: 0.5946\n",
      "Epoch [9/10], Batch [532/938], Loss: 0.5853\n",
      "Epoch [9/10], Batch [533/938], Loss: 0.5878\n",
      "Epoch [9/10], Batch [534/938], Loss: 0.5862\n",
      "Epoch [9/10], Batch [535/938], Loss: 0.5850\n",
      "Epoch [9/10], Batch [536/938], Loss: 0.5610\n",
      "Epoch [9/10], Batch [537/938], Loss: 0.6081\n",
      "Epoch [9/10], Batch [538/938], Loss: 0.5935\n",
      "Epoch [9/10], Batch [539/938], Loss: 0.5878\n",
      "Epoch [9/10], Batch [540/938], Loss: 0.6277\n",
      "Epoch [9/10], Batch [541/938], Loss: 0.5830\n",
      "Epoch [9/10], Batch [542/938], Loss: 0.6137\n",
      "Epoch [9/10], Batch [543/938], Loss: 0.5955\n",
      "Epoch [9/10], Batch [544/938], Loss: 0.5835\n",
      "Epoch [9/10], Batch [545/938], Loss: 0.5844\n",
      "Epoch [9/10], Batch [546/938], Loss: 0.5702\n",
      "Epoch [9/10], Batch [547/938], Loss: 0.5730\n",
      "Epoch [9/10], Batch [548/938], Loss: 0.5947\n",
      "Epoch [9/10], Batch [549/938], Loss: 0.6138\n",
      "Epoch [9/10], Batch [550/938], Loss: 0.5975\n",
      "Epoch [9/10], Batch [551/938], Loss: 0.5639\n",
      "Epoch [9/10], Batch [552/938], Loss: 0.5789\n",
      "Epoch [9/10], Batch [553/938], Loss: 0.5832\n",
      "Epoch [9/10], Batch [554/938], Loss: 0.5603\n",
      "Epoch [9/10], Batch [555/938], Loss: 0.5740\n",
      "Epoch [9/10], Batch [556/938], Loss: 0.5766\n",
      "Epoch [9/10], Batch [557/938], Loss: 0.6025\n",
      "Epoch [9/10], Batch [558/938], Loss: 0.5641\n",
      "Epoch [9/10], Batch [559/938], Loss: 0.5945\n",
      "Epoch [9/10], Batch [560/938], Loss: 0.5863\n",
      "Epoch [9/10], Batch [561/938], Loss: 0.5818\n",
      "Epoch [9/10], Batch [562/938], Loss: 0.5701\n",
      "Epoch [9/10], Batch [563/938], Loss: 0.5899\n",
      "Epoch [9/10], Batch [564/938], Loss: 0.5796\n",
      "Epoch [9/10], Batch [565/938], Loss: 0.5559\n",
      "Epoch [9/10], Batch [566/938], Loss: 0.5895\n",
      "Epoch [9/10], Batch [567/938], Loss: 0.5793\n",
      "Epoch [9/10], Batch [568/938], Loss: 0.5898\n",
      "Epoch [9/10], Batch [569/938], Loss: 0.6102\n",
      "Epoch [9/10], Batch [570/938], Loss: 0.5651\n",
      "Epoch [9/10], Batch [571/938], Loss: 0.5846\n",
      "Epoch [9/10], Batch [572/938], Loss: 0.5687\n",
      "Epoch [9/10], Batch [573/938], Loss: 0.6090\n",
      "Epoch [9/10], Batch [574/938], Loss: 0.5604\n",
      "Epoch [9/10], Batch [575/938], Loss: 0.5664\n",
      "Epoch [9/10], Batch [576/938], Loss: 0.5553\n",
      "Epoch [9/10], Batch [577/938], Loss: 0.5819\n",
      "Epoch [9/10], Batch [578/938], Loss: 0.5924\n",
      "Epoch [9/10], Batch [579/938], Loss: 0.6006\n",
      "Epoch [9/10], Batch [580/938], Loss: 0.6019\n",
      "Epoch [9/10], Batch [581/938], Loss: 0.5943\n",
      "Epoch [9/10], Batch [582/938], Loss: 0.5735\n",
      "Epoch [9/10], Batch [583/938], Loss: 0.5751\n",
      "Epoch [9/10], Batch [584/938], Loss: 0.5795\n",
      "Epoch [9/10], Batch [585/938], Loss: 0.5839\n",
      "Epoch [9/10], Batch [586/938], Loss: 0.5770\n",
      "Epoch [9/10], Batch [587/938], Loss: 0.6029\n",
      "Epoch [9/10], Batch [588/938], Loss: 0.5911\n",
      "Epoch [9/10], Batch [589/938], Loss: 0.6325\n",
      "Epoch [9/10], Batch [590/938], Loss: 0.5811\n",
      "Epoch [9/10], Batch [591/938], Loss: 0.6019\n",
      "Epoch [9/10], Batch [592/938], Loss: 0.5882\n",
      "Epoch [9/10], Batch [593/938], Loss: 0.5845\n",
      "Epoch [9/10], Batch [594/938], Loss: 0.6104\n",
      "Epoch [9/10], Batch [595/938], Loss: 0.5844\n",
      "Epoch [9/10], Batch [596/938], Loss: 0.5772\n",
      "Epoch [9/10], Batch [597/938], Loss: 0.5819\n",
      "Epoch [9/10], Batch [598/938], Loss: 0.5896\n",
      "Epoch [9/10], Batch [599/938], Loss: 0.5863\n",
      "Epoch [9/10], Batch [600/938], Loss: 0.5713\n",
      "Epoch [9/10], Batch [601/938], Loss: 0.6044\n",
      "Epoch [9/10], Batch [602/938], Loss: 0.6088\n",
      "Epoch [9/10], Batch [603/938], Loss: 0.5602\n",
      "Epoch [9/10], Batch [604/938], Loss: 0.5836\n",
      "Epoch [9/10], Batch [605/938], Loss: 0.5914\n",
      "Epoch [9/10], Batch [606/938], Loss: 0.5849\n",
      "Epoch [9/10], Batch [607/938], Loss: 0.5942\n",
      "Epoch [9/10], Batch [608/938], Loss: 0.5896\n",
      "Epoch [9/10], Batch [609/938], Loss: 0.5916\n",
      "Epoch [9/10], Batch [610/938], Loss: 0.5848\n",
      "Epoch [9/10], Batch [611/938], Loss: 0.5859\n",
      "Epoch [9/10], Batch [612/938], Loss: 0.5860\n",
      "Epoch [9/10], Batch [613/938], Loss: 0.6023\n",
      "Epoch [9/10], Batch [614/938], Loss: 0.5849\n",
      "Epoch [9/10], Batch [615/938], Loss: 0.5718\n",
      "Epoch [9/10], Batch [616/938], Loss: 0.5768\n",
      "Epoch [9/10], Batch [617/938], Loss: 0.5787\n",
      "Epoch [9/10], Batch [618/938], Loss: 0.5997\n",
      "Epoch [9/10], Batch [619/938], Loss: 0.5857\n",
      "Epoch [9/10], Batch [620/938], Loss: 0.5889\n",
      "Epoch [9/10], Batch [621/938], Loss: 0.5542\n",
      "Epoch [9/10], Batch [622/938], Loss: 0.5716\n",
      "Epoch [9/10], Batch [623/938], Loss: 0.5768\n",
      "Epoch [9/10], Batch [624/938], Loss: 0.6077\n",
      "Epoch [9/10], Batch [625/938], Loss: 0.5736\n",
      "Epoch [9/10], Batch [626/938], Loss: 0.5916\n",
      "Epoch [9/10], Batch [627/938], Loss: 0.5646\n",
      "Epoch [9/10], Batch [628/938], Loss: 0.6135\n",
      "Epoch [9/10], Batch [629/938], Loss: 0.5826\n",
      "Epoch [9/10], Batch [630/938], Loss: 0.5576\n",
      "Epoch [9/10], Batch [631/938], Loss: 0.5797\n",
      "Epoch [9/10], Batch [632/938], Loss: 0.5625\n",
      "Epoch [9/10], Batch [633/938], Loss: 0.5647\n",
      "Epoch [9/10], Batch [634/938], Loss: 0.5941\n",
      "Epoch [9/10], Batch [635/938], Loss: 0.5774\n",
      "Epoch [9/10], Batch [636/938], Loss: 0.5849\n",
      "Epoch [9/10], Batch [637/938], Loss: 0.5678\n",
      "Epoch [9/10], Batch [638/938], Loss: 0.5932\n",
      "Epoch [9/10], Batch [639/938], Loss: 0.5971\n",
      "Epoch [9/10], Batch [640/938], Loss: 0.5725\n",
      "Epoch [9/10], Batch [641/938], Loss: 0.5745\n",
      "Epoch [9/10], Batch [642/938], Loss: 0.5853\n",
      "Epoch [9/10], Batch [643/938], Loss: 0.5832\n",
      "Epoch [9/10], Batch [644/938], Loss: 0.5905\n",
      "Epoch [9/10], Batch [645/938], Loss: 0.5586\n",
      "Epoch [9/10], Batch [646/938], Loss: 0.6099\n",
      "Epoch [9/10], Batch [647/938], Loss: 0.5707\n",
      "Epoch [9/10], Batch [648/938], Loss: 0.6012\n",
      "Epoch [9/10], Batch [649/938], Loss: 0.5645\n",
      "Epoch [9/10], Batch [650/938], Loss: 0.5888\n",
      "Epoch [9/10], Batch [651/938], Loss: 0.5690\n",
      "Epoch [9/10], Batch [652/938], Loss: 0.5719\n",
      "Epoch [9/10], Batch [653/938], Loss: 0.5973\n",
      "Epoch [9/10], Batch [654/938], Loss: 0.5863\n",
      "Epoch [9/10], Batch [655/938], Loss: 0.5863\n",
      "Epoch [9/10], Batch [656/938], Loss: 0.5879\n",
      "Epoch [9/10], Batch [657/938], Loss: 0.6176\n",
      "Epoch [9/10], Batch [658/938], Loss: 0.5688\n",
      "Epoch [9/10], Batch [659/938], Loss: 0.5782\n",
      "Epoch [9/10], Batch [660/938], Loss: 0.5738\n",
      "Epoch [9/10], Batch [661/938], Loss: 0.6047\n",
      "Epoch [9/10], Batch [662/938], Loss: 0.6000\n",
      "Epoch [9/10], Batch [663/938], Loss: 0.5971\n",
      "Epoch [9/10], Batch [664/938], Loss: 0.5621\n",
      "Epoch [9/10], Batch [665/938], Loss: 0.5843\n",
      "Epoch [9/10], Batch [666/938], Loss: 0.6119\n",
      "Epoch [9/10], Batch [667/938], Loss: 0.5604\n",
      "Epoch [9/10], Batch [668/938], Loss: 0.5836\n",
      "Epoch [9/10], Batch [669/938], Loss: 0.5724\n",
      "Epoch [9/10], Batch [670/938], Loss: 0.5844\n",
      "Epoch [9/10], Batch [671/938], Loss: 0.5804\n",
      "Epoch [9/10], Batch [672/938], Loss: 0.5791\n",
      "Epoch [9/10], Batch [673/938], Loss: 0.5827\n",
      "Epoch [9/10], Batch [674/938], Loss: 0.5868\n",
      "Epoch [9/10], Batch [675/938], Loss: 0.5974\n",
      "Epoch [9/10], Batch [676/938], Loss: 0.5765\n",
      "Epoch [9/10], Batch [677/938], Loss: 0.5640\n",
      "Epoch [9/10], Batch [678/938], Loss: 0.6010\n",
      "Epoch [9/10], Batch [679/938], Loss: 0.5766\n",
      "Epoch [9/10], Batch [680/938], Loss: 0.5575\n",
      "Epoch [9/10], Batch [681/938], Loss: 0.5879\n",
      "Epoch [9/10], Batch [682/938], Loss: 0.5628\n",
      "Epoch [9/10], Batch [683/938], Loss: 0.5886\n",
      "Epoch [9/10], Batch [684/938], Loss: 0.5531\n",
      "Epoch [9/10], Batch [685/938], Loss: 0.6078\n",
      "Epoch [9/10], Batch [686/938], Loss: 0.5619\n",
      "Epoch [9/10], Batch [687/938], Loss: 0.6070\n",
      "Epoch [9/10], Batch [688/938], Loss: 0.5787\n",
      "Epoch [9/10], Batch [689/938], Loss: 0.6075\n",
      "Epoch [9/10], Batch [690/938], Loss: 0.6194\n",
      "Epoch [9/10], Batch [691/938], Loss: 0.5613\n",
      "Epoch [9/10], Batch [692/938], Loss: 0.5862\n",
      "Epoch [9/10], Batch [693/938], Loss: 0.6093\n",
      "Epoch [9/10], Batch [694/938], Loss: 0.5714\n",
      "Epoch [9/10], Batch [695/938], Loss: 0.5740\n",
      "Epoch [9/10], Batch [696/938], Loss: 0.5682\n",
      "Epoch [9/10], Batch [697/938], Loss: 0.5866\n",
      "Epoch [9/10], Batch [698/938], Loss: 0.5759\n",
      "Epoch [9/10], Batch [699/938], Loss: 0.5926\n",
      "Epoch [9/10], Batch [700/938], Loss: 0.5755\n",
      "Epoch [9/10], Batch [701/938], Loss: 0.5755\n",
      "Epoch [9/10], Batch [702/938], Loss: 0.5859\n",
      "Epoch [9/10], Batch [703/938], Loss: 0.5796\n",
      "Epoch [9/10], Batch [704/938], Loss: 0.5959\n",
      "Epoch [9/10], Batch [705/938], Loss: 0.5804\n",
      "Epoch [9/10], Batch [706/938], Loss: 0.5752\n",
      "Epoch [9/10], Batch [707/938], Loss: 0.5997\n",
      "Epoch [9/10], Batch [708/938], Loss: 0.5603\n",
      "Epoch [9/10], Batch [709/938], Loss: 0.5696\n",
      "Epoch [9/10], Batch [710/938], Loss: 0.5644\n",
      "Epoch [9/10], Batch [711/938], Loss: 0.6254\n",
      "Epoch [9/10], Batch [712/938], Loss: 0.6084\n",
      "Epoch [9/10], Batch [713/938], Loss: 0.5613\n",
      "Epoch [9/10], Batch [714/938], Loss: 0.5901\n",
      "Epoch [9/10], Batch [715/938], Loss: 0.5802\n",
      "Epoch [9/10], Batch [716/938], Loss: 0.5648\n",
      "Epoch [9/10], Batch [717/938], Loss: 0.5929\n",
      "Epoch [9/10], Batch [718/938], Loss: 0.6000\n",
      "Epoch [9/10], Batch [719/938], Loss: 0.6087\n",
      "Epoch [9/10], Batch [720/938], Loss: 0.5803\n",
      "Epoch [9/10], Batch [721/938], Loss: 0.5890\n",
      "Epoch [9/10], Batch [722/938], Loss: 0.5737\n",
      "Epoch [9/10], Batch [723/938], Loss: 0.6122\n",
      "Epoch [9/10], Batch [724/938], Loss: 0.5920\n",
      "Epoch [9/10], Batch [725/938], Loss: 0.5522\n",
      "Epoch [9/10], Batch [726/938], Loss: 0.6083\n",
      "Epoch [9/10], Batch [727/938], Loss: 0.5770\n",
      "Epoch [9/10], Batch [728/938], Loss: 0.5754\n",
      "Epoch [9/10], Batch [729/938], Loss: 0.5979\n",
      "Epoch [9/10], Batch [730/938], Loss: 0.5918\n",
      "Epoch [9/10], Batch [731/938], Loss: 0.5669\n",
      "Epoch [9/10], Batch [732/938], Loss: 0.5969\n",
      "Epoch [9/10], Batch [733/938], Loss: 0.5946\n",
      "Epoch [9/10], Batch [734/938], Loss: 0.5681\n",
      "Epoch [9/10], Batch [735/938], Loss: 0.5728\n",
      "Epoch [9/10], Batch [736/938], Loss: 0.5866\n",
      "Epoch [9/10], Batch [737/938], Loss: 0.5864\n",
      "Epoch [9/10], Batch [738/938], Loss: 0.5598\n",
      "Epoch [9/10], Batch [739/938], Loss: 0.5782\n",
      "Epoch [9/10], Batch [740/938], Loss: 0.5836\n",
      "Epoch [9/10], Batch [741/938], Loss: 0.5819\n",
      "Epoch [9/10], Batch [742/938], Loss: 0.5994\n",
      "Epoch [9/10], Batch [743/938], Loss: 0.5852\n",
      "Epoch [9/10], Batch [744/938], Loss: 0.6093\n",
      "Epoch [9/10], Batch [745/938], Loss: 0.5526\n",
      "Epoch [9/10], Batch [746/938], Loss: 0.6007\n",
      "Epoch [9/10], Batch [747/938], Loss: 0.5898\n",
      "Epoch [9/10], Batch [748/938], Loss: 0.5882\n",
      "Epoch [9/10], Batch [749/938], Loss: 0.5915\n",
      "Epoch [9/10], Batch [750/938], Loss: 0.5769\n",
      "Epoch [9/10], Batch [751/938], Loss: 0.5991\n",
      "Epoch [9/10], Batch [752/938], Loss: 0.6018\n",
      "Epoch [9/10], Batch [753/938], Loss: 0.5563\n",
      "Epoch [9/10], Batch [754/938], Loss: 0.5907\n",
      "Epoch [9/10], Batch [755/938], Loss: 0.5711\n",
      "Epoch [9/10], Batch [756/938], Loss: 0.5779\n",
      "Epoch [9/10], Batch [757/938], Loss: 0.6127\n",
      "Epoch [9/10], Batch [758/938], Loss: 0.5833\n",
      "Epoch [9/10], Batch [759/938], Loss: 0.5865\n",
      "Epoch [9/10], Batch [760/938], Loss: 0.6047\n",
      "Epoch [9/10], Batch [761/938], Loss: 0.5735\n",
      "Epoch [9/10], Batch [762/938], Loss: 0.5866\n",
      "Epoch [9/10], Batch [763/938], Loss: 0.5851\n",
      "Epoch [9/10], Batch [764/938], Loss: 0.5655\n",
      "Epoch [9/10], Batch [765/938], Loss: 0.5813\n",
      "Epoch [9/10], Batch [766/938], Loss: 0.5884\n",
      "Epoch [9/10], Batch [767/938], Loss: 0.5947\n",
      "Epoch [9/10], Batch [768/938], Loss: 0.5841\n",
      "Epoch [9/10], Batch [769/938], Loss: 0.5973\n",
      "Epoch [9/10], Batch [770/938], Loss: 0.5650\n",
      "Epoch [9/10], Batch [771/938], Loss: 0.5793\n",
      "Epoch [9/10], Batch [772/938], Loss: 0.6198\n",
      "Epoch [9/10], Batch [773/938], Loss: 0.5821\n",
      "Epoch [9/10], Batch [774/938], Loss: 0.5743\n",
      "Epoch [9/10], Batch [775/938], Loss: 0.5944\n",
      "Epoch [9/10], Batch [776/938], Loss: 0.5690\n",
      "Epoch [9/10], Batch [777/938], Loss: 0.6057\n",
      "Epoch [9/10], Batch [778/938], Loss: 0.5974\n",
      "Epoch [9/10], Batch [779/938], Loss: 0.6021\n",
      "Epoch [9/10], Batch [780/938], Loss: 0.5771\n",
      "Epoch [9/10], Batch [781/938], Loss: 0.5663\n",
      "Epoch [9/10], Batch [782/938], Loss: 0.5969\n",
      "Epoch [9/10], Batch [783/938], Loss: 0.5891\n",
      "Epoch [9/10], Batch [784/938], Loss: 0.5786\n",
      "Epoch [9/10], Batch [785/938], Loss: 0.6021\n",
      "Epoch [9/10], Batch [786/938], Loss: 0.5555\n",
      "Epoch [9/10], Batch [787/938], Loss: 0.6053\n",
      "Epoch [9/10], Batch [788/938], Loss: 0.5833\n",
      "Epoch [9/10], Batch [789/938], Loss: 0.5604\n",
      "Epoch [9/10], Batch [790/938], Loss: 0.5732\n",
      "Epoch [9/10], Batch [791/938], Loss: 0.5737\n",
      "Epoch [9/10], Batch [792/938], Loss: 0.5973\n",
      "Epoch [9/10], Batch [793/938], Loss: 0.5679\n",
      "Epoch [9/10], Batch [794/938], Loss: 0.6246\n",
      "Epoch [9/10], Batch [795/938], Loss: 0.5993\n",
      "Epoch [9/10], Batch [796/938], Loss: 0.5894\n",
      "Epoch [9/10], Batch [797/938], Loss: 0.5374\n",
      "Epoch [9/10], Batch [798/938], Loss: 0.5872\n",
      "Epoch [9/10], Batch [799/938], Loss: 0.6099\n",
      "Epoch [9/10], Batch [800/938], Loss: 0.5939\n",
      "Epoch [9/10], Batch [801/938], Loss: 0.5974\n",
      "Epoch [9/10], Batch [802/938], Loss: 0.5915\n",
      "Epoch [9/10], Batch [803/938], Loss: 0.5769\n",
      "Epoch [9/10], Batch [804/938], Loss: 0.5973\n",
      "Epoch [9/10], Batch [805/938], Loss: 0.5558\n",
      "Epoch [9/10], Batch [806/938], Loss: 0.5794\n",
      "Epoch [9/10], Batch [807/938], Loss: 0.6246\n",
      "Epoch [9/10], Batch [808/938], Loss: 0.5696\n",
      "Epoch [9/10], Batch [809/938], Loss: 0.5926\n",
      "Epoch [9/10], Batch [810/938], Loss: 0.5864\n",
      "Epoch [9/10], Batch [811/938], Loss: 0.5855\n",
      "Epoch [9/10], Batch [812/938], Loss: 0.5739\n",
      "Epoch [9/10], Batch [813/938], Loss: 0.5802\n",
      "Epoch [9/10], Batch [814/938], Loss: 0.5837\n",
      "Epoch [9/10], Batch [815/938], Loss: 0.5703\n",
      "Epoch [9/10], Batch [816/938], Loss: 0.5986\n",
      "Epoch [9/10], Batch [817/938], Loss: 0.5626\n",
      "Epoch [9/10], Batch [818/938], Loss: 0.6108\n",
      "Epoch [9/10], Batch [819/938], Loss: 0.5753\n",
      "Epoch [9/10], Batch [820/938], Loss: 0.5605\n",
      "Epoch [9/10], Batch [821/938], Loss: 0.5968\n",
      "Epoch [9/10], Batch [822/938], Loss: 0.5910\n",
      "Epoch [9/10], Batch [823/938], Loss: 0.5788\n",
      "Epoch [9/10], Batch [824/938], Loss: 0.6110\n",
      "Epoch [9/10], Batch [825/938], Loss: 0.5764\n",
      "Epoch [9/10], Batch [826/938], Loss: 0.5826\n",
      "Epoch [9/10], Batch [827/938], Loss: 0.5953\n",
      "Epoch [9/10], Batch [828/938], Loss: 0.5513\n",
      "Epoch [9/10], Batch [829/938], Loss: 0.5867\n",
      "Epoch [9/10], Batch [830/938], Loss: 0.5965\n",
      "Epoch [9/10], Batch [831/938], Loss: 0.5962\n",
      "Epoch [9/10], Batch [832/938], Loss: 0.5818\n",
      "Epoch [9/10], Batch [833/938], Loss: 0.5805\n",
      "Epoch [9/10], Batch [834/938], Loss: 0.6008\n",
      "Epoch [9/10], Batch [835/938], Loss: 0.5698\n",
      "Epoch [9/10], Batch [836/938], Loss: 0.5719\n",
      "Epoch [9/10], Batch [837/938], Loss: 0.5473\n",
      "Epoch [9/10], Batch [838/938], Loss: 0.5611\n",
      "Epoch [9/10], Batch [839/938], Loss: 0.5624\n",
      "Epoch [9/10], Batch [840/938], Loss: 0.6202\n",
      "Epoch [9/10], Batch [841/938], Loss: 0.5727\n",
      "Epoch [9/10], Batch [842/938], Loss: 0.5847\n",
      "Epoch [9/10], Batch [843/938], Loss: 0.5742\n",
      "Epoch [9/10], Batch [844/938], Loss: 0.5972\n",
      "Epoch [9/10], Batch [845/938], Loss: 0.5974\n",
      "Epoch [9/10], Batch [846/938], Loss: 0.5850\n",
      "Epoch [9/10], Batch [847/938], Loss: 0.6056\n",
      "Epoch [9/10], Batch [848/938], Loss: 0.5882\n",
      "Epoch [9/10], Batch [849/938], Loss: 0.6112\n",
      "Epoch [9/10], Batch [850/938], Loss: 0.5721\n",
      "Epoch [9/10], Batch [851/938], Loss: 0.5825\n",
      "Epoch [9/10], Batch [852/938], Loss: 0.5831\n",
      "Epoch [9/10], Batch [853/938], Loss: 0.5845\n",
      "Epoch [9/10], Batch [854/938], Loss: 0.5885\n",
      "Epoch [9/10], Batch [855/938], Loss: 0.5755\n",
      "Epoch [9/10], Batch [856/938], Loss: 0.6066\n",
      "Epoch [9/10], Batch [857/938], Loss: 0.5948\n",
      "Epoch [9/10], Batch [858/938], Loss: 0.5934\n",
      "Epoch [9/10], Batch [859/938], Loss: 0.5851\n",
      "Epoch [9/10], Batch [860/938], Loss: 0.5797\n",
      "Epoch [9/10], Batch [861/938], Loss: 0.5811\n",
      "Epoch [9/10], Batch [862/938], Loss: 0.5782\n",
      "Epoch [9/10], Batch [863/938], Loss: 0.5816\n",
      "Epoch [9/10], Batch [864/938], Loss: 0.5963\n",
      "Epoch [9/10], Batch [865/938], Loss: 0.5760\n",
      "Epoch [9/10], Batch [866/938], Loss: 0.5682\n",
      "Epoch [9/10], Batch [867/938], Loss: 0.5774\n",
      "Epoch [9/10], Batch [868/938], Loss: 0.5599\n",
      "Epoch [9/10], Batch [869/938], Loss: 0.5753\n",
      "Epoch [9/10], Batch [870/938], Loss: 0.5847\n",
      "Epoch [9/10], Batch [871/938], Loss: 0.5992\n",
      "Epoch [9/10], Batch [872/938], Loss: 0.5648\n",
      "Epoch [9/10], Batch [873/938], Loss: 0.5923\n",
      "Epoch [9/10], Batch [874/938], Loss: 0.5893\n",
      "Epoch [9/10], Batch [875/938], Loss: 0.5734\n",
      "Epoch [9/10], Batch [876/938], Loss: 0.5740\n",
      "Epoch [9/10], Batch [877/938], Loss: 0.5624\n",
      "Epoch [9/10], Batch [878/938], Loss: 0.5972\n",
      "Epoch [9/10], Batch [879/938], Loss: 0.5826\n",
      "Epoch [9/10], Batch [880/938], Loss: 0.5991\n",
      "Epoch [9/10], Batch [881/938], Loss: 0.5711\n",
      "Epoch [9/10], Batch [882/938], Loss: 0.5962\n",
      "Epoch [9/10], Batch [883/938], Loss: 0.5904\n",
      "Epoch [9/10], Batch [884/938], Loss: 0.5667\n",
      "Epoch [9/10], Batch [885/938], Loss: 0.5770\n",
      "Epoch [9/10], Batch [886/938], Loss: 0.5588\n",
      "Epoch [9/10], Batch [887/938], Loss: 0.5625\n",
      "Epoch [9/10], Batch [888/938], Loss: 0.5898\n",
      "Epoch [9/10], Batch [889/938], Loss: 0.6087\n",
      "Epoch [9/10], Batch [890/938], Loss: 0.5759\n",
      "Epoch [9/10], Batch [891/938], Loss: 0.5517\n",
      "Epoch [9/10], Batch [892/938], Loss: 0.5744\n",
      "Epoch [9/10], Batch [893/938], Loss: 0.5637\n",
      "Epoch [9/10], Batch [894/938], Loss: 0.5786\n",
      "Epoch [9/10], Batch [895/938], Loss: 0.5733\n",
      "Epoch [9/10], Batch [896/938], Loss: 0.5814\n",
      "Epoch [9/10], Batch [897/938], Loss: 0.5742\n",
      "Epoch [9/10], Batch [898/938], Loss: 0.6137\n",
      "Epoch [9/10], Batch [899/938], Loss: 0.5501\n",
      "Epoch [9/10], Batch [900/938], Loss: 0.5881\n",
      "Epoch [9/10], Batch [901/938], Loss: 0.5945\n",
      "Epoch [9/10], Batch [902/938], Loss: 0.5997\n",
      "Epoch [9/10], Batch [903/938], Loss: 0.5996\n",
      "Epoch [9/10], Batch [904/938], Loss: 0.5901\n",
      "Epoch [9/10], Batch [905/938], Loss: 0.5573\n",
      "Epoch [9/10], Batch [906/938], Loss: 0.5676\n",
      "Epoch [9/10], Batch [907/938], Loss: 0.5704\n",
      "Epoch [9/10], Batch [908/938], Loss: 0.5710\n",
      "Epoch [9/10], Batch [909/938], Loss: 0.5780\n",
      "Epoch [9/10], Batch [910/938], Loss: 0.5691\n",
      "Epoch [9/10], Batch [911/938], Loss: 0.5863\n",
      "Epoch [9/10], Batch [912/938], Loss: 0.6247\n",
      "Epoch [9/10], Batch [913/938], Loss: 0.5914\n",
      "Epoch [9/10], Batch [914/938], Loss: 0.5669\n",
      "Epoch [9/10], Batch [915/938], Loss: 0.5982\n",
      "Epoch [9/10], Batch [916/938], Loss: 0.6099\n",
      "Epoch [9/10], Batch [917/938], Loss: 0.5869\n",
      "Epoch [9/10], Batch [918/938], Loss: 0.5748\n",
      "Epoch [9/10], Batch [919/938], Loss: 0.5477\n",
      "Epoch [9/10], Batch [920/938], Loss: 0.5686\n",
      "Epoch [9/10], Batch [921/938], Loss: 0.5759\n",
      "Epoch [9/10], Batch [922/938], Loss: 0.5692\n",
      "Epoch [9/10], Batch [923/938], Loss: 0.5900\n",
      "Epoch [9/10], Batch [924/938], Loss: 0.5702\n",
      "Epoch [9/10], Batch [925/938], Loss: 0.5999\n",
      "Epoch [9/10], Batch [926/938], Loss: 0.5802\n",
      "Epoch [9/10], Batch [927/938], Loss: 0.5700\n",
      "Epoch [9/10], Batch [928/938], Loss: 0.5857\n",
      "Epoch [9/10], Batch [929/938], Loss: 0.5995\n",
      "Epoch [9/10], Batch [930/938], Loss: 0.5462\n",
      "Epoch [9/10], Batch [931/938], Loss: 0.5890\n",
      "Epoch [9/10], Batch [932/938], Loss: 0.5558\n",
      "Epoch [9/10], Batch [933/938], Loss: 0.5724\n",
      "Epoch [9/10], Batch [934/938], Loss: 0.5818\n",
      "Epoch [9/10], Batch [935/938], Loss: 0.5667\n",
      "Epoch [9/10], Batch [936/938], Loss: 0.5793\n",
      "Epoch [9/10], Batch [937/938], Loss: 0.5611\n",
      "Epoch [9/10], Batch [938/938], Loss: 0.6010\n",
      "Epoch [9/10], Loss: 0.6010\n",
      "Epoch [10/10], Batch [1/938], Loss: 0.5839\n",
      "Epoch [10/10], Batch [2/938], Loss: 0.5769\n",
      "Epoch [10/10], Batch [3/938], Loss: 0.5858\n",
      "Epoch [10/10], Batch [4/938], Loss: 0.5815\n",
      "Epoch [10/10], Batch [5/938], Loss: 0.5834\n",
      "Epoch [10/10], Batch [6/938], Loss: 0.6013\n",
      "Epoch [10/10], Batch [7/938], Loss: 0.5840\n",
      "Epoch [10/10], Batch [8/938], Loss: 0.5963\n",
      "Epoch [10/10], Batch [9/938], Loss: 0.5990\n",
      "Epoch [10/10], Batch [10/938], Loss: 0.5758\n",
      "Epoch [10/10], Batch [11/938], Loss: 0.5638\n",
      "Epoch [10/10], Batch [12/938], Loss: 0.5854\n",
      "Epoch [10/10], Batch [13/938], Loss: 0.5658\n",
      "Epoch [10/10], Batch [14/938], Loss: 0.5886\n",
      "Epoch [10/10], Batch [15/938], Loss: 0.5579\n",
      "Epoch [10/10], Batch [16/938], Loss: 0.5996\n",
      "Epoch [10/10], Batch [17/938], Loss: 0.5990\n",
      "Epoch [10/10], Batch [18/938], Loss: 0.6011\n",
      "Epoch [10/10], Batch [19/938], Loss: 0.6381\n",
      "Epoch [10/10], Batch [20/938], Loss: 0.5857\n",
      "Epoch [10/10], Batch [21/938], Loss: 0.5916\n",
      "Epoch [10/10], Batch [22/938], Loss: 0.5853\n",
      "Epoch [10/10], Batch [23/938], Loss: 0.5886\n",
      "Epoch [10/10], Batch [24/938], Loss: 0.5831\n",
      "Epoch [10/10], Batch [25/938], Loss: 0.5653\n",
      "Epoch [10/10], Batch [26/938], Loss: 0.5981\n",
      "Epoch [10/10], Batch [27/938], Loss: 0.5982\n",
      "Epoch [10/10], Batch [28/938], Loss: 0.5903\n",
      "Epoch [10/10], Batch [29/938], Loss: 0.6037\n",
      "Epoch [10/10], Batch [30/938], Loss: 0.5958\n",
      "Epoch [10/10], Batch [31/938], Loss: 0.5896\n",
      "Epoch [10/10], Batch [32/938], Loss: 0.5861\n",
      "Epoch [10/10], Batch [33/938], Loss: 0.5756\n",
      "Epoch [10/10], Batch [34/938], Loss: 0.6030\n",
      "Epoch [10/10], Batch [35/938], Loss: 0.5842\n",
      "Epoch [10/10], Batch [36/938], Loss: 0.5655\n",
      "Epoch [10/10], Batch [37/938], Loss: 0.5834\n",
      "Epoch [10/10], Batch [38/938], Loss: 0.5585\n",
      "Epoch [10/10], Batch [39/938], Loss: 0.5925\n",
      "Epoch [10/10], Batch [40/938], Loss: 0.5878\n",
      "Epoch [10/10], Batch [41/938], Loss: 0.6167\n",
      "Epoch [10/10], Batch [42/938], Loss: 0.5639\n",
      "Epoch [10/10], Batch [43/938], Loss: 0.6016\n",
      "Epoch [10/10], Batch [44/938], Loss: 0.5698\n",
      "Epoch [10/10], Batch [45/938], Loss: 0.5999\n",
      "Epoch [10/10], Batch [46/938], Loss: 0.5930\n",
      "Epoch [10/10], Batch [47/938], Loss: 0.5856\n",
      "Epoch [10/10], Batch [48/938], Loss: 0.5525\n",
      "Epoch [10/10], Batch [49/938], Loss: 0.5539\n",
      "Epoch [10/10], Batch [50/938], Loss: 0.6134\n",
      "Epoch [10/10], Batch [51/938], Loss: 0.6162\n",
      "Epoch [10/10], Batch [52/938], Loss: 0.5817\n",
      "Epoch [10/10], Batch [53/938], Loss: 0.5723\n",
      "Epoch [10/10], Batch [54/938], Loss: 0.5840\n",
      "Epoch [10/10], Batch [55/938], Loss: 0.5551\n",
      "Epoch [10/10], Batch [56/938], Loss: 0.5571\n",
      "Epoch [10/10], Batch [57/938], Loss: 0.5920\n",
      "Epoch [10/10], Batch [58/938], Loss: 0.5982\n",
      "Epoch [10/10], Batch [59/938], Loss: 0.6037\n",
      "Epoch [10/10], Batch [60/938], Loss: 0.5619\n",
      "Epoch [10/10], Batch [61/938], Loss: 0.5817\n",
      "Epoch [10/10], Batch [62/938], Loss: 0.5933\n",
      "Epoch [10/10], Batch [63/938], Loss: 0.5927\n",
      "Epoch [10/10], Batch [64/938], Loss: 0.5980\n",
      "Epoch [10/10], Batch [65/938], Loss: 0.5857\n",
      "Epoch [10/10], Batch [66/938], Loss: 0.5973\n",
      "Epoch [10/10], Batch [67/938], Loss: 0.5873\n",
      "Epoch [10/10], Batch [68/938], Loss: 0.6052\n",
      "Epoch [10/10], Batch [69/938], Loss: 0.5999\n",
      "Epoch [10/10], Batch [70/938], Loss: 0.5936\n",
      "Epoch [10/10], Batch [71/938], Loss: 0.5841\n",
      "Epoch [10/10], Batch [72/938], Loss: 0.5670\n",
      "Epoch [10/10], Batch [73/938], Loss: 0.5811\n",
      "Epoch [10/10], Batch [74/938], Loss: 0.5721\n",
      "Epoch [10/10], Batch [75/938], Loss: 0.5928\n",
      "Epoch [10/10], Batch [76/938], Loss: 0.5765\n",
      "Epoch [10/10], Batch [77/938], Loss: 0.5765\n",
      "Epoch [10/10], Batch [78/938], Loss: 0.5930\n",
      "Epoch [10/10], Batch [79/938], Loss: 0.5976\n",
      "Epoch [10/10], Batch [80/938], Loss: 0.6271\n",
      "Epoch [10/10], Batch [81/938], Loss: 0.6033\n",
      "Epoch [10/10], Batch [82/938], Loss: 0.5782\n",
      "Epoch [10/10], Batch [83/938], Loss: 0.6039\n",
      "Epoch [10/10], Batch [84/938], Loss: 0.5864\n",
      "Epoch [10/10], Batch [85/938], Loss: 0.6101\n",
      "Epoch [10/10], Batch [86/938], Loss: 0.5831\n",
      "Epoch [10/10], Batch [87/938], Loss: 0.5864\n",
      "Epoch [10/10], Batch [88/938], Loss: 0.5969\n",
      "Epoch [10/10], Batch [89/938], Loss: 0.5908\n",
      "Epoch [10/10], Batch [90/938], Loss: 0.5711\n",
      "Epoch [10/10], Batch [91/938], Loss: 0.5726\n",
      "Epoch [10/10], Batch [92/938], Loss: 0.5552\n",
      "Epoch [10/10], Batch [93/938], Loss: 0.5761\n",
      "Epoch [10/10], Batch [94/938], Loss: 0.5902\n",
      "Epoch [10/10], Batch [95/938], Loss: 0.6111\n",
      "Epoch [10/10], Batch [96/938], Loss: 0.5953\n",
      "Epoch [10/10], Batch [97/938], Loss: 0.5644\n",
      "Epoch [10/10], Batch [98/938], Loss: 0.5953\n",
      "Epoch [10/10], Batch [99/938], Loss: 0.5704\n",
      "Epoch [10/10], Batch [100/938], Loss: 0.5773\n",
      "Epoch [10/10], Batch [101/938], Loss: 0.5672\n",
      "Epoch [10/10], Batch [102/938], Loss: 0.5726\n",
      "Epoch [10/10], Batch [103/938], Loss: 0.5824\n",
      "Epoch [10/10], Batch [104/938], Loss: 0.6210\n",
      "Epoch [10/10], Batch [105/938], Loss: 0.5549\n",
      "Epoch [10/10], Batch [106/938], Loss: 0.5990\n",
      "Epoch [10/10], Batch [107/938], Loss: 0.6044\n",
      "Epoch [10/10], Batch [108/938], Loss: 0.5719\n",
      "Epoch [10/10], Batch [109/938], Loss: 0.5756\n",
      "Epoch [10/10], Batch [110/938], Loss: 0.5910\n",
      "Epoch [10/10], Batch [111/938], Loss: 0.5912\n",
      "Epoch [10/10], Batch [112/938], Loss: 0.5600\n",
      "Epoch [10/10], Batch [113/938], Loss: 0.5686\n",
      "Epoch [10/10], Batch [114/938], Loss: 0.5665\n",
      "Epoch [10/10], Batch [115/938], Loss: 0.5713\n",
      "Epoch [10/10], Batch [116/938], Loss: 0.5859\n",
      "Epoch [10/10], Batch [117/938], Loss: 0.5706\n",
      "Epoch [10/10], Batch [118/938], Loss: 0.5840\n",
      "Epoch [10/10], Batch [119/938], Loss: 0.5722\n",
      "Epoch [10/10], Batch [120/938], Loss: 0.5612\n",
      "Epoch [10/10], Batch [121/938], Loss: 0.5748\n",
      "Epoch [10/10], Batch [122/938], Loss: 0.6151\n",
      "Epoch [10/10], Batch [123/938], Loss: 0.5918\n",
      "Epoch [10/10], Batch [124/938], Loss: 0.5651\n",
      "Epoch [10/10], Batch [125/938], Loss: 0.5598\n",
      "Epoch [10/10], Batch [126/938], Loss: 0.6000\n",
      "Epoch [10/10], Batch [127/938], Loss: 0.5888\n",
      "Epoch [10/10], Batch [128/938], Loss: 0.5747\n",
      "Epoch [10/10], Batch [129/938], Loss: 0.5773\n",
      "Epoch [10/10], Batch [130/938], Loss: 0.5722\n",
      "Epoch [10/10], Batch [131/938], Loss: 0.5848\n",
      "Epoch [10/10], Batch [132/938], Loss: 0.5791\n",
      "Epoch [10/10], Batch [133/938], Loss: 0.6078\n",
      "Epoch [10/10], Batch [134/938], Loss: 0.6058\n",
      "Epoch [10/10], Batch [135/938], Loss: 0.5851\n",
      "Epoch [10/10], Batch [136/938], Loss: 0.5484\n",
      "Epoch [10/10], Batch [137/938], Loss: 0.5897\n",
      "Epoch [10/10], Batch [138/938], Loss: 0.5812\n",
      "Epoch [10/10], Batch [139/938], Loss: 0.5782\n",
      "Epoch [10/10], Batch [140/938], Loss: 0.5858\n",
      "Epoch [10/10], Batch [141/938], Loss: 0.5950\n",
      "Epoch [10/10], Batch [142/938], Loss: 0.5677\n",
      "Epoch [10/10], Batch [143/938], Loss: 0.5666\n",
      "Epoch [10/10], Batch [144/938], Loss: 0.5734\n",
      "Epoch [10/10], Batch [145/938], Loss: 0.5702\n",
      "Epoch [10/10], Batch [146/938], Loss: 0.5770\n",
      "Epoch [10/10], Batch [147/938], Loss: 0.5743\n",
      "Epoch [10/10], Batch [148/938], Loss: 0.5673\n",
      "Epoch [10/10], Batch [149/938], Loss: 0.5691\n",
      "Epoch [10/10], Batch [150/938], Loss: 0.5799\n",
      "Epoch [10/10], Batch [151/938], Loss: 0.5572\n",
      "Epoch [10/10], Batch [152/938], Loss: 0.5700\n",
      "Epoch [10/10], Batch [153/938], Loss: 0.5834\n",
      "Epoch [10/10], Batch [154/938], Loss: 0.5785\n",
      "Epoch [10/10], Batch [155/938], Loss: 0.5885\n",
      "Epoch [10/10], Batch [156/938], Loss: 0.5970\n",
      "Epoch [10/10], Batch [157/938], Loss: 0.5860\n",
      "Epoch [10/10], Batch [158/938], Loss: 0.5517\n",
      "Epoch [10/10], Batch [159/938], Loss: 0.5852\n",
      "Epoch [10/10], Batch [160/938], Loss: 0.5484\n",
      "Epoch [10/10], Batch [161/938], Loss: 0.5789\n",
      "Epoch [10/10], Batch [162/938], Loss: 0.5693\n",
      "Epoch [10/10], Batch [163/938], Loss: 0.5770\n",
      "Epoch [10/10], Batch [164/938], Loss: 0.5686\n",
      "Epoch [10/10], Batch [165/938], Loss: 0.5855\n",
      "Epoch [10/10], Batch [166/938], Loss: 0.5648\n",
      "Epoch [10/10], Batch [167/938], Loss: 0.5815\n",
      "Epoch [10/10], Batch [168/938], Loss: 0.5978\n",
      "Epoch [10/10], Batch [169/938], Loss: 0.5742\n",
      "Epoch [10/10], Batch [170/938], Loss: 0.5764\n",
      "Epoch [10/10], Batch [171/938], Loss: 0.5840\n",
      "Epoch [10/10], Batch [172/938], Loss: 0.5875\n",
      "Epoch [10/10], Batch [173/938], Loss: 0.5978\n",
      "Epoch [10/10], Batch [174/938], Loss: 0.5572\n",
      "Epoch [10/10], Batch [175/938], Loss: 0.5774\n",
      "Epoch [10/10], Batch [176/938], Loss: 0.5773\n",
      "Epoch [10/10], Batch [177/938], Loss: 0.6050\n",
      "Epoch [10/10], Batch [178/938], Loss: 0.6370\n",
      "Epoch [10/10], Batch [179/938], Loss: 0.5862\n",
      "Epoch [10/10], Batch [180/938], Loss: 0.5886\n",
      "Epoch [10/10], Batch [181/938], Loss: 0.5939\n",
      "Epoch [10/10], Batch [182/938], Loss: 0.5805\n",
      "Epoch [10/10], Batch [183/938], Loss: 0.5807\n",
      "Epoch [10/10], Batch [184/938], Loss: 0.5779\n",
      "Epoch [10/10], Batch [185/938], Loss: 0.5899\n",
      "Epoch [10/10], Batch [186/938], Loss: 0.5800\n",
      "Epoch [10/10], Batch [187/938], Loss: 0.5728\n",
      "Epoch [10/10], Batch [188/938], Loss: 0.5952\n",
      "Epoch [10/10], Batch [189/938], Loss: 0.6032\n",
      "Epoch [10/10], Batch [190/938], Loss: 0.5488\n",
      "Epoch [10/10], Batch [191/938], Loss: 0.5894\n",
      "Epoch [10/10], Batch [192/938], Loss: 0.5685\n",
      "Epoch [10/10], Batch [193/938], Loss: 0.6125\n",
      "Epoch [10/10], Batch [194/938], Loss: 0.5613\n",
      "Epoch [10/10], Batch [195/938], Loss: 0.5715\n",
      "Epoch [10/10], Batch [196/938], Loss: 0.5490\n",
      "Epoch [10/10], Batch [197/938], Loss: 0.5989\n",
      "Epoch [10/10], Batch [198/938], Loss: 0.6047\n",
      "Epoch [10/10], Batch [199/938], Loss: 0.5657\n",
      "Epoch [10/10], Batch [200/938], Loss: 0.5667\n",
      "Epoch [10/10], Batch [201/938], Loss: 0.5936\n",
      "Epoch [10/10], Batch [202/938], Loss: 0.6009\n",
      "Epoch [10/10], Batch [203/938], Loss: 0.5765\n",
      "Epoch [10/10], Batch [204/938], Loss: 0.5988\n",
      "Epoch [10/10], Batch [205/938], Loss: 0.5775\n",
      "Epoch [10/10], Batch [206/938], Loss: 0.5755\n",
      "Epoch [10/10], Batch [207/938], Loss: 0.5991\n",
      "Epoch [10/10], Batch [208/938], Loss: 0.5810\n",
      "Epoch [10/10], Batch [209/938], Loss: 0.5656\n",
      "Epoch [10/10], Batch [210/938], Loss: 0.5795\n",
      "Epoch [10/10], Batch [211/938], Loss: 0.5505\n",
      "Epoch [10/10], Batch [212/938], Loss: 0.5698\n",
      "Epoch [10/10], Batch [213/938], Loss: 0.5695\n",
      "Epoch [10/10], Batch [214/938], Loss: 0.6014\n",
      "Epoch [10/10], Batch [215/938], Loss: 0.5992\n",
      "Epoch [10/10], Batch [216/938], Loss: 0.6017\n",
      "Epoch [10/10], Batch [217/938], Loss: 0.5656\n",
      "Epoch [10/10], Batch [218/938], Loss: 0.5750\n",
      "Epoch [10/10], Batch [219/938], Loss: 0.6057\n",
      "Epoch [10/10], Batch [220/938], Loss: 0.5922\n",
      "Epoch [10/10], Batch [221/938], Loss: 0.5693\n",
      "Epoch [10/10], Batch [222/938], Loss: 0.5585\n",
      "Epoch [10/10], Batch [223/938], Loss: 0.5845\n",
      "Epoch [10/10], Batch [224/938], Loss: 0.6049\n",
      "Epoch [10/10], Batch [225/938], Loss: 0.6066\n",
      "Epoch [10/10], Batch [226/938], Loss: 0.5643\n",
      "Epoch [10/10], Batch [227/938], Loss: 0.5963\n",
      "Epoch [10/10], Batch [228/938], Loss: 0.5655\n",
      "Epoch [10/10], Batch [229/938], Loss: 0.5540\n",
      "Epoch [10/10], Batch [230/938], Loss: 0.5970\n",
      "Epoch [10/10], Batch [231/938], Loss: 0.6261\n",
      "Epoch [10/10], Batch [232/938], Loss: 0.5802\n",
      "Epoch [10/10], Batch [233/938], Loss: 0.5798\n",
      "Epoch [10/10], Batch [234/938], Loss: 0.5760\n",
      "Epoch [10/10], Batch [235/938], Loss: 0.5806\n",
      "Epoch [10/10], Batch [236/938], Loss: 0.5936\n",
      "Epoch [10/10], Batch [237/938], Loss: 0.5897\n",
      "Epoch [10/10], Batch [238/938], Loss: 0.5844\n",
      "Epoch [10/10], Batch [239/938], Loss: 0.5575\n",
      "Epoch [10/10], Batch [240/938], Loss: 0.5975\n",
      "Epoch [10/10], Batch [241/938], Loss: 0.5801\n",
      "Epoch [10/10], Batch [242/938], Loss: 0.5802\n",
      "Epoch [10/10], Batch [243/938], Loss: 0.6003\n",
      "Epoch [10/10], Batch [244/938], Loss: 0.6026\n",
      "Epoch [10/10], Batch [245/938], Loss: 0.6001\n",
      "Epoch [10/10], Batch [246/938], Loss: 0.5812\n",
      "Epoch [10/10], Batch [247/938], Loss: 0.5995\n",
      "Epoch [10/10], Batch [248/938], Loss: 0.5743\n",
      "Epoch [10/10], Batch [249/938], Loss: 0.5617\n",
      "Epoch [10/10], Batch [250/938], Loss: 0.5741\n",
      "Epoch [10/10], Batch [251/938], Loss: 0.5587\n",
      "Epoch [10/10], Batch [252/938], Loss: 0.5863\n",
      "Epoch [10/10], Batch [253/938], Loss: 0.5539\n",
      "Epoch [10/10], Batch [254/938], Loss: 0.5618\n",
      "Epoch [10/10], Batch [255/938], Loss: 0.5831\n",
      "Epoch [10/10], Batch [256/938], Loss: 0.5601\n",
      "Epoch [10/10], Batch [257/938], Loss: 0.5739\n",
      "Epoch [10/10], Batch [258/938], Loss: 0.6002\n",
      "Epoch [10/10], Batch [259/938], Loss: 0.5854\n",
      "Epoch [10/10], Batch [260/938], Loss: 0.5967\n",
      "Epoch [10/10], Batch [261/938], Loss: 0.5824\n",
      "Epoch [10/10], Batch [262/938], Loss: 0.6124\n",
      "Epoch [10/10], Batch [263/938], Loss: 0.5696\n",
      "Epoch [10/10], Batch [264/938], Loss: 0.5860\n",
      "Epoch [10/10], Batch [265/938], Loss: 0.5858\n",
      "Epoch [10/10], Batch [266/938], Loss: 0.5783\n",
      "Epoch [10/10], Batch [267/938], Loss: 0.5746\n",
      "Epoch [10/10], Batch [268/938], Loss: 0.6244\n",
      "Epoch [10/10], Batch [269/938], Loss: 0.6112\n",
      "Epoch [10/10], Batch [270/938], Loss: 0.5869\n",
      "Epoch [10/10], Batch [271/938], Loss: 0.5622\n",
      "Epoch [10/10], Batch [272/938], Loss: 0.5687\n",
      "Epoch [10/10], Batch [273/938], Loss: 0.6038\n",
      "Epoch [10/10], Batch [274/938], Loss: 0.5915\n",
      "Epoch [10/10], Batch [275/938], Loss: 0.5796\n",
      "Epoch [10/10], Batch [276/938], Loss: 0.5842\n",
      "Epoch [10/10], Batch [277/938], Loss: 0.5916\n",
      "Epoch [10/10], Batch [278/938], Loss: 0.5875\n",
      "Epoch [10/10], Batch [279/938], Loss: 0.5756\n",
      "Epoch [10/10], Batch [280/938], Loss: 0.5939\n",
      "Epoch [10/10], Batch [281/938], Loss: 0.6172\n",
      "Epoch [10/10], Batch [282/938], Loss: 0.5700\n",
      "Epoch [10/10], Batch [283/938], Loss: 0.6013\n",
      "Epoch [10/10], Batch [284/938], Loss: 0.5503\n",
      "Epoch [10/10], Batch [285/938], Loss: 0.5705\n",
      "Epoch [10/10], Batch [286/938], Loss: 0.5672\n",
      "Epoch [10/10], Batch [287/938], Loss: 0.6062\n",
      "Epoch [10/10], Batch [288/938], Loss: 0.5756\n",
      "Epoch [10/10], Batch [289/938], Loss: 0.5771\n",
      "Epoch [10/10], Batch [290/938], Loss: 0.6300\n",
      "Epoch [10/10], Batch [291/938], Loss: 0.6029\n",
      "Epoch [10/10], Batch [292/938], Loss: 0.5560\n",
      "Epoch [10/10], Batch [293/938], Loss: 0.5901\n",
      "Epoch [10/10], Batch [294/938], Loss: 0.5749\n",
      "Epoch [10/10], Batch [295/938], Loss: 0.5918\n",
      "Epoch [10/10], Batch [296/938], Loss: 0.6055\n",
      "Epoch [10/10], Batch [297/938], Loss: 0.5682\n",
      "Epoch [10/10], Batch [298/938], Loss: 0.6094\n",
      "Epoch [10/10], Batch [299/938], Loss: 0.5639\n",
      "Epoch [10/10], Batch [300/938], Loss: 0.6007\n",
      "Epoch [10/10], Batch [301/938], Loss: 0.5731\n",
      "Epoch [10/10], Batch [302/938], Loss: 0.5611\n",
      "Epoch [10/10], Batch [303/938], Loss: 0.5545\n",
      "Epoch [10/10], Batch [304/938], Loss: 0.5703\n",
      "Epoch [10/10], Batch [305/938], Loss: 0.5971\n",
      "Epoch [10/10], Batch [306/938], Loss: 0.5802\n",
      "Epoch [10/10], Batch [307/938], Loss: 0.5834\n",
      "Epoch [10/10], Batch [308/938], Loss: 0.5707\n",
      "Epoch [10/10], Batch [309/938], Loss: 0.5741\n",
      "Epoch [10/10], Batch [310/938], Loss: 0.5860\n",
      "Epoch [10/10], Batch [311/938], Loss: 0.5610\n",
      "Epoch [10/10], Batch [312/938], Loss: 0.5849\n",
      "Epoch [10/10], Batch [313/938], Loss: 0.5834\n",
      "Epoch [10/10], Batch [314/938], Loss: 0.5556\n",
      "Epoch [10/10], Batch [315/938], Loss: 0.5944\n",
      "Epoch [10/10], Batch [316/938], Loss: 0.5765\n",
      "Epoch [10/10], Batch [317/938], Loss: 0.5502\n",
      "Epoch [10/10], Batch [318/938], Loss: 0.5782\n",
      "Epoch [10/10], Batch [319/938], Loss: 0.5923\n",
      "Epoch [10/10], Batch [320/938], Loss: 0.5667\n",
      "Epoch [10/10], Batch [321/938], Loss: 0.6017\n",
      "Epoch [10/10], Batch [322/938], Loss: 0.5557\n",
      "Epoch [10/10], Batch [323/938], Loss: 0.5970\n",
      "Epoch [10/10], Batch [324/938], Loss: 0.6050\n",
      "Epoch [10/10], Batch [325/938], Loss: 0.6076\n",
      "Epoch [10/10], Batch [326/938], Loss: 0.5700\n",
      "Epoch [10/10], Batch [327/938], Loss: 0.5877\n",
      "Epoch [10/10], Batch [328/938], Loss: 0.5779\n",
      "Epoch [10/10], Batch [329/938], Loss: 0.5936\n",
      "Epoch [10/10], Batch [330/938], Loss: 0.5640\n",
      "Epoch [10/10], Batch [331/938], Loss: 0.5736\n",
      "Epoch [10/10], Batch [332/938], Loss: 0.5715\n",
      "Epoch [10/10], Batch [333/938], Loss: 0.5867\n",
      "Epoch [10/10], Batch [334/938], Loss: 0.5938\n",
      "Epoch [10/10], Batch [335/938], Loss: 0.5554\n",
      "Epoch [10/10], Batch [336/938], Loss: 0.5942\n",
      "Epoch [10/10], Batch [337/938], Loss: 0.6054\n",
      "Epoch [10/10], Batch [338/938], Loss: 0.5972\n",
      "Epoch [10/10], Batch [339/938], Loss: 0.5951\n",
      "Epoch [10/10], Batch [340/938], Loss: 0.5532\n",
      "Epoch [10/10], Batch [341/938], Loss: 0.5827\n",
      "Epoch [10/10], Batch [342/938], Loss: 0.5770\n",
      "Epoch [10/10], Batch [343/938], Loss: 0.5493\n",
      "Epoch [10/10], Batch [344/938], Loss: 0.5883\n",
      "Epoch [10/10], Batch [345/938], Loss: 0.5951\n",
      "Epoch [10/10], Batch [346/938], Loss: 0.5745\n",
      "Epoch [10/10], Batch [347/938], Loss: 0.5758\n",
      "Epoch [10/10], Batch [348/938], Loss: 0.5657\n",
      "Epoch [10/10], Batch [349/938], Loss: 0.5750\n",
      "Epoch [10/10], Batch [350/938], Loss: 0.6205\n",
      "Epoch [10/10], Batch [351/938], Loss: 0.6032\n",
      "Epoch [10/10], Batch [352/938], Loss: 0.5782\n",
      "Epoch [10/10], Batch [353/938], Loss: 0.5698\n",
      "Epoch [10/10], Batch [354/938], Loss: 0.5860\n",
      "Epoch [10/10], Batch [355/938], Loss: 0.5655\n",
      "Epoch [10/10], Batch [356/938], Loss: 0.5719\n",
      "Epoch [10/10], Batch [357/938], Loss: 0.6263\n",
      "Epoch [10/10], Batch [358/938], Loss: 0.5801\n",
      "Epoch [10/10], Batch [359/938], Loss: 0.5671\n",
      "Epoch [10/10], Batch [360/938], Loss: 0.5925\n",
      "Epoch [10/10], Batch [361/938], Loss: 0.5906\n",
      "Epoch [10/10], Batch [362/938], Loss: 0.5781\n",
      "Epoch [10/10], Batch [363/938], Loss: 0.5784\n",
      "Epoch [10/10], Batch [364/938], Loss: 0.5615\n",
      "Epoch [10/10], Batch [365/938], Loss: 0.5956\n",
      "Epoch [10/10], Batch [366/938], Loss: 0.6068\n",
      "Epoch [10/10], Batch [367/938], Loss: 0.6020\n",
      "Epoch [10/10], Batch [368/938], Loss: 0.5610\n",
      "Epoch [10/10], Batch [369/938], Loss: 0.5922\n",
      "Epoch [10/10], Batch [370/938], Loss: 0.6049\n",
      "Epoch [10/10], Batch [371/938], Loss: 0.5697\n",
      "Epoch [10/10], Batch [372/938], Loss: 0.5858\n",
      "Epoch [10/10], Batch [373/938], Loss: 0.5773\n",
      "Epoch [10/10], Batch [374/938], Loss: 0.5836\n",
      "Epoch [10/10], Batch [375/938], Loss: 0.5926\n",
      "Epoch [10/10], Batch [376/938], Loss: 0.5824\n",
      "Epoch [10/10], Batch [377/938], Loss: 0.5513\n",
      "Epoch [10/10], Batch [378/938], Loss: 0.5839\n",
      "Epoch [10/10], Batch [379/938], Loss: 0.5694\n",
      "Epoch [10/10], Batch [380/938], Loss: 0.5964\n",
      "Epoch [10/10], Batch [381/938], Loss: 0.5719\n",
      "Epoch [10/10], Batch [382/938], Loss: 0.6102\n",
      "Epoch [10/10], Batch [383/938], Loss: 0.6163\n",
      "Epoch [10/10], Batch [384/938], Loss: 0.5999\n",
      "Epoch [10/10], Batch [385/938], Loss: 0.5724\n",
      "Epoch [10/10], Batch [386/938], Loss: 0.5925\n",
      "Epoch [10/10], Batch [387/938], Loss: 0.5683\n",
      "Epoch [10/10], Batch [388/938], Loss: 0.5903\n",
      "Epoch [10/10], Batch [389/938], Loss: 0.6078\n",
      "Epoch [10/10], Batch [390/938], Loss: 0.5569\n",
      "Epoch [10/10], Batch [391/938], Loss: 0.5537\n",
      "Epoch [10/10], Batch [392/938], Loss: 0.6258\n",
      "Epoch [10/10], Batch [393/938], Loss: 0.5758\n",
      "Epoch [10/10], Batch [394/938], Loss: 0.6036\n",
      "Epoch [10/10], Batch [395/938], Loss: 0.5871\n",
      "Epoch [10/10], Batch [396/938], Loss: 0.5583\n",
      "Epoch [10/10], Batch [397/938], Loss: 0.5983\n",
      "Epoch [10/10], Batch [398/938], Loss: 0.5783\n",
      "Epoch [10/10], Batch [399/938], Loss: 0.6216\n",
      "Epoch [10/10], Batch [400/938], Loss: 0.6032\n",
      "Epoch [10/10], Batch [401/938], Loss: 0.5660\n",
      "Epoch [10/10], Batch [402/938], Loss: 0.5609\n",
      "Epoch [10/10], Batch [403/938], Loss: 0.5614\n",
      "Epoch [10/10], Batch [404/938], Loss: 0.5934\n",
      "Epoch [10/10], Batch [405/938], Loss: 0.5680\n",
      "Epoch [10/10], Batch [406/938], Loss: 0.6090\n",
      "Epoch [10/10], Batch [407/938], Loss: 0.5363\n",
      "Epoch [10/10], Batch [408/938], Loss: 0.6163\n",
      "Epoch [10/10], Batch [409/938], Loss: 0.5541\n",
      "Epoch [10/10], Batch [410/938], Loss: 0.6040\n",
      "Epoch [10/10], Batch [411/938], Loss: 0.5763\n",
      "Epoch [10/10], Batch [412/938], Loss: 0.5935\n",
      "Epoch [10/10], Batch [413/938], Loss: 0.6083\n",
      "Epoch [10/10], Batch [414/938], Loss: 0.6042\n",
      "Epoch [10/10], Batch [415/938], Loss: 0.5661\n",
      "Epoch [10/10], Batch [416/938], Loss: 0.5924\n",
      "Epoch [10/10], Batch [417/938], Loss: 0.5923\n",
      "Epoch [10/10], Batch [418/938], Loss: 0.5674\n",
      "Epoch [10/10], Batch [419/938], Loss: 0.5691\n",
      "Epoch [10/10], Batch [420/938], Loss: 0.5967\n",
      "Epoch [10/10], Batch [421/938], Loss: 0.5513\n",
      "Epoch [10/10], Batch [422/938], Loss: 0.5916\n",
      "Epoch [10/10], Batch [423/938], Loss: 0.5818\n",
      "Epoch [10/10], Batch [424/938], Loss: 0.5998\n",
      "Epoch [10/10], Batch [425/938], Loss: 0.5764\n",
      "Epoch [10/10], Batch [426/938], Loss: 0.5883\n",
      "Epoch [10/10], Batch [427/938], Loss: 0.5996\n",
      "Epoch [10/10], Batch [428/938], Loss: 0.5595\n",
      "Epoch [10/10], Batch [429/938], Loss: 0.5605\n",
      "Epoch [10/10], Batch [430/938], Loss: 0.5710\n",
      "Epoch [10/10], Batch [431/938], Loss: 0.5913\n",
      "Epoch [10/10], Batch [432/938], Loss: 0.5845\n",
      "Epoch [10/10], Batch [433/938], Loss: 0.5932\n",
      "Epoch [10/10], Batch [434/938], Loss: 0.5640\n",
      "Epoch [10/10], Batch [435/938], Loss: 0.5818\n",
      "Epoch [10/10], Batch [436/938], Loss: 0.5739\n",
      "Epoch [10/10], Batch [437/938], Loss: 0.5835\n",
      "Epoch [10/10], Batch [438/938], Loss: 0.6232\n",
      "Epoch [10/10], Batch [439/938], Loss: 0.5754\n",
      "Epoch [10/10], Batch [440/938], Loss: 0.5740\n",
      "Epoch [10/10], Batch [441/938], Loss: 0.5626\n",
      "Epoch [10/10], Batch [442/938], Loss: 0.5878\n",
      "Epoch [10/10], Batch [443/938], Loss: 0.6049\n",
      "Epoch [10/10], Batch [444/938], Loss: 0.5886\n",
      "Epoch [10/10], Batch [445/938], Loss: 0.5820\n",
      "Epoch [10/10], Batch [446/938], Loss: 0.5868\n",
      "Epoch [10/10], Batch [447/938], Loss: 0.5761\n",
      "Epoch [10/10], Batch [448/938], Loss: 0.5935\n",
      "Epoch [10/10], Batch [449/938], Loss: 0.5579\n",
      "Epoch [10/10], Batch [450/938], Loss: 0.5591\n",
      "Epoch [10/10], Batch [451/938], Loss: 0.5512\n",
      "Epoch [10/10], Batch [452/938], Loss: 0.5900\n",
      "Epoch [10/10], Batch [453/938], Loss: 0.5623\n",
      "Epoch [10/10], Batch [454/938], Loss: 0.5778\n",
      "Epoch [10/10], Batch [455/938], Loss: 0.5909\n",
      "Epoch [10/10], Batch [456/938], Loss: 0.5846\n",
      "Epoch [10/10], Batch [457/938], Loss: 0.5704\n",
      "Epoch [10/10], Batch [458/938], Loss: 0.5853\n",
      "Epoch [10/10], Batch [459/938], Loss: 0.5694\n",
      "Epoch [10/10], Batch [460/938], Loss: 0.5906\n",
      "Epoch [10/10], Batch [461/938], Loss: 0.5634\n",
      "Epoch [10/10], Batch [462/938], Loss: 0.6185\n",
      "Epoch [10/10], Batch [463/938], Loss: 0.5717\n",
      "Epoch [10/10], Batch [464/938], Loss: 0.5495\n",
      "Epoch [10/10], Batch [465/938], Loss: 0.5865\n",
      "Epoch [10/10], Batch [466/938], Loss: 0.5837\n",
      "Epoch [10/10], Batch [467/938], Loss: 0.5963\n",
      "Epoch [10/10], Batch [468/938], Loss: 0.5790\n",
      "Epoch [10/10], Batch [469/938], Loss: 0.6084\n",
      "Epoch [10/10], Batch [470/938], Loss: 0.5727\n",
      "Epoch [10/10], Batch [471/938], Loss: 0.5758\n",
      "Epoch [10/10], Batch [472/938], Loss: 0.5616\n",
      "Epoch [10/10], Batch [473/938], Loss: 0.6053\n",
      "Epoch [10/10], Batch [474/938], Loss: 0.5834\n",
      "Epoch [10/10], Batch [475/938], Loss: 0.5863\n",
      "Epoch [10/10], Batch [476/938], Loss: 0.6013\n",
      "Epoch [10/10], Batch [477/938], Loss: 0.6235\n",
      "Epoch [10/10], Batch [478/938], Loss: 0.5766\n",
      "Epoch [10/10], Batch [479/938], Loss: 0.5726\n",
      "Epoch [10/10], Batch [480/938], Loss: 0.5590\n",
      "Epoch [10/10], Batch [481/938], Loss: 0.5786\n",
      "Epoch [10/10], Batch [482/938], Loss: 0.5856\n",
      "Epoch [10/10], Batch [483/938], Loss: 0.6071\n",
      "Epoch [10/10], Batch [484/938], Loss: 0.5953\n",
      "Epoch [10/10], Batch [485/938], Loss: 0.5835\n",
      "Epoch [10/10], Batch [486/938], Loss: 0.6051\n",
      "Epoch [10/10], Batch [487/938], Loss: 0.5893\n",
      "Epoch [10/10], Batch [488/938], Loss: 0.5930\n",
      "Epoch [10/10], Batch [489/938], Loss: 0.5855\n",
      "Epoch [10/10], Batch [490/938], Loss: 0.5854\n",
      "Epoch [10/10], Batch [491/938], Loss: 0.5548\n",
      "Epoch [10/10], Batch [492/938], Loss: 0.5804\n",
      "Epoch [10/10], Batch [493/938], Loss: 0.5831\n",
      "Epoch [10/10], Batch [494/938], Loss: 0.5934\n",
      "Epoch [10/10], Batch [495/938], Loss: 0.5827\n",
      "Epoch [10/10], Batch [496/938], Loss: 0.5592\n",
      "Epoch [10/10], Batch [497/938], Loss: 0.6153\n",
      "Epoch [10/10], Batch [498/938], Loss: 0.5629\n",
      "Epoch [10/10], Batch [499/938], Loss: 0.5860\n",
      "Epoch [10/10], Batch [500/938], Loss: 0.5504\n",
      "Epoch [10/10], Batch [501/938], Loss: 0.5819\n",
      "Epoch [10/10], Batch [502/938], Loss: 0.5745\n",
      "Epoch [10/10], Batch [503/938], Loss: 0.6052\n",
      "Epoch [10/10], Batch [504/938], Loss: 0.5711\n",
      "Epoch [10/10], Batch [505/938], Loss: 0.5804\n",
      "Epoch [10/10], Batch [506/938], Loss: 0.5841\n",
      "Epoch [10/10], Batch [507/938], Loss: 0.5889\n",
      "Epoch [10/10], Batch [508/938], Loss: 0.5916\n",
      "Epoch [10/10], Batch [509/938], Loss: 0.5909\n",
      "Epoch [10/10], Batch [510/938], Loss: 0.5801\n",
      "Epoch [10/10], Batch [511/938], Loss: 0.5853\n",
      "Epoch [10/10], Batch [512/938], Loss: 0.5839\n",
      "Epoch [10/10], Batch [513/938], Loss: 0.5941\n",
      "Epoch [10/10], Batch [514/938], Loss: 0.6012\n",
      "Epoch [10/10], Batch [515/938], Loss: 0.5896\n",
      "Epoch [10/10], Batch [516/938], Loss: 0.5656\n",
      "Epoch [10/10], Batch [517/938], Loss: 0.5837\n",
      "Epoch [10/10], Batch [518/938], Loss: 0.5872\n",
      "Epoch [10/10], Batch [519/938], Loss: 0.5517\n",
      "Epoch [10/10], Batch [520/938], Loss: 0.5695\n",
      "Epoch [10/10], Batch [521/938], Loss: 0.5616\n",
      "Epoch [10/10], Batch [522/938], Loss: 0.6000\n",
      "Epoch [10/10], Batch [523/938], Loss: 0.5966\n",
      "Epoch [10/10], Batch [524/938], Loss: 0.6114\n",
      "Epoch [10/10], Batch [525/938], Loss: 0.5660\n",
      "Epoch [10/10], Batch [526/938], Loss: 0.6081\n",
      "Epoch [10/10], Batch [527/938], Loss: 0.5682\n",
      "Epoch [10/10], Batch [528/938], Loss: 0.5630\n",
      "Epoch [10/10], Batch [529/938], Loss: 0.5804\n",
      "Epoch [10/10], Batch [530/938], Loss: 0.5742\n",
      "Epoch [10/10], Batch [531/938], Loss: 0.5493\n",
      "Epoch [10/10], Batch [532/938], Loss: 0.5970\n",
      "Epoch [10/10], Batch [533/938], Loss: 0.5491\n",
      "Epoch [10/10], Batch [534/938], Loss: 0.5707\n",
      "Epoch [10/10], Batch [535/938], Loss: 0.6070\n",
      "Epoch [10/10], Batch [536/938], Loss: 0.6069\n",
      "Epoch [10/10], Batch [537/938], Loss: 0.5777\n",
      "Epoch [10/10], Batch [538/938], Loss: 0.6076\n",
      "Epoch [10/10], Batch [539/938], Loss: 0.6081\n",
      "Epoch [10/10], Batch [540/938], Loss: 0.5917\n",
      "Epoch [10/10], Batch [541/938], Loss: 0.5829\n",
      "Epoch [10/10], Batch [542/938], Loss: 0.5439\n",
      "Epoch [10/10], Batch [543/938], Loss: 0.5584\n",
      "Epoch [10/10], Batch [544/938], Loss: 0.5797\n",
      "Epoch [10/10], Batch [545/938], Loss: 0.5902\n",
      "Epoch [10/10], Batch [546/938], Loss: 0.5798\n",
      "Epoch [10/10], Batch [547/938], Loss: 0.5887\n",
      "Epoch [10/10], Batch [548/938], Loss: 0.5687\n",
      "Epoch [10/10], Batch [549/938], Loss: 0.5599\n",
      "Epoch [10/10], Batch [550/938], Loss: 0.5826\n",
      "Epoch [10/10], Batch [551/938], Loss: 0.5940\n",
      "Epoch [10/10], Batch [552/938], Loss: 0.5907\n",
      "Epoch [10/10], Batch [553/938], Loss: 0.5626\n",
      "Epoch [10/10], Batch [554/938], Loss: 0.5707\n",
      "Epoch [10/10], Batch [555/938], Loss: 0.5828\n",
      "Epoch [10/10], Batch [556/938], Loss: 0.5923\n",
      "Epoch [10/10], Batch [557/938], Loss: 0.5834\n",
      "Epoch [10/10], Batch [558/938], Loss: 0.5729\n",
      "Epoch [10/10], Batch [559/938], Loss: 0.5597\n",
      "Epoch [10/10], Batch [560/938], Loss: 0.5769\n",
      "Epoch [10/10], Batch [561/938], Loss: 0.5919\n",
      "Epoch [10/10], Batch [562/938], Loss: 0.5519\n",
      "Epoch [10/10], Batch [563/938], Loss: 0.5636\n",
      "Epoch [10/10], Batch [564/938], Loss: 0.5687\n",
      "Epoch [10/10], Batch [565/938], Loss: 0.5769\n",
      "Epoch [10/10], Batch [566/938], Loss: 0.5606\n",
      "Epoch [10/10], Batch [567/938], Loss: 0.6003\n",
      "Epoch [10/10], Batch [568/938], Loss: 0.5595\n",
      "Epoch [10/10], Batch [569/938], Loss: 0.5952\n",
      "Epoch [10/10], Batch [570/938], Loss: 0.5723\n",
      "Epoch [10/10], Batch [571/938], Loss: 0.5915\n",
      "Epoch [10/10], Batch [572/938], Loss: 0.6057\n",
      "Epoch [10/10], Batch [573/938], Loss: 0.5575\n",
      "Epoch [10/10], Batch [574/938], Loss: 0.5775\n",
      "Epoch [10/10], Batch [575/938], Loss: 0.5789\n",
      "Epoch [10/10], Batch [576/938], Loss: 0.5836\n",
      "Epoch [10/10], Batch [577/938], Loss: 0.5848\n",
      "Epoch [10/10], Batch [578/938], Loss: 0.5824\n",
      "Epoch [10/10], Batch [579/938], Loss: 0.5837\n",
      "Epoch [10/10], Batch [580/938], Loss: 0.5913\n",
      "Epoch [10/10], Batch [581/938], Loss: 0.5890\n",
      "Epoch [10/10], Batch [582/938], Loss: 0.6025\n",
      "Epoch [10/10], Batch [583/938], Loss: 0.5792\n",
      "Epoch [10/10], Batch [584/938], Loss: 0.5769\n",
      "Epoch [10/10], Batch [585/938], Loss: 0.5755\n",
      "Epoch [10/10], Batch [586/938], Loss: 0.5834\n",
      "Epoch [10/10], Batch [587/938], Loss: 0.5628\n",
      "Epoch [10/10], Batch [588/938], Loss: 0.5691\n",
      "Epoch [10/10], Batch [589/938], Loss: 0.5710\n",
      "Epoch [10/10], Batch [590/938], Loss: 0.5779\n",
      "Epoch [10/10], Batch [591/938], Loss: 0.5819\n",
      "Epoch [10/10], Batch [592/938], Loss: 0.5490\n",
      "Epoch [10/10], Batch [593/938], Loss: 0.5871\n",
      "Epoch [10/10], Batch [594/938], Loss: 0.5768\n",
      "Epoch [10/10], Batch [595/938], Loss: 0.6087\n",
      "Epoch [10/10], Batch [596/938], Loss: 0.5723\n",
      "Epoch [10/10], Batch [597/938], Loss: 0.6051\n",
      "Epoch [10/10], Batch [598/938], Loss: 0.5797\n",
      "Epoch [10/10], Batch [599/938], Loss: 0.5653\n",
      "Epoch [10/10], Batch [600/938], Loss: 0.5595\n",
      "Epoch [10/10], Batch [601/938], Loss: 0.5642\n",
      "Epoch [10/10], Batch [602/938], Loss: 0.5915\n",
      "Epoch [10/10], Batch [603/938], Loss: 0.5921\n",
      "Epoch [10/10], Batch [604/938], Loss: 0.5743\n",
      "Epoch [10/10], Batch [605/938], Loss: 0.5819\n",
      "Epoch [10/10], Batch [606/938], Loss: 0.5812\n",
      "Epoch [10/10], Batch [607/938], Loss: 0.5642\n",
      "Epoch [10/10], Batch [608/938], Loss: 0.5801\n",
      "Epoch [10/10], Batch [609/938], Loss: 0.5713\n",
      "Epoch [10/10], Batch [610/938], Loss: 0.5800\n",
      "Epoch [10/10], Batch [611/938], Loss: 0.5815\n",
      "Epoch [10/10], Batch [612/938], Loss: 0.6080\n",
      "Epoch [10/10], Batch [613/938], Loss: 0.5570\n",
      "Epoch [10/10], Batch [614/938], Loss: 0.5975\n",
      "Epoch [10/10], Batch [615/938], Loss: 0.5780\n",
      "Epoch [10/10], Batch [616/938], Loss: 0.5792\n",
      "Epoch [10/10], Batch [617/938], Loss: 0.5761\n",
      "Epoch [10/10], Batch [618/938], Loss: 0.5684\n",
      "Epoch [10/10], Batch [619/938], Loss: 0.5696\n",
      "Epoch [10/10], Batch [620/938], Loss: 0.5554\n",
      "Epoch [10/10], Batch [621/938], Loss: 0.5647\n",
      "Epoch [10/10], Batch [622/938], Loss: 0.5664\n",
      "Epoch [10/10], Batch [623/938], Loss: 0.5633\n",
      "Epoch [10/10], Batch [624/938], Loss: 0.5666\n",
      "Epoch [10/10], Batch [625/938], Loss: 0.5725\n",
      "Epoch [10/10], Batch [626/938], Loss: 0.5849\n",
      "Epoch [10/10], Batch [627/938], Loss: 0.5651\n",
      "Epoch [10/10], Batch [628/938], Loss: 0.6062\n",
      "Epoch [10/10], Batch [629/938], Loss: 0.5857\n",
      "Epoch [10/10], Batch [630/938], Loss: 0.5936\n",
      "Epoch [10/10], Batch [631/938], Loss: 0.5417\n",
      "Epoch [10/10], Batch [632/938], Loss: 0.5604\n",
      "Epoch [10/10], Batch [633/938], Loss: 0.5665\n",
      "Epoch [10/10], Batch [634/938], Loss: 0.5756\n",
      "Epoch [10/10], Batch [635/938], Loss: 0.5884\n",
      "Epoch [10/10], Batch [636/938], Loss: 0.5683\n",
      "Epoch [10/10], Batch [637/938], Loss: 0.6095\n",
      "Epoch [10/10], Batch [638/938], Loss: 0.5607\n",
      "Epoch [10/10], Batch [639/938], Loss: 0.5752\n",
      "Epoch [10/10], Batch [640/938], Loss: 0.5725\n",
      "Epoch [10/10], Batch [641/938], Loss: 0.6008\n",
      "Epoch [10/10], Batch [642/938], Loss: 0.6001\n",
      "Epoch [10/10], Batch [643/938], Loss: 0.6147\n",
      "Epoch [10/10], Batch [644/938], Loss: 0.5691\n",
      "Epoch [10/10], Batch [645/938], Loss: 0.5721\n",
      "Epoch [10/10], Batch [646/938], Loss: 0.6042\n",
      "Epoch [10/10], Batch [647/938], Loss: 0.5707\n",
      "Epoch [10/10], Batch [648/938], Loss: 0.5889\n",
      "Epoch [10/10], Batch [649/938], Loss: 0.5735\n",
      "Epoch [10/10], Batch [650/938], Loss: 0.5831\n",
      "Epoch [10/10], Batch [651/938], Loss: 0.5799\n",
      "Epoch [10/10], Batch [652/938], Loss: 0.5592\n",
      "Epoch [10/10], Batch [653/938], Loss: 0.5726\n",
      "Epoch [10/10], Batch [654/938], Loss: 0.5750\n",
      "Epoch [10/10], Batch [655/938], Loss: 0.5874\n",
      "Epoch [10/10], Batch [656/938], Loss: 0.5866\n",
      "Epoch [10/10], Batch [657/938], Loss: 0.5794\n",
      "Epoch [10/10], Batch [658/938], Loss: 0.5828\n",
      "Epoch [10/10], Batch [659/938], Loss: 0.6135\n",
      "Epoch [10/10], Batch [660/938], Loss: 0.6149\n",
      "Epoch [10/10], Batch [661/938], Loss: 0.5829\n",
      "Epoch [10/10], Batch [662/938], Loss: 0.6234\n",
      "Epoch [10/10], Batch [663/938], Loss: 0.5689\n",
      "Epoch [10/10], Batch [664/938], Loss: 0.5685\n",
      "Epoch [10/10], Batch [665/938], Loss: 0.5764\n",
      "Epoch [10/10], Batch [666/938], Loss: 0.6014\n",
      "Epoch [10/10], Batch [667/938], Loss: 0.5717\n",
      "Epoch [10/10], Batch [668/938], Loss: 0.6042\n",
      "Epoch [10/10], Batch [669/938], Loss: 0.5811\n",
      "Epoch [10/10], Batch [670/938], Loss: 0.5791\n",
      "Epoch [10/10], Batch [671/938], Loss: 0.5745\n",
      "Epoch [10/10], Batch [672/938], Loss: 0.5997\n",
      "Epoch [10/10], Batch [673/938], Loss: 0.5768\n",
      "Epoch [10/10], Batch [674/938], Loss: 0.5783\n",
      "Epoch [10/10], Batch [675/938], Loss: 0.6148\n",
      "Epoch [10/10], Batch [676/938], Loss: 0.5578\n",
      "Epoch [10/10], Batch [677/938], Loss: 0.5723\n",
      "Epoch [10/10], Batch [678/938], Loss: 0.5818\n",
      "Epoch [10/10], Batch [679/938], Loss: 0.5874\n",
      "Epoch [10/10], Batch [680/938], Loss: 0.5897\n",
      "Epoch [10/10], Batch [681/938], Loss: 0.5878\n",
      "Epoch [10/10], Batch [682/938], Loss: 0.5820\n",
      "Epoch [10/10], Batch [683/938], Loss: 0.6165\n",
      "Epoch [10/10], Batch [684/938], Loss: 0.5577\n",
      "Epoch [10/10], Batch [685/938], Loss: 0.5863\n",
      "Epoch [10/10], Batch [686/938], Loss: 0.5652\n",
      "Epoch [10/10], Batch [687/938], Loss: 0.5914\n",
      "Epoch [10/10], Batch [688/938], Loss: 0.5737\n",
      "Epoch [10/10], Batch [689/938], Loss: 0.5918\n",
      "Epoch [10/10], Batch [690/938], Loss: 0.5678\n",
      "Epoch [10/10], Batch [691/938], Loss: 0.5666\n",
      "Epoch [10/10], Batch [692/938], Loss: 0.5725\n",
      "Epoch [10/10], Batch [693/938], Loss: 0.5888\n",
      "Epoch [10/10], Batch [694/938], Loss: 0.5688\n",
      "Epoch [10/10], Batch [695/938], Loss: 0.5754\n",
      "Epoch [10/10], Batch [696/938], Loss: 0.5751\n",
      "Epoch [10/10], Batch [697/938], Loss: 0.5857\n",
      "Epoch [10/10], Batch [698/938], Loss: 0.6118\n",
      "Epoch [10/10], Batch [699/938], Loss: 0.5723\n",
      "Epoch [10/10], Batch [700/938], Loss: 0.5684\n",
      "Epoch [10/10], Batch [701/938], Loss: 0.5926\n",
      "Epoch [10/10], Batch [702/938], Loss: 0.5905\n",
      "Epoch [10/10], Batch [703/938], Loss: 0.5776\n",
      "Epoch [10/10], Batch [704/938], Loss: 0.5825\n",
      "Epoch [10/10], Batch [705/938], Loss: 0.5643\n",
      "Epoch [10/10], Batch [706/938], Loss: 0.5934\n",
      "Epoch [10/10], Batch [707/938], Loss: 0.6053\n",
      "Epoch [10/10], Batch [708/938], Loss: 0.5890\n",
      "Epoch [10/10], Batch [709/938], Loss: 0.5848\n",
      "Epoch [10/10], Batch [710/938], Loss: 0.5769\n",
      "Epoch [10/10], Batch [711/938], Loss: 0.5562\n",
      "Epoch [10/10], Batch [712/938], Loss: 0.6074\n",
      "Epoch [10/10], Batch [713/938], Loss: 0.5736\n",
      "Epoch [10/10], Batch [714/938], Loss: 0.5931\n",
      "Epoch [10/10], Batch [715/938], Loss: 0.5650\n",
      "Epoch [10/10], Batch [716/938], Loss: 0.5613\n",
      "Epoch [10/10], Batch [717/938], Loss: 0.6066\n",
      "Epoch [10/10], Batch [718/938], Loss: 0.5776\n",
      "Epoch [10/10], Batch [719/938], Loss: 0.5870\n",
      "Epoch [10/10], Batch [720/938], Loss: 0.5540\n",
      "Epoch [10/10], Batch [721/938], Loss: 0.5856\n",
      "Epoch [10/10], Batch [722/938], Loss: 0.5906\n",
      "Epoch [10/10], Batch [723/938], Loss: 0.6044\n",
      "Epoch [10/10], Batch [724/938], Loss: 0.5883\n",
      "Epoch [10/10], Batch [725/938], Loss: 0.5973\n",
      "Epoch [10/10], Batch [726/938], Loss: 0.5728\n",
      "Epoch [10/10], Batch [727/938], Loss: 0.6047\n",
      "Epoch [10/10], Batch [728/938], Loss: 0.5624\n",
      "Epoch [10/10], Batch [729/938], Loss: 0.5951\n",
      "Epoch [10/10], Batch [730/938], Loss: 0.5973\n",
      "Epoch [10/10], Batch [731/938], Loss: 0.5874\n",
      "Epoch [10/10], Batch [732/938], Loss: 0.5803\n",
      "Epoch [10/10], Batch [733/938], Loss: 0.5754\n",
      "Epoch [10/10], Batch [734/938], Loss: 0.5827\n",
      "Epoch [10/10], Batch [735/938], Loss: 0.5700\n",
      "Epoch [10/10], Batch [736/938], Loss: 0.5802\n",
      "Epoch [10/10], Batch [737/938], Loss: 0.5909\n",
      "Epoch [10/10], Batch [738/938], Loss: 0.6116\n",
      "Epoch [10/10], Batch [739/938], Loss: 0.5705\n",
      "Epoch [10/10], Batch [740/938], Loss: 0.6158\n",
      "Epoch [10/10], Batch [741/938], Loss: 0.5641\n",
      "Epoch [10/10], Batch [742/938], Loss: 0.6192\n",
      "Epoch [10/10], Batch [743/938], Loss: 0.6060\n",
      "Epoch [10/10], Batch [744/938], Loss: 0.5792\n",
      "Epoch [10/10], Batch [745/938], Loss: 0.5905\n",
      "Epoch [10/10], Batch [746/938], Loss: 0.5622\n",
      "Epoch [10/10], Batch [747/938], Loss: 0.5792\n",
      "Epoch [10/10], Batch [748/938], Loss: 0.5590\n",
      "Epoch [10/10], Batch [749/938], Loss: 0.5754\n",
      "Epoch [10/10], Batch [750/938], Loss: 0.5824\n",
      "Epoch [10/10], Batch [751/938], Loss: 0.5710\n",
      "Epoch [10/10], Batch [752/938], Loss: 0.6018\n",
      "Epoch [10/10], Batch [753/938], Loss: 0.5813\n",
      "Epoch [10/10], Batch [754/938], Loss: 0.5779\n",
      "Epoch [10/10], Batch [755/938], Loss: 0.5577\n",
      "Epoch [10/10], Batch [756/938], Loss: 0.5790\n",
      "Epoch [10/10], Batch [757/938], Loss: 0.5690\n",
      "Epoch [10/10], Batch [758/938], Loss: 0.5869\n",
      "Epoch [10/10], Batch [759/938], Loss: 0.5958\n",
      "Epoch [10/10], Batch [760/938], Loss: 0.6034\n",
      "Epoch [10/10], Batch [761/938], Loss: 0.5546\n",
      "Epoch [10/10], Batch [762/938], Loss: 0.5908\n",
      "Epoch [10/10], Batch [763/938], Loss: 0.5569\n",
      "Epoch [10/10], Batch [764/938], Loss: 0.5847\n",
      "Epoch [10/10], Batch [765/938], Loss: 0.5940\n",
      "Epoch [10/10], Batch [766/938], Loss: 0.5755\n",
      "Epoch [10/10], Batch [767/938], Loss: 0.6002\n",
      "Epoch [10/10], Batch [768/938], Loss: 0.6038\n",
      "Epoch [10/10], Batch [769/938], Loss: 0.5901\n",
      "Epoch [10/10], Batch [770/938], Loss: 0.5930\n",
      "Epoch [10/10], Batch [771/938], Loss: 0.5702\n",
      "Epoch [10/10], Batch [772/938], Loss: 0.5800\n",
      "Epoch [10/10], Batch [773/938], Loss: 0.6254\n",
      "Epoch [10/10], Batch [774/938], Loss: 0.5553\n",
      "Epoch [10/10], Batch [775/938], Loss: 0.6188\n",
      "Epoch [10/10], Batch [776/938], Loss: 0.5701\n",
      "Epoch [10/10], Batch [777/938], Loss: 0.5500\n",
      "Epoch [10/10], Batch [778/938], Loss: 0.5672\n",
      "Epoch [10/10], Batch [779/938], Loss: 0.5910\n",
      "Epoch [10/10], Batch [780/938], Loss: 0.5884\n",
      "Epoch [10/10], Batch [781/938], Loss: 0.6063\n",
      "Epoch [10/10], Batch [782/938], Loss: 0.5696\n",
      "Epoch [10/10], Batch [783/938], Loss: 0.5891\n",
      "Epoch [10/10], Batch [784/938], Loss: 0.6007\n",
      "Epoch [10/10], Batch [785/938], Loss: 0.5889\n",
      "Epoch [10/10], Batch [786/938], Loss: 0.6052\n",
      "Epoch [10/10], Batch [787/938], Loss: 0.5907\n",
      "Epoch [10/10], Batch [788/938], Loss: 0.6046\n",
      "Epoch [10/10], Batch [789/938], Loss: 0.5545\n",
      "Epoch [10/10], Batch [790/938], Loss: 0.5922\n",
      "Epoch [10/10], Batch [791/938], Loss: 0.5917\n",
      "Epoch [10/10], Batch [792/938], Loss: 0.5768\n",
      "Epoch [10/10], Batch [793/938], Loss: 0.5813\n",
      "Epoch [10/10], Batch [794/938], Loss: 0.5777\n",
      "Epoch [10/10], Batch [795/938], Loss: 0.5764\n",
      "Epoch [10/10], Batch [796/938], Loss: 0.5837\n",
      "Epoch [10/10], Batch [797/938], Loss: 0.5841\n",
      "Epoch [10/10], Batch [798/938], Loss: 0.5797\n",
      "Epoch [10/10], Batch [799/938], Loss: 0.5723\n",
      "Epoch [10/10], Batch [800/938], Loss: 0.5991\n",
      "Epoch [10/10], Batch [801/938], Loss: 0.5925\n",
      "Epoch [10/10], Batch [802/938], Loss: 0.5688\n",
      "Epoch [10/10], Batch [803/938], Loss: 0.5681\n",
      "Epoch [10/10], Batch [804/938], Loss: 0.5675\n",
      "Epoch [10/10], Batch [805/938], Loss: 0.5999\n",
      "Epoch [10/10], Batch [806/938], Loss: 0.5741\n",
      "Epoch [10/10], Batch [807/938], Loss: 0.5730\n",
      "Epoch [10/10], Batch [808/938], Loss: 0.5551\n",
      "Epoch [10/10], Batch [809/938], Loss: 0.5648\n",
      "Epoch [10/10], Batch [810/938], Loss: 0.5872\n",
      "Epoch [10/10], Batch [811/938], Loss: 0.5838\n",
      "Epoch [10/10], Batch [812/938], Loss: 0.5658\n",
      "Epoch [10/10], Batch [813/938], Loss: 0.5699\n",
      "Epoch [10/10], Batch [814/938], Loss: 0.5756\n",
      "Epoch [10/10], Batch [815/938], Loss: 0.5740\n",
      "Epoch [10/10], Batch [816/938], Loss: 0.6032\n",
      "Epoch [10/10], Batch [817/938], Loss: 0.5918\n",
      "Epoch [10/10], Batch [818/938], Loss: 0.5670\n",
      "Epoch [10/10], Batch [819/938], Loss: 0.5894\n",
      "Epoch [10/10], Batch [820/938], Loss: 0.5801\n",
      "Epoch [10/10], Batch [821/938], Loss: 0.5952\n",
      "Epoch [10/10], Batch [822/938], Loss: 0.5919\n",
      "Epoch [10/10], Batch [823/938], Loss: 0.6058\n",
      "Epoch [10/10], Batch [824/938], Loss: 0.5790\n",
      "Epoch [10/10], Batch [825/938], Loss: 0.5682\n",
      "Epoch [10/10], Batch [826/938], Loss: 0.5911\n",
      "Epoch [10/10], Batch [827/938], Loss: 0.5812\n",
      "Epoch [10/10], Batch [828/938], Loss: 0.5849\n",
      "Epoch [10/10], Batch [829/938], Loss: 0.5888\n",
      "Epoch [10/10], Batch [830/938], Loss: 0.5695\n",
      "Epoch [10/10], Batch [831/938], Loss: 0.5982\n",
      "Epoch [10/10], Batch [832/938], Loss: 0.5610\n",
      "Epoch [10/10], Batch [833/938], Loss: 0.5637\n",
      "Epoch [10/10], Batch [834/938], Loss: 0.5694\n",
      "Epoch [10/10], Batch [835/938], Loss: 0.5772\n",
      "Epoch [10/10], Batch [836/938], Loss: 0.5427\n",
      "Epoch [10/10], Batch [837/938], Loss: 0.5861\n",
      "Epoch [10/10], Batch [838/938], Loss: 0.5926\n",
      "Epoch [10/10], Batch [839/938], Loss: 0.5847\n",
      "Epoch [10/10], Batch [840/938], Loss: 0.5595\n",
      "Epoch [10/10], Batch [841/938], Loss: 0.5613\n",
      "Epoch [10/10], Batch [842/938], Loss: 0.5574\n",
      "Epoch [10/10], Batch [843/938], Loss: 0.5847\n",
      "Epoch [10/10], Batch [844/938], Loss: 0.5978\n",
      "Epoch [10/10], Batch [845/938], Loss: 0.5938\n",
      "Epoch [10/10], Batch [846/938], Loss: 0.5880\n",
      "Epoch [10/10], Batch [847/938], Loss: 0.5866\n",
      "Epoch [10/10], Batch [848/938], Loss: 0.6129\n",
      "Epoch [10/10], Batch [849/938], Loss: 0.5747\n",
      "Epoch [10/10], Batch [850/938], Loss: 0.5848\n",
      "Epoch [10/10], Batch [851/938], Loss: 0.5880\n",
      "Epoch [10/10], Batch [852/938], Loss: 0.5636\n",
      "Epoch [10/10], Batch [853/938], Loss: 0.5620\n",
      "Epoch [10/10], Batch [854/938], Loss: 0.5925\n",
      "Epoch [10/10], Batch [855/938], Loss: 0.5968\n",
      "Epoch [10/10], Batch [856/938], Loss: 0.5560\n",
      "Epoch [10/10], Batch [857/938], Loss: 0.5835\n",
      "Epoch [10/10], Batch [858/938], Loss: 0.5504\n",
      "Epoch [10/10], Batch [859/938], Loss: 0.5776\n",
      "Epoch [10/10], Batch [860/938], Loss: 0.5879\n",
      "Epoch [10/10], Batch [861/938], Loss: 0.5743\n",
      "Epoch [10/10], Batch [862/938], Loss: 0.6028\n",
      "Epoch [10/10], Batch [863/938], Loss: 0.6012\n",
      "Epoch [10/10], Batch [864/938], Loss: 0.5639\n",
      "Epoch [10/10], Batch [865/938], Loss: 0.5871\n",
      "Epoch [10/10], Batch [866/938], Loss: 0.5741\n",
      "Epoch [10/10], Batch [867/938], Loss: 0.5866\n",
      "Epoch [10/10], Batch [868/938], Loss: 0.5448\n",
      "Epoch [10/10], Batch [869/938], Loss: 0.5859\n",
      "Epoch [10/10], Batch [870/938], Loss: 0.5850\n",
      "Epoch [10/10], Batch [871/938], Loss: 0.5683\n",
      "Epoch [10/10], Batch [872/938], Loss: 0.5529\n",
      "Epoch [10/10], Batch [873/938], Loss: 0.5876\n",
      "Epoch [10/10], Batch [874/938], Loss: 0.5830\n",
      "Epoch [10/10], Batch [875/938], Loss: 0.6072\n",
      "Epoch [10/10], Batch [876/938], Loss: 0.6137\n",
      "Epoch [10/10], Batch [877/938], Loss: 0.5830\n",
      "Epoch [10/10], Batch [878/938], Loss: 0.5817\n",
      "Epoch [10/10], Batch [879/938], Loss: 0.5663\n",
      "Epoch [10/10], Batch [880/938], Loss: 0.5704\n",
      "Epoch [10/10], Batch [881/938], Loss: 0.5850\n",
      "Epoch [10/10], Batch [882/938], Loss: 0.5992\n",
      "Epoch [10/10], Batch [883/938], Loss: 0.5796\n",
      "Epoch [10/10], Batch [884/938], Loss: 0.5928\n",
      "Epoch [10/10], Batch [885/938], Loss: 0.5854\n",
      "Epoch [10/10], Batch [886/938], Loss: 0.5804\n",
      "Epoch [10/10], Batch [887/938], Loss: 0.5658\n",
      "Epoch [10/10], Batch [888/938], Loss: 0.5856\n",
      "Epoch [10/10], Batch [889/938], Loss: 0.5775\n",
      "Epoch [10/10], Batch [890/938], Loss: 0.5732\n",
      "Epoch [10/10], Batch [891/938], Loss: 0.5867\n",
      "Epoch [10/10], Batch [892/938], Loss: 0.5801\n",
      "Epoch [10/10], Batch [893/938], Loss: 0.5995\n",
      "Epoch [10/10], Batch [894/938], Loss: 0.5730\n",
      "Epoch [10/10], Batch [895/938], Loss: 0.5968\n",
      "Epoch [10/10], Batch [896/938], Loss: 0.5856\n",
      "Epoch [10/10], Batch [897/938], Loss: 0.5939\n",
      "Epoch [10/10], Batch [898/938], Loss: 0.5775\n",
      "Epoch [10/10], Batch [899/938], Loss: 0.5977\n",
      "Epoch [10/10], Batch [900/938], Loss: 0.6046\n",
      "Epoch [10/10], Batch [901/938], Loss: 0.5684\n",
      "Epoch [10/10], Batch [902/938], Loss: 0.5899\n",
      "Epoch [10/10], Batch [903/938], Loss: 0.5825\n",
      "Epoch [10/10], Batch [904/938], Loss: 0.5848\n",
      "Epoch [10/10], Batch [905/938], Loss: 0.5613\n",
      "Epoch [10/10], Batch [906/938], Loss: 0.5805\n",
      "Epoch [10/10], Batch [907/938], Loss: 0.5550\n",
      "Epoch [10/10], Batch [908/938], Loss: 0.5765\n",
      "Epoch [10/10], Batch [909/938], Loss: 0.5840\n",
      "Epoch [10/10], Batch [910/938], Loss: 0.5919\n",
      "Epoch [10/10], Batch [911/938], Loss: 0.6059\n",
      "Epoch [10/10], Batch [912/938], Loss: 0.5844\n",
      "Epoch [10/10], Batch [913/938], Loss: 0.6015\n",
      "Epoch [10/10], Batch [914/938], Loss: 0.5936\n",
      "Epoch [10/10], Batch [915/938], Loss: 0.5979\n",
      "Epoch [10/10], Batch [916/938], Loss: 0.5720\n",
      "Epoch [10/10], Batch [917/938], Loss: 0.5807\n",
      "Epoch [10/10], Batch [918/938], Loss: 0.5530\n",
      "Epoch [10/10], Batch [919/938], Loss: 0.5618\n",
      "Epoch [10/10], Batch [920/938], Loss: 0.5863\n",
      "Epoch [10/10], Batch [921/938], Loss: 0.5764\n",
      "Epoch [10/10], Batch [922/938], Loss: 0.5666\n",
      "Epoch [10/10], Batch [923/938], Loss: 0.5905\n",
      "Epoch [10/10], Batch [924/938], Loss: 0.5784\n",
      "Epoch [10/10], Batch [925/938], Loss: 0.5694\n",
      "Epoch [10/10], Batch [926/938], Loss: 0.5564\n",
      "Epoch [10/10], Batch [927/938], Loss: 0.5639\n",
      "Epoch [10/10], Batch [928/938], Loss: 0.6016\n",
      "Epoch [10/10], Batch [929/938], Loss: 0.5678\n",
      "Epoch [10/10], Batch [930/938], Loss: 0.5948\n",
      "Epoch [10/10], Batch [931/938], Loss: 0.5623\n",
      "Epoch [10/10], Batch [932/938], Loss: 0.5502\n",
      "Epoch [10/10], Batch [933/938], Loss: 0.5841\n",
      "Epoch [10/10], Batch [934/938], Loss: 0.5656\n",
      "Epoch [10/10], Batch [935/938], Loss: 0.5905\n",
      "Epoch [10/10], Batch [936/938], Loss: 0.5956\n",
      "Epoch [10/10], Batch [937/938], Loss: 0.5979\n",
      "Epoch [10/10], Batch [938/938], Loss: 0.5800\n",
      "Epoch [10/10], Loss: 0.5800\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "input_size = (13, 768)\n",
    "num_classes = 9\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = MultiClassClassifier(input_size, num_classes).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataset = EmbeddingDataset(train_embedding_label)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "train(model, criterion, optimizer, train_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MERT_model_different_threshold.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/3590111010.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('MERT_model_different_threshold.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiClassClassifier(\n",
       "  (fc1): Linear(in_features=9984, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=9, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model architecture\n",
    "model = MultiClassClassifier(input_size, num_classes)\n",
    "\n",
    "# Load the state dictionary\n",
    "model.load_state_dict(torch.load('MERT_model_different_threshold.pth'))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get all the validation audio file names\n",
    "validation_audio_files = []\n",
    "for root, dirs, files in os.walk(validation_audio_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.npy'):\n",
    "            validation_audio_files.append(file)\n",
    "\n",
    "# for all the file in the dataset(under the validation audio path), store the audio-file name pair in a list\n",
    "validation_audio = []\n",
    "for file in range(len(validation_audio_files)):\n",
    "    audio = np.load(validation_audio_path + validation_audio_files[file])\n",
    "    validation_audio.append((validation_audio_files[file], audio))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read the label file\n",
    "with open(validation_label_path, 'r') as f:\n",
    "    validation_label = json.load(f)\n",
    "\n",
    "# for every key in the label file, find the corresponding label in train_audio, and append it in the tuple\n",
    "validation_data = []\n",
    "for key in validation_label:\n",
    "    for audio in validation_audio:\n",
    "        if key == audio[0]:\n",
    "            validation_data.append((audio[0], audio[1], validation_label[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if GPU is available, use it, otherwise use CPU\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# loading our model weights\n",
    "# model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True)\n",
    "MERT_model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True).to(device)\n",
    "# loading the corresponding preprocessor config\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-95M\",trust_remote_code=True)\n",
    "\n",
    "# happen to be 24kHz, the same as the dataset\n",
    "resample_rate = processor.sampling_rate\n",
    "\n",
    "# (label, embedding)\n",
    "validation_embedding_label = []\n",
    "\n",
    "# use tqdm to show the progress\n",
    "# process the data in batches, or the kernel will die\n",
    "# total: 3747\n",
    "\n",
    "for(filename, audio, label) in tqdm(validation_data):\n",
    "    input_audio = torch.tensor(audio).float().to(device)\n",
    "    # input_audio = torch.tensor(audio).float()\n",
    "    inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\").to(device)\n",
    "    # inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = MERT_model(**inputs, output_hidden_states=True)\n",
    "    all_layer_hidden_states = torch.stack(outputs.hidden_states).squeeze() # (13, 374, 768)\n",
    "    time_reduced_hidden_states = all_layer_hidden_states.mean(-2) # (13, 768)\n",
    "    \n",
    "    validation_embedding_label.append((label, time_reduced_hidden_states))\n",
    "\n",
    "# Convert tensors to lists\n",
    "serializable_validation_embedding_label = [\n",
    "    (label, embedding.tolist()) for label, embedding in validation_embedding_label\n",
    "]\n",
    "\n",
    "# save train_embedding_label\n",
    "with open('validation_embedding_label.json', 'w') as f:\n",
    "    json.dump(serializable_validation_embedding_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the label file back\n",
    "# Load all JSON file\n",
    "embedded_data_filename = [\"validation_embedding_label\"]\n",
    "\n",
    "validation_embedding_label = []\n",
    "for filename in embedded_data_filename:\n",
    "    with open(filename, 'r') as f:\n",
    "        loaded_validation_embedding_label = json.load(f)\n",
    "        loaded_validation_embedding_label = [\n",
    "            (label, torch.tensor(embedding)) for label, embedding in loaded_validation_embedding_label\n",
    "        ]\n",
    "        validation_embedding_label.extend(loaded_validation_embedding_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = EmbeddingDataset(validation_embedding_label)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: [0.1, 0.5, 0.5, 0.1, 0.1, 0.1, 0.5, 0.1, 0.5], Best Score: [0.9224782067247821, 0, 0, 0.9315109642535296, 0.9726918075422627, 0.8211448918104629, 0, 0.6764447051921385, 0]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set to find the best threshold\n",
    "thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "best_threshold, best_score = evaluate(model, val_loader, thresholds)\n",
    "print(f'Best Threshold: {best_threshold}, Best Score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_files = []\n",
    "for root, dirs, files in os.walk(test_audio_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.npy'):\n",
    "            test_audio_files.append(file)\n",
    "\n",
    "# for all the file in the dataset(under the test audio path), store the audio-file name pair in a list\n",
    "test_audio = []\n",
    "for file in range(len(test_audio_files)):\n",
    "    audio = np.load(test_audio_path + test_audio_files[file])\n",
    "    test_audio.append((test_audio_files[file], audio))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read the label file\n",
    "with open(test_label_path, 'r') as f:\n",
    "    test_label = json.load(f)\n",
    "\n",
    "# for every key in the label file, find the corresponding label in train_audio, and append it in the tuple\n",
    "test_data = []\n",
    "for key in test_label:\n",
    "    for audio in test_audio:\n",
    "        if key == audio[0]:\n",
    "            test_data.append((audio[0], audio[1], test_label[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if GPU is available, use it, otherwise use CPU\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# loading our model weights\n",
    "# model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True)\n",
    "MERT_model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True).to(device)\n",
    "# loading the corresponding preprocessor config\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-95M\",trust_remote_code=True)\n",
    "\n",
    "# happen to be 24kHz, the same as the dataset\n",
    "resample_rate = processor.sampling_rate\n",
    "\n",
    "# (label, embedding)\n",
    "test_embedding_label = []\n",
    "\n",
    "# use tqdm to show the progress\n",
    "# process the data in batches, or the kernel will die\n",
    "for(filename, audio, label) in tqdm(test_data):\n",
    "    input_audio = torch.tensor(audio).float().to(device)\n",
    "    # input_audio = torch.tensor(audio).float()\n",
    "    inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\").to(device)\n",
    "    # inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = MERT_model(**inputs, output_hidden_states=True)\n",
    "    all_layer_hidden_states = torch.stack(outputs.hidden_states).squeeze() # (13, 374, 768)\n",
    "    time_reduced_hidden_states = all_layer_hidden_states.mean(-2) # (13, 768)\n",
    "    \n",
    "    test_embedding_label.append((label, time_reduced_hidden_states))\n",
    "\n",
    "# Convert tensors to lists\n",
    "serializable_test_embedding_label = [\n",
    "    (label, embedding.tolist()) for label, embedding in test_embedding_label\n",
    "]\n",
    "\n",
    "# save train_embedding_label\n",
    "with open('test_embedding_label.json', 'w') as f:\n",
    "    json.dump(serializable_test_embedding_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the label file back\n",
    "# Load all JSON file\n",
    "embedded_data_filename = [\"test_embedding_label.json\"]\n",
    "\n",
    "test_embedding_label = []\n",
    "for filename in embedded_data_filename:\n",
    "    with open(filename, 'r') as f:\n",
    "        loaded_test_embedding_label = json.load(f)\n",
    "        loaded_test_embedding_label = [\n",
    "            (label, torch.tensor(embedding)) for label, embedding in loaded_test_embedding_label\n",
    "        ]\n",
    "        test_embedding_label.extend(loaded_test_embedding_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = EmbeddingDataset(test_embedding_label)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_8030/1501802402.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1762\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           Piano       0.91      0.93      0.92      1889\n",
      "      Percussion       0.00      0.00      0.00       243\n",
      "           Organ       0.00      0.00      0.00       461\n",
      "          Guitar       0.90      0.97      0.93      1943\n",
      "            Bass       0.97      0.97      0.97      2076\n",
      "         Strings       0.88      0.70      0.78      1235\n",
      "           Voice       0.00      0.00      0.00       485\n",
      "Wind Instruments       0.57      0.71      0.63       889\n",
      "           Synth       0.00      0.00      0.00       647\n",
      "\n",
      "       micro avg       0.87      0.73      0.79      9868\n",
      "       macro avg       0.47      0.48      0.47      9868\n",
      "    weighted avg       0.72      0.73      0.72      9868\n",
      "     samples avg       0.87      0.73      0.78      9868\n",
      "\n",
      "Test Metrics: (0.17623497997329773, '                  precision    recall  f1-score   support\\n\\n           Piano       0.91      0.93      0.92      1889\\n      Percussion       0.00      0.00      0.00       243\\n           Organ       0.00      0.00      0.00       461\\n          Guitar       0.90      0.97      0.93      1943\\n            Bass       0.97      0.97      0.97      2076\\n         Strings       0.88      0.70      0.78      1235\\n           Voice       0.00      0.00      0.00       485\\nWind Instruments       0.57      0.71      0.63       889\\n           Synth       0.00      0.00      0.00       647\\n\\n       micro avg       0.87      0.73      0.79      9868\\n       macro avg       0.47      0.48      0.47      9868\\n    weighted avg       0.72      0.73      0.72      9868\\n     samples avg       0.87      0.73      0.78      9868\\n')\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test set using the best threshold\n",
    "test_metrics = test(model, test_loader, best_threshold, class_idx2MIDIClass)\n",
    "print(f'Test Metrics: {test_metrics}')\n",
    "\n",
    "# Set the best threshold in the model\n",
    "model.threshold = best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.5, 0.5, 0.1, 0.1, 0.1, 0.5, 0.1, 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MERT_model_different_threshold.pth')\n",
    "# save the threshold\n",
    "with open('best_threshold.json', 'w') as f:\n",
    "    json.dump(best_threshold, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inference with the best threshold\n",
    "# sample_input = torch.randn(1, *input_size).to(device)  # Ensure the input is on the same device as the model\n",
    "# binary_output = model.predict(sample_input)\n",
    "# print(binary_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
