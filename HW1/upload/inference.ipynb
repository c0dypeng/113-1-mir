{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx2MIDIClass_path = \"hw1/class_idx2MIDIClass.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_audio_path = \"hw1/test_track/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# huggingface\n",
    "# from transformers import Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchaudio.transforms as T\n",
    "from datasets import load_dataset\n",
    "import torchaudio\n",
    "import nnAudio\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, embedding = self.data[idx]\n",
    "        embedding = torch.tensor(embedding, dtype=torch.float32)  # Shape: [13, 768]\n",
    "        return embedding, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "class MultiClassClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, thresholds=None):\n",
    "        super(MultiClassClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size[0] * input_size[1], 512)  # Adjusted input size\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.thresholds = thresholds if thresholds is not None else [0.5] * num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 13, 768]\n",
    "        x = x.view(x.size(0), -1)  # Flatten, shape: [batch_size, 13 * 768]\n",
    "        x = torch.relu(self.fc1(x))  # Fully connected layer with ReLU activation\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return self.sigmoid(x)  # Apply sigmoid to get probabilities\n",
    "\n",
    "    def predict(self, x, thresholds=None):\n",
    "        if thresholds is None:\n",
    "            thresholds = self.thresholds\n",
    "        with torch.no_grad():\n",
    "            probabilities = self.forward(x)\n",
    "            return (probabilities >= torch.tensor(thresholds).to(probabilities.device)).float()  # Apply thresholds to get binary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, num_epochs):\n",
    "    model.train()\n",
    "    total_batches = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_num}/{total_batches}], Loss: {loss.item():.4f}')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, thresholds):\n",
    "    num_classes = len(thresholds)\n",
    "    model.eval()\n",
    "    best_thresholds = [0.5] * num_classes\n",
    "    best_scores = [0] * num_classes\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        for threshold in thresholds:\n",
    "            all_labels = []\n",
    "            all_preds = []\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                preds = model.predict(inputs, thresholds=[threshold] * num_classes)\n",
    "                all_labels.append(labels.cpu().numpy()[:, i])\n",
    "                all_preds.append(preds.cpu().numpy()[:, i])\n",
    "            all_labels = np.concatenate(all_labels, axis=0)\n",
    "            all_preds = np.concatenate(all_preds, axis=0)\n",
    "            score = f1_score(all_labels, all_preds)  # Use F1-score for evaluation\n",
    "            if score > best_scores[i]:\n",
    "                best_scores[i] = score\n",
    "                best_thresholds[i] = threshold\n",
    "    \n",
    "    return best_thresholds, best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, thresholds, class_idx2MIDIClass):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    # Collect predictions and true labels\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        preds = model.predict(inputs, thresholds)\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all predictions and labels\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # Generate classification report\n",
    "    target_names = [class_idx2MIDIClass[str(i)] for i in range(len(class_idx2MIDIClass))]\n",
    "    report = classification_report(all_labels, all_preds, target_names=target_names, zero_division=0)\n",
    "    \n",
    "    # Print accuracy and classification report\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(report)\n",
    "    \n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "input_size = (13, 768)\n",
    "num_classes = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_11364/3657246210.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('MERT_model_different_threshold.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the best thresholds\n",
    "with open('best_threshold.json', 'r') as f:\n",
    "    thresholds = json.load(f)\n",
    "\n",
    "# Instantiate the model architecture\n",
    "model = MultiClassClassifier(input_size, num_classes, thresholds=thresholds).to(device)\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load('MERT_model_different_threshold.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.5, 0.5, 0.1, 0.1, 0.1, 0.5, 0.1, 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(model.thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hw1/test_track/Track01937.flac', 'hw1/test_track/Track02024.flac', 'hw1/test_track/Track02100.flac', 'hw1/test_track/Track02078.flac', 'hw1/test_track/Track01876.flac']\n"
     ]
    }
   ],
   "source": [
    "midi_path_list = glob(os.path.join(inference_audio_path, '*.flac'))\n",
    "print(midi_path_list)\n",
    "\n",
    "# sort the midi_path_list\n",
    "midi_path_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "(220500,)\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# for each song, cut the song into 5 seconds and embed it\n",
    "\n",
    "# Load the audio file\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "wav_file = []\n",
    "for file in midi_path_list:\n",
    "    waveform, sr = torchaudio.load(file)\n",
    "    song = []\n",
    "    # cut the song into 5 seconds， ignore the last clip that is less than 5 seconds\n",
    "    for i in range(waveform.shape[1]//SAMPLE_RATE//5):\n",
    "        clip = waveform[:, i*SAMPLE_RATE*5:(i+1)*SAMPLE_RATE*5]\n",
    "        # cast clip type back fro tensor\n",
    "        clip = clip.numpy()\n",
    "        # flatten the clip\n",
    "        clip = clip.flatten()\n",
    "        song.append(clip)\n",
    "    wav_file.append(song)\n",
    "\n",
    "print(wav_file[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at m-a-p/MERT-v1-95M were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v1-95M and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 51/51 [00:12<00:00,  4.03it/s]\n",
      "100%|██████████| 40/40 [00:09<00:00,  4.18it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.22it/s]\n",
      "100%|██████████| 43/43 [00:10<00:00,  4.27it/s]\n",
      "100%|██████████| 45/45 [00:10<00:00,  4.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# if GPU is available, use it, otherwise use CPU\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# loading our model weights\n",
    "# model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True)\n",
    "MERT_model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True).to(device)\n",
    "# loading the corresponding preprocessor config\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-95M\",trust_remote_code=True)\n",
    "\n",
    "# happen to be 24kHz, the same as the dataset\n",
    "resample_rate = processor.sampling_rate\n",
    "\n",
    "resampler = T.Resample(SAMPLE_RATE, resample_rate)\n",
    "\n",
    "# (label, embedding)\n",
    "inference_embedding = []\n",
    "\n",
    "# use tqdm to show the progress\n",
    "# process the data in batches, or the kernel will die\n",
    "for i in range(len(wav_file)):\n",
    "    song_embedding = []\n",
    "    for clip in tqdm(wav_file[i]):\n",
    "        input_audio = resampler(torch.from_numpy(clip)).float().to(device)\n",
    "        # input_audio = torch.tensor(audio).float()\n",
    "        inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = MERT_model(**inputs, output_hidden_states=True)\n",
    "        all_layer_hidden_states = torch.stack(outputs.hidden_states).squeeze() # (13, 374, 768)\n",
    "        time_reduced_hidden_states = all_layer_hidden_states.mean(-2) # (13, 768)\n",
    "\n",
    "        song_embedding.append(time_reduced_hidden_states)\n",
    "    inference_embedding.append(song_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 768])\n",
      "tensor([[1., 0., 0., 1., 1., 0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Inference with the best threshold\n",
    "sample_input = torch.randn(1, *input_size).to(device)  # Ensure the input is on the same device as the model\n",
    "print(sample_input.shape)\n",
    "binary_output = model.predict(sample_input)\n",
    "print(binary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = []\n",
    "\n",
    "for song in inference_embedding:\n",
    "    song_embedding = []\n",
    "    for clip in song:\n",
    "        # inference \n",
    "        # make the input shape from (13, 768) to (1, 13, 768)\n",
    "        clip = clip.unsqueeze(0)\n",
    "        output = model.predict(clip)\n",
    "        output = output.cpu().numpy()\n",
    "        # turn from (1, 9) to (9)\n",
    "        output = output[0]\n",
    "        song_embedding.append(output)\n",
    "    embedding.append(song_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if hw1/embedding folder exists\n",
    "if not os.path.exists('hw1/embedding'):\n",
    "    os.makedirs('hw1/embedding')\n",
    "\n",
    "for i in range(len(embedding)):\n",
    "    # dump the embedding into pickle\n",
    "    filename = midi_path_list[i].split('/')[-1].split('.')[0]\n",
    "    with open(f'hw1/embedding/{filename}.pkl', 'wb') as f:\n",
    "        pickle.dump(embedding[i], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
