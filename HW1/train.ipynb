{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-label classification\n",
    "# using MERT model as pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# huggingface\n",
    "# from transformers import Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchaudio.transforms as T\n",
    "from datasets import load_dataset\n",
    "import nnAudio\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'Piano', '1': 'Percussion', '2': 'Organ', '3': 'Guitar', '4': 'Bass', '5': 'Strings', '6': 'Voice', '7': 'Wind Instruments', '8': 'Synth'}\n",
      "Piano\n"
     ]
    }
   ],
   "source": [
    "# read index-label mapping\n",
    "with open('hw1/class_idx2MIDIClass.json', 'r') as f:\n",
    "    class_idx2MIDIClass = json.load(f)\n",
    "\n",
    "print(class_idx2MIDIClass)\n",
    "# print the mapping\n",
    "print(class_idx2MIDIClass['0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function cast np array file back into wav file\n",
    "# sampling rate 24kHz\n",
    "# use scipy.io.wavfile\n",
    "import scipy.io.wavfile as wavfile\n",
    "def np2wav(np_array, filename):\n",
    "    wavfile.write(filename, 24000, np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train dataset\n",
    "train_audio_path = 'hw1/slakh/train/'\n",
    "\n",
    "# get all the audio file names\n",
    "train_audio_files = []\n",
    "for root, dirs, files in os.walk(train_audio_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.npy'):\n",
    "            train_audio_files.append(file)\n",
    "\n",
    "# for all the file in the dataset(under the train audio path), store the audio-file name pair in a list\n",
    "train_audio = []\n",
    "for file in range(len(train_audio_files)):\n",
    "    audio = np.load(train_audio_path + train_audio_files[file])\n",
    "    train_audio.append((train_audio_files[file], audio))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train label\n",
    "train_label_path = 'hw1/slakh/train_labels.json'\n",
    "\n",
    "# read the label file\n",
    "with open(train_label_path, 'r') as f:\n",
    "    train_label = json.load(f)\n",
    "\n",
    "# for every key in the label file, find the corresponding label in train_audio, and append it in the tuple\n",
    "train_data = []\n",
    "for key in train_label:\n",
    "    for audio in train_audio:\n",
    "        if key == audio[0]:\n",
    "            train_data.append((audio[0], audio[1], train_label[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Track00001_17.npy', array([-0.18277621, -0.21051419, -0.16272902, ...,  0.06085217,\n",
      "        0.04477724,  0.04972876], dtype=float32), [1, 0, 1, 1, 1, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# train data \n",
    "# (filename, audio, label)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at m-a-p/MERT-v1-95M were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v1-95M and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"mps\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# loading our model weights\n",
    "model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True).to(device)\n",
    "# loading the corresponding preprocessor config\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-95M\",trust_remote_code=True)\n",
    "\n",
    "# happen to be 24kHz, the same as the dataset\n",
    "resample_rate = processor.sampling_rate\n",
    "\n",
    "# (label, embedding)\n",
    "train_embedding_label = []\n",
    "\n",
    "# use tqdm to show the progress\n",
    "# for(filename, audio, label) in tqdm(train_data[:2]): # only use the first 2 data for test\n",
    "for(filename, audio, label) in tqdm(train_data):\n",
    "    input_audio = torch.tensor(audio).float().to(device)\n",
    "    inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    all_layer_hidden_states = torch.stack(outputs.hidden_states).squeeze()\n",
    "    train_embedding_label.append((label, all_layer_hidden_states))\n",
    "\n",
    "# save train_embedding_label\n",
    "with open('train_embedding_label.json', 'w') as f:\n",
    "    json.dump(train_embedding_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 374, 768])\n"
     ]
    }
   ],
   "source": [
    "print(train_embedding_label[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, embedding = self.data[idx]\n",
    "        return torch.tensor(embedding, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# class MultiClassClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, num_classes):\n",
    "#         super(MultiClassClassifier, self).__init__()\n",
    "#         self.pool = nn.AdaptiveAvgPool1d(1)  # Pooling along the time axis\n",
    "#         self.fc1 = nn.Linear(input_size[2], 512)  # Adjusted input size\n",
    "#         self.fc2 = nn.Linear(512, num_classes)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x shape: [batch_size, 13, 374, 768]\n",
    "#         x = x.mean(dim=1)  # Pooling along the time axis, resulting shape: [batch_size, 374, 768]\n",
    "#         x = self.pool(x.permute(0, 2, 1)).squeeze(-1)  # Shape: [batch_size, 768]\n",
    "#         x = torch.relu(self.fc1(x))  # Fully connected layer with ReLU activation\n",
    "#         x = self.fc2(x)  # Output layer\n",
    "#         return self.sigmoid(x)  # Apply sigmoid to get multi-hot output\n",
    "\n",
    "class MultiClassClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, threshold=0.5):\n",
    "        super(MultiClassClassifier, self).__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)  # Pooling along the time axis\n",
    "        self.fc1 = nn.Linear(input_size[2], 512)  # Adjusted input size\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 13, 374, 768]\n",
    "        x = x.mean(dim=1)  # Pooling along the time axis, resulting shape: [batch_size, 374, 768]\n",
    "        x = self.pool(x.permute(0, 2, 1)).squeeze(-1)  # Shape: [batch_size, 768]\n",
    "        x = torch.relu(self.fc1(x))  # Fully connected layer with ReLU activation\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return self.sigmoid(x)  # Apply sigmoid to get probabilities\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            probabilities = self.forward(x)\n",
    "            return (probabilities >= self.threshold).float()  # Apply threshold to get binary output\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6957\n",
      "Epoch [2/10], Loss: 0.6825\n",
      "Epoch [3/10], Loss: 0.6704\n",
      "Epoch [4/10], Loss: 0.6584\n",
      "Epoch [5/10], Loss: 0.6455\n",
      "Epoch [6/10], Loss: 0.6316\n",
      "Epoch [7/10], Loss: 0.6172\n",
      "Epoch [8/10], Loss: 0.6028\n",
      "Epoch [9/10], Loss: 0.5889\n",
      "Epoch [10/10], Loss: 0.5760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/rxqxvrmd0z1bnnny3trbtfz80000gn/T/ipykernel_57627/2943078684.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(embedding, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "input_size = (13, 374, 768)\n",
    "num_classes = 9\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = MultiClassClassifier(input_size, num_classes)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "dataset = EmbeddingDataset(train_embedding_label)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "train(model, criterion, optimizer, train_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# # inference\n",
    "\n",
    "# sample_input = torch.randn(1, *input_size)\n",
    "# binary_output = model.predict(sample_input)\n",
    "# print(binary_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
